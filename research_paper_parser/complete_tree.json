[
  [
    {
      "id": "2025-training_large_language_models_to_reason_in_a_continuous_latent_space",
      "type": "core",
      "title": "Training Large Language Models to Reason in a Continuous Latent Space",
      "year": 2025,
      "authors": null,
      "name": "Chain Of Continuous Thoughts"
    },
    {
      "id": "2025-search-o1:_agentic_search-enhanced_large_reasoning_models",
      "type": "core",
      "title": "Search-o1: Agentic Search-Enhanced Large Reasoning Models",
      "year": 2025,
      "authors": null,
      "name": "Search-o1"
    },
    {
      "id": "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1",
      "type": "core",
      "title": "A Tutorial on LLM Reasoning: Relevant Methods behind ChatGPT o1",
      "year": 2025,
      "authors": null,
      "name": "LLM Reasoning"
    },
    {
      "id": "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations",
      "type": "core",
      "title": "Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations",
      "year": 2023,
      "authors": null,
      "name": "Recommendation AI Agent"
    },
    {
      "id": "2019-temporal_collaborative_ranking_via_personalized_transformer",
      "type": "core",
      "title": "Temporal Collaborative Ranking  Via Personalized Transformer",
      "year": "2019",
      "authors": null,
      "name": "SSE-PT"
    },
    {
      "id": "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations",
      "type": "core",
      "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
      "year": 2019,
      "authors": null,
      "name": "ALBERT"
    },
    {
      "id": "2016-wide_&_deep_learning_for_recommender_systems",
      "type": "core",
      "title": "Wide & Deep Learning for Recommender Systems",
      "year": "2016",
      "authors": null,
      "name": "Wide & Deep"
    }
  ],
  [
    {
      "id": "2303-hugginggpt:_solving_ai_tasks_with_chatgpt_and_its_friends_in_huggingface",
      "type": "core",
      "title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
      "year": "2303",
      "authors": "Shen, Y.; Song, K.; Tan, X.; Li, D.; Lu, W.; and Zhuang, Y. 2023",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "As models show emergent intelligence, researchers have started exploring the potential to leverage LLMs as autonomous agents(Wang et al2023a; Zhao, Jin, and Cheng2023), augmented with memory modules, planning ability, and tool-using capabilities",
          "com/Significant-Gravitas/Auto-GPT, HuggingGPT(Shen et al2023), and Visual ChatGPT(Wu et al2023)"
        ]
      }
    },
    {
      "id": "2101-what_makes_good_in-context_examples_for_gpt-3333?",
      "type": "core",
      "title": "What Makes Good In-Context Examples for GPT-3333?",
      "year": "2101",
      "authors": "Liu, J.; Shen, D.; Zhang, Y.; Dolan, B.; Carin, L.; and Chen, W. 2021",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "The scaling-up of parameters and data has led to significant advancements in the capabilities of LLMs, including in-context learning(Brown et al2020; Liu et al2021; Rubin, Herzig, and Berant2021), instruction following(Ouyang et al2022; Touvron et al2023a; OpenAI2023), planning and reasoning(Wei et al2022; Wang et al2022a; Yao et al2022; Yang et al2023; Wang et al2023b)"
        ]
      }
    },
    {
      "id": "2025-scaling_up_test-time_compute_with_latent_reasoning:_a_recurrent_depth_approach",
      "type": "core",
      "title": "Scaling up test-time compute with latent reasoning: A recurrent depth approach",
      "year": 2025,
      "authors": "Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Tom Goldstein",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "We are encouraged by recent progress in this area(Geiping et al,2025; Barrault et al,2024; Gladstone et al,2025)"
        ]
      }
    },
    {
      "id": "2025-reasoning_by_superposition:_a_theoretical_perspective_on_chain_of_continuous_thought",
      "type": "core",
      "title": "Reasoning by superposition: A theoretical perspective on chain of continuous thought",
      "year": 2025,
      "authors": "Hanlin Zhu, Shibo Hao, Zhiting Hu, Jiantao Jiao, Stuart Russell, Yuandong Tian",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Building onCoconut,Zhu et al (2025b)developed a theoretical framework demonstrating that continuous CoT can be more efficient than discrete CoT on certain tasks by encoding multiple reasoning paths in superposition states"
        ]
      }
    },
    {
      "id": "2025-learning_to_reason_with_llms",
      "type": "core",
      "title": "Learning to reason with llms",
      "year": 2025,
      "authors": "Junghyun Lee, Branislav Kveton, Anup Rao, Subhojyoti Mukherjee, Ryan A. Rossi, Sunav Choudhary, Alexa Siu",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "OpenAI has recently unveiled ChatGPT o1[17], a groundbreaking Large Language Model (LLM) that represents a giant leap forward in strong AI"
        ]
      }
    },
    {
      "id": "2025-energy-based_transformers_are_scalable_learners_and_thinkers",
      "type": "core",
      "title": "Energy-based transformers are scalable learners and thinkers",
      "year": 2025,
      "authors": "Alexi Gladstone, Ganesh Nanduru, Md Mofijul Islam, Peixuan Han, Hyeonjeong Ha, Aman Chadha, Yilun Du, Heng Ji, Jundong Li, Tariq Iqbal",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "We are encouraged by recent progress in this area(Geiping et al,2025; Barrault et al,2024; Gladstone et al,2025)"
        ]
      }
    },
    {
      "id": "2025-emergence_of_superposition:_unveiling_the_training_dynamics_of_chain_of_continuous_thought",
      "type": "core",
      "title": "Emergence of superposition: Unveiling the training dynamics of chain of continuous thought",
      "year": 2025,
      "authors": "Hanlin Zhu, Shibo Hao, Zhiting Hu, Jiantao Jiao, Stuart Russell, Yuandong Tian",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Subsequently,Zhu et al (2025a)analyzed the training dynamics to explain how such superposition emerges under theCoconuttraining objective"
        ]
      }
    },
    {
      "id": "2025-deepseek-r1:_incentivizing_reasoning_capability_in_llms_via_reinforcement_learning",
      "type": "core",
      "title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
      "year": 2025,
      "authors": "DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, Zhen Zhang",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Thus, generating more tokens serves as a way to inference-time scaling for reasoning(Guo et al,2025; Snell et al,2024)"
        ]
      }
    },
    {
      "id": "2024-we-math:_does_your_large_multimodal_model_achieve_human-like_mathematical_reasoning?",
      "type": "core",
      "title": "We-math: Does your large multimodal model achieve human-like mathematical reasoning?",
      "year": 2024,
      "authors": "Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, Runfeng Qiao, Yifan Zhang, Xiao Zong, Yida Xu, Muxi Diao, Zhimin Bao, Chen Li, Honggang Zhang",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "The o1-like reasoning paradigm has demonstrated strong performance across diverse domains, including vision-language reasoning[65,11,48,69], code generation[81,32], healthcare[3], and machine translation[57]"
        ]
      }
    },
    {
      "id": "2024-understand_what_llm_needs:_dual_preference_alignment_for_retrieval-augmented_generation",
      "type": "core",
      "title": "Understand what LLM needs: Dual preference alignment for retrieval-augmented generation",
      "year": 2024,
      "authors": "Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, Ji-Rong Wen",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on"
        ]
      }
    },
    {
      "id": "2024-trustworthiness_in_retrieval-augmented_generation_systems:_a_survey",
      "type": "core",
      "title": "Trustworthiness in retrieval-augmented generation systems: A survey",
      "year": 2024,
      "authors": "Yujia Zhou, Yan Liu, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Zheng Liu, Chaozhuo Li, Zhicheng Dou, Tsung-Yi Ho, Philip S. Yu",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Retrieval-augmented generation (RAG) introduces retrieval mechanisms to address the limitations of static parameters in generative models, allowing access to external knowledge to solve more complex problems[30,82,35,86]"
        ]
      }
    },
    {
      "id": "2024-towards_conversational_recommender_systems",
      "type": "core",
      "title": "Towards conversational recommender systems",
      "year": 2024,
      "authors": "Peixin Qin, Chen Huang, Yang Deng, Wenqiang Lei, Tat-Seng Chua",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "Key research challenges in this area include developing strategies for selecting queried attributes(Mirzadeh, Ricci, and Bansal2005; Zhang et al2018)and addressing the exploration-exploitation trade-off(Christakopoulou, Radlinski, and Hofmann2016; Xie et al2021)"
        ]
      }
    },
    {
      "id": "2024-toward_general_instruction-following_alignment_for_retrieval-augmented_generation",
      "type": "core",
      "title": "Toward general instruction-following alignment for retrieval-augmented generation",
      "year": 2024,
      "authors": "Guanting Dong, Xiaoshuai Song, Yutao Zhu, Runqi Qiao, Zhicheng Dou, Ji-Rong Wen",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on"
        ]
      }
    },
    {
      "id": "2024-token-level_direct_preference_optimization",
      "type": "core",
      "title": "Token-level direct preference optimization",
      "year": 2024,
      "authors": "Yongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, Jun Wang",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "One can explore more efficient offline methods such as token-level DPO[35]without PRM but with sequential reasoning data"
        ]
      }
    },
    {
      "id": "2024-technical_report:_enhancing_llm_reasoning_with_reward-guided_tree_search",
      "type": "core",
      "title": "Technical report: Enhancing llm reasoning with reward-guided tree search",
      "year": "2024",
      "authors": "Jinhao Jiang, Zhipeng Chen, Yingqian Min, Jie Chen, Xiaoxue Cheng, Jiapeng Wang, Yiru Tang, Haoxiang Sun, Jia Deng, Wayne Xin Zhao, et al",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Some methods combine policy and reward models with Monte Carlo Tree Search (MCTS)[25], though this does not internalize reasoning within the model",
          "This advancement has inspired a series of foundational efforts aimed at exploring and reproducing o1-like reasoning patterns, to broaden their application to a wider range of foundational models[49,19,77,80,71,25,45]"
        ]
      }
    },
    {
      "id": "2024-teaching_large_language_models_to_reason_with_reinforcement_learning",
      "type": "core",
      "title": "Teaching large language models to reason with reinforcement learning",
      "year": 2024,
      "authors": "Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, Roberta Raileanu",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "This includes prompting LLMs(Wei et al,2022; Khot et al,2022; Zhou et al,2022), or training LLMs to generate reasoning chains, either with supervised finetuning(Yue et al,2023; Yu et al,2023)or reinforcement learning(Wang et al,2024; Havrilla et al,2024; Shao et al,2024; Yu et al,2024a)"
        ]
      }
    },
    {
      "id": "2024-stream_of_search_(sos):_learning_to_search_in_language",
      "type": "core",
      "title": "Stream of search (sos): Learning to search in language",
      "year": 2024,
      "authors": "Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, Noah D. Goodman",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "This includes prompting LLMs(Wei et al,2022; Khot et al,2022; Zhou et al,2022), or training LLMs to generate reasoning chains, either with supervised finetuning(Yue et al,2023; Yu et al,2023)or reinforcement learning(Wang et al,2024; Havrilla et al,2024; Shao et al,2024; Yu et al,2024a)"
        ]
      }
    },
    {
      "id": "2024-small_models_big_insights:_leveraging_slim_proxy_models_to_decide_when_and_what_to_retrieve_for_llms",
      "type": "core",
      "title": "Small models, big insights: Leveraging slim proxy models to decide when and what to retrieve for llms",
      "year": 2024,
      "authors": "Jiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo, Kun Fang, Ji-Rong Wen",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on"
        ]
      }
    },
    {
      "id": "2024-self-play_with_execution_feedback:_improving_instruction-following_capabilities_of_large_language_models",
      "type": "core",
      "title": "Self-play with execution feedback: Improving instruction-following capabilities of large language models",
      "year": 2024,
      "authors": "Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, Jingren Zhou",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Retrieval-augmented generation (RAG) introduces retrieval mechanisms to address the limitations of static parameters in generative models, allowing access to external knowledge to solve more complex problems[30,82,35,86]"
        ]
      }
    },
    {
      "id": "2024-scaling_of_search_and_learning:_a_roadmap_to_reproduce_o1_from_reinforcement_learning_perspective",
      "type": "core",
      "title": "Scaling of search and learning: A roadmap to reproduce o1 from reinforcement learning perspective",
      "year": 2024,
      "authors": "Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Bo Wang, Shimin Li, Yunhua Zhou, Qipeng Guo, Xuanjing Huang, Xipeng Qiu",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Large reasoning models focus on enhancing performance at test time by utilizing extended reasoning steps, contrasting with traditional large pre-trained models that achieve scalability during training by increasing model size or expanding training data[17,66,50,85,76]"
        ]
      }
    },
    {
      "id": "2024-retrollm:_empowering_large_language_models_to_retrieve_fine-grained_evidence_within_generation",
      "type": "core",
      "title": "Retrollm: Empowering large language models to retrieve fine-grained evidence within generation",
      "year": 2024,
      "authors": "Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yongkang Wu, Zhonghua Li, Qi Ye, Zhicheng Dou",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Furthermore, some studies have explored end-to-end model training to implement RAG systems[1,36,33,34]and knowledge-graph-based RAG systems[14,37]"
        ]
      }
    },
    {
      "id": "2024-retrieval-augmented_generation_for_ai-generated_content:_a_survey",
      "type": "core",
      "title": "Retrieval-augmented generation for ai-generated content: A survey",
      "year": 2024,
      "authors": "Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, Bin Cui",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Retrieval-augmented generation (RAG) introduces retrieval mechanisms to address the limitations of static parameters in generative models, allowing access to external knowledge to solve more complex problems[30,82,35,86]"
        ]
      }
    },
    {
      "id": "2024-qwq:_reflect_deeply_on_the_boundaries_of_the_unknown_november_2024",
      "type": "core",
      "title": "Qwq: Reflect deeply on the boundaries of the unknown, November 2024",
      "year": "2024",
      "authors": "Qwen Team",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Recently, models like OpenAI-o1[22], Qwen-QwQ[54]and DeepSeek-R1[7]explicitly demonstrate chain-of-thought reasoning[59], mimicking human problem-solving approaches in domains such as mathematics, coding, and so on",
          "For the backbone large reasoning model in Search-o1, we utilize the open-sourced QwQ-32B-Preview[54]",
          "Recently emerged large reasoning models (LRMs), exemplified by OpenAI’s o1[22], Qwen-QwQ[54]and DeepSeek-R1[7], employ large-scale reinforcement learning foster impressive long-sequence stepwise reasoning capabilities, offering promising solutions to complex reasoning problems[46,31,59,84,73,74,67]",
          "5-Coder-32B-Instruct[20], QwQ-32B-Preview[54], Qwen2"
        ]
      }
    },
    {
      "id": "2024-qwen2.5_technical_report_2024",
      "type": "core",
      "title": "Qwen2.5 technical report, 2024",
      "year": "2024",
      "authors": "Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Large reasoning models focus on enhancing performance at test time by utilizing extended reasoning steps, contrasting with traditional large pre-trained models that achieve scalability during training by increasing model size or expanding training data[17,66,50,85,76]",
          "5-32B-Instruct[50], Qwen2"
        ]
      }
    },
    {
      "id": "2024-qwen2.5-math_technical_report:_toward_mathematical_expert_model_via_self-improvement",
      "type": "core",
      "title": "Qwen2.5-math technical report: Toward mathematical expert model via self-improvement",
      "year": 2024,
      "authors": "An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, Zhenru Zhang",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Recently emerged large reasoning models (LRMs), exemplified by OpenAI’s o1[22], Qwen-QwQ[54]and DeepSeek-R1[7], employ large-scale reinforcement learning foster impressive long-sequence stepwise reasoning capabilities, offering promising solutions to complex reasoning problems[46,31,59,84,73,74,67]"
        ]
      }
    },
    {
      "id": "2024-qwen2.5-coder_technical_report",
      "type": "core",
      "title": "Qwen2.5-coder technical report",
      "year": 2024,
      "authors": "Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, Junyang Lin",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "5-Coder-32B-Instruct[20], QwQ-32B-Preview[54], Qwen2"
        ]
      }
    },
    {
      "id": "2024-progressive_multimodal_reasoning_via_active_retrieval",
      "type": "core",
      "title": "Progressive multimodal reasoning via active retrieval",
      "year": 2024,
      "authors": "Guanting Dong, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, Ji-Rong Wen",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "This finding aligns with human intuition, as standard RAG retrieves relevant knowledge only once in a problem-oriented manner, while the knowledge required for each step in complex reasoning scenarios is often varied and diverse[83,41,11]",
          "The o1-like reasoning paradigm has demonstrated strong performance across diverse domains, including vision-language reasoning[65,11,48,69], code generation[81,32], healthcare[3], and machine translation[57]"
        ]
      }
    },
    {
      "id": "2024-processbench:_identifying_process_errors_in_mathematical_reasoning",
      "type": "core",
      "title": "Processbench: Identifying process errors in mathematical reasoning",
      "year": 2024,
      "authors": "Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, Junyang Lin",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "This finding aligns with human intuition, as standard RAG retrieves relevant knowledge only once in a problem-oriented manner, while the knowledge required for each step in complex reasoning scenarios is often varied and diverse[83,41,11]"
        ]
      }
    },
    {
      "id": "2024-planxrag:_planning-guided_retrieval_augmented_generation",
      "type": "core",
      "title": "Planxrag: Planning-guided retrieval augmented generation",
      "year": "2024",
      "authors": "Prakhar Verma, Sukruta Prakash Midigeshi, Gaurav Sinha, Arno Solin, Nagarajan Natarajan, and Amit Sharma",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Recently, agentic RAG systems empower models to autonomously determine when and what knowledge to retrieve as needed, showcasing enhanced planning and problem-solving capabilities[5,56,70]"
        ]
      }
    },
    {
      "id": "2024-physics_of_language_models:_part_2.2_how_to_learn_from_mistakes_on_grade-school_math_problems",
      "type": "core",
      "title": "Physics of language models: Part 2.2, how to learn from mistakes on grade-school math problems",
      "year": 2024,
      "authors": "Tian Ye, Zicheng Xu, Yuanzhi Li, Zeyuan Allen-Zhu",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Other studies incorporate deliberate errors in reasoning paths during training to partially internalize these abilities[49,71]",
          "This advancement has inspired a series of foundational efforts aimed at exploring and reproducing o1-like reasoning patterns, to broaden their application to a wider range of foundational models[49,19,77,80,71,25,45]"
        ]
      }
    },
    {
      "id": "2024-openai_o1_system_card",
      "type": "core",
      "title": "Openai o1 system card",
      "year": 2024,
      "authors": "OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quiñonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, Zhuohan Li",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Recently, models like OpenAI-o1[22], Qwen-QwQ[54]and DeepSeek-R1[7]explicitly demonstrate chain-of-thought reasoning[59], mimicking human problem-solving approaches in domains such as mathematics, coding, and so on",
          "Closed-source non-proprietary models include DeepSeek-R1-Lite-Preview[7], OpenAI GPT-4o[21], and o1-preview[22]",
          "Recently emerged large reasoning models (LRMs), exemplified by OpenAI’s o1[22], Qwen-QwQ[54]and DeepSeek-R1[7], employ large-scale reinforcement learning foster impressive long-sequence stepwise reasoning capabilities, offering promising solutions to complex reasoning problems[46,31,59,84,73,74,67]"
        ]
      }
    },
    {
      "id": "2024-o1_replication_journey–part_2:_surpassing_o1-preview_through_simple_distillation_big_progress_or_bitter_lesson?",
      "type": "core",
      "title": "O1 replication journey–part 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson?",
      "year": 2024,
      "authors": "Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, Pengfei Liu",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "This advancement has inspired a series of foundational efforts aimed at exploring and reproducing o1-like reasoning patterns, to broaden their application to a wider range of foundational models[49,19,77,80,71,25,45]"
        ]
      }
    },
    {
      "id": "2024-o1_replication_journey:_a_strategic_progress_report–part_1",
      "type": "core",
      "title": "O1 replication journey: A strategic progress report–part 1",
      "year": 2024,
      "authors": "Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, Pengfei Liu",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Other studies incorporate deliberate errors in reasoning paths during training to partially internalize these abilities[49,71]",
          "This advancement has inspired a series of foundational efforts aimed at exploring and reproducing o1-like reasoning patterns, to broaden their application to a wider range of foundational models[49,19,77,80,71,25,45]"
        ]
      }
    },
    {
      "id": "2024-o1-coder:_an_o1_replication_for_coding",
      "type": "core",
      "title": "o1-coder: an o1 replication for coding",
      "year": 2024,
      "authors": "Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, Jitao Sang",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "The o1-like reasoning paradigm has demonstrated strong performance across diverse domains, including vision-language reasoning[65,11,48,69], code generation[81,32], healthcare[3], and machine translation[57]",
          "This advancement has inspired a series of foundational efforts aimed at exploring and reproducing o1-like reasoning patterns, to broaden their application to a wider range of foundational models[49,19,77,80,71,25,45]"
        ]
      }
    },
    {
      "id": "2024-multi-step_problem_solving_through_a_verifier:_an_empirical_analysis_on_model-induced_process_supervision",
      "type": "core",
      "title": "Multi-step problem solving through a verifier: An empirical analysis on model-induced process supervision",
      "year": 2024,
      "authors": "Zihan Wang, Yunxuan Li, Yuexin Wu, Liangchen Luo, Le Hou, Hongkun Yu, Jingbo Shang",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "The work in[31]highlights the importance of practical data acquisition for reasoning tasks, particularly in coding problems"
        ]
      }
    },
    {
      "id": "2024-mulberry:_empowering_mllm_with_o1-like_reasoning_and_reflection_via_collective_monte_carlo_tree_search",
      "type": "core",
      "title": "Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search",
      "year": 2024,
      "authors": "Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, Dacheng Tao",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "The o1-like reasoning paradigm has demonstrated strong performance across diverse domains, including vision-language reasoning[65,11,48,69], code generation[81,32], healthcare[3], and machine translation[57]"
        ]
      }
    },
    {
      "id": "2024-mindsearch:_mimicking_human_minds_elicits_deep_ai_searcher",
      "type": "core",
      "title": "Mindsearch: Mimicking human minds elicits deep ai searcher",
      "year": 2024,
      "authors": "Zehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu, Wenwei Zhang, Kai Chen, Feng Zhao",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Recently, agentic RAG systems empower models to autonomously determine when and what knowledge to retrieve as needed, showcasing enhanced planning and problem-solving capabilities[5,56,70]"
        ]
      }
    },
    {
      "id": "2024-metacognitive_retrieval-augmented_large_language_models",
      "type": "core",
      "title": "Metacognitive retrieval-augmented large language models",
      "year": 2024,
      "authors": "Yujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, Zhicheng Dou",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on"
        ]
      }
    },
    {
      "id": "2024-looped_transformers_for_length_generalization",
      "type": "core",
      "title": "Looped transformers for length generalization",
      "year": 2024,
      "authors": "Ying Fan, Yilun Du, Kannan Ramchandran, Kangwook Lee",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Previous works mostly define latent reasoning in LLMs as the hidden computation in transformers(Yang et al,2024; Biran et al,2024)"
        ]
      }
    },
    {
      "id": "2024-long-context_llms_meet_rag:_overcoming_challenges_for_long_inputs_in_rag",
      "type": "core",
      "title": "Long-context llms meet RAG: overcoming challenges for long inputs in RAG",
      "year": 2024,
      "authors": "Bowen Jin, Jinsung Yoon, Jiawei Han, Sercan O. Arik",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Retrieved documents are often lengthy and contain redundant information, directly inputting them into LRMs may disrupt the original coherence of reasoning and even introduce noise[62,72,26]"
        ]
      }
    },
    {
      "id": "2024-llm_reasoners:_new_evaluation_library_and_analysis_of_step-by-step_reasoning_with_large_language_models",
      "type": "core",
      "title": "Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models",
      "year": 2024,
      "authors": "Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, Zhen Wang, Zhiting Hu",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "This includes prompting LLMs(Wei et al,2022; Khot et al,2022; Zhou et al,2022), or training LLMs to generate reasoning chains, either with supervised finetuning(Yue et al,2023; Yu et al,2023)or reinforcement learning(Wang et al,2024; Havrilla et al,2024; Shao et al,2024; Yu et al,2024a)"
        ]
      }
    },
    {
      "id": "2024-llava-o1:_let_vision_language_models_reason_step-by-step",
      "type": "core",
      "title": "Llava-o1: Let vision language models reason step-by-step",
      "year": "2024",
      "authors": "Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "The o1-like reasoning paradigm has demonstrated strong performance across diverse domains, including vision-language reasoning[65,11,48,69], code generation[81,32], healthcare[3], and machine translation[57]"
        ]
      }
    },
    {
      "id": "2024-llama-berry:_pairwise_optimization_for_o1-like_olympiad-level_mathematical_reasoning",
      "type": "core",
      "title": "Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning",
      "year": 2024,
      "authors": "Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, Wanli Ouyang, Dongzhan Zhou",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "This advancement has inspired a series of foundational efforts aimed at exploring and reproducing o1-like reasoning patterns, to broaden their application to a wider range of foundational models[49,19,77,80,71,25,45]"
        ]
      }
    },
    {
      "id": "2024-let’s_think_dot_by_dot:_hidden_computation_in_transformer_language_models",
      "type": "core",
      "title": "Let’s think dot by dot: Hidden computation in transformer language models",
      "year": 2024,
      "authors": "Jacob Pfau, William Merrill, Samuel R. Bowman",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Previous works mostly define latent reasoning in LLMs as the hidden computation in transformers(Yang et al,2024; Biran et al,2024)"
        ]
      }
    },
    {
      "id": "2024-learning_to_reason_with_llms_2024",
      "type": "core",
      "title": "Learning to reason with llms, 2024",
      "year": "2024",
      "authors": "OpenAI",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Recently emerged large reasoning models (LRMs), exemplified by OpenAI’s o1[22], Qwen-QwQ[54]and DeepSeek-R1[7], employ large-scale reinforcement learning foster impressive long-sequence stepwise reasoning capabilities, offering promising solutions to complex reasoning problems[46,31,59,84,73,74,67]"
        ]
      }
    },
    {
      "id": "2024-large_concept_models:_language_modeling_in_a_sentence_representation_space",
      "type": "core",
      "title": "Large concept models: Language modeling in a sentence representation space",
      "year": 2024,
      "authors": "LCM team, Loïc Barrault, Paul-Ambroise Duquenne, Maha Elbayad, Artyom Kozhevnikov, Belen Alastruey, Pierre Andrews, Mariano Coria, Guillaume Couairon, Marta R. Costa-jussà, David Dale, Hady Elsahar, Kevin Heffernan, João Maria Janeiro, Tuan Tran, Christophe Ropers, Eduardo Sánchez, Robin San Roman, Alexandre Mourachko, Safiyyah Saleem, Holger Schwenk",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "We are encouraged by recent progress in this area(Geiping et al,2025; Barrault et al,2024; Gladstone et al,2025)",
          "Previous works mostly define latent reasoning in LLMs as the hidden computation in transformers(Yang et al,2024; Biran et al,2024)"
        ]
      }
    },
    {
      "id": "2024-language_models_don’t_always_say_what_they_think:_unfaithful_explanations_in_chain-of-thought_prompting",
      "type": "core",
      "title": "Language models don’t always say what they think: unfaithful explanations in chain-of-thought prompting",
      "year": "2024",
      "authors": "Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Previous works mostly define latent reasoning in LLMs as the hidden computation in transformers(Yang et al,2024; Biran et al,2024)"
        ]
      }
    },
    {
      "id": "2024-language_is_primarily_a_tool_for_communication_rather_than_thought",
      "type": "core",
      "title": "Language is primarily a tool for communication rather than thought",
      "year": "2024",
      "authors": "Evelina Fedorenko, Steven T Piantadosi, and Edward AF Gibson",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Large language models (LLMs) have demonstrated remarkable reasoning abilities, emerging from extensive pretraining on human languages(Dubey et al,2024; Achiam et al,2023)"
        ]
      }
    },
    {
      "id": "2024-kag:_boosting_llms_in_professional_domains_via_knowledge_augmented_generation_2024",
      "type": "core",
      "title": "Kag: Boosting llms in professional domains via knowledge augmented generation, 2024",
      "year": "2024",
      "authors": "Lei Liang, Mengshu Sun, Zhengke Gui, Zhongshu Zhu, Zhouyu Jiang, Ling Zhong, Yuan Qu, Peilong Zhao, Zhongpu Bo, Jin Yang, Huaidong Xiong, Lin Yuan, Jun Xu, Zaoyang Wang, Zhiqiang Zhang, Wen Zhang, Huajun Chen, Wenguang Chen, and Jun Zhou",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Furthermore, some studies have explored end-to-end model training to implement RAG systems[1,36,33,34]and knowledge-graph-based RAG systems[14,37]"
        ]
      }
    },
    {
      "id": "2024-improve_mathematical_reasoning_in_language_models_by_automated_process_supervision",
      "type": "core",
      "title": "Improve mathematical reasoning in language models by automated process supervision",
      "year": 2024,
      "authors": "Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, Abhinav Rastogi",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "For longer reasoning sequences, techniques such as Monte Carlo Tree Search (MCTS)[6,15]are employed to guide the LLM policy to find correct reasoning steps efficiently in a more fine-grained manner",
          "The training typically involves optimising a classification loss function based on the correctness of the reasoning steps[15]:",
          "Proposed methods range from collecting specialised training data to building reward models[18,11,15]and increasing the computational complexity of decoding[24,33], but none have yet achieved significant performance breakthroughs at scale",
          "MCTS has been used for acquiring data in[6], whereas it has been extended with linear search for efficiency in[15]"
        ]
      }
    },
    {
      "id": "2024-imitate_explore_and_self-improve:_a_reproduction_report_on_slow-thinking_reasoning_systems",
      "type": "core",
      "title": "Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning systems",
      "year": 2024,
      "authors": "Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, Ji-Rong Wen",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Additionally, distilling training data has been shown to enhance models’ o1-like reasoning skills[45]",
          "This advancement has inspired a series of foundational efforts aimed at exploring and reproducing o1-like reasoning patterns, to broaden their application to a wider range of foundational models[49,19,77,80,71,25,45]"
        ]
      }
    },
    {
      "id": "2024-huatuogpt-o1_towards_medical_complex_reasoning_with_llms",
      "type": "core",
      "title": "Huatuogpt-o1, towards medical complex reasoning with llms",
      "year": 2024,
      "authors": "Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, Benyou Wang",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "The o1-like reasoning paradigm has demonstrated strong performance across diverse domains, including vision-language reasoning[65,11,48,69], code generation[81,32], healthcare[3], and machine translation[57]"
        ]
      }
    },
    {
      "id": "2024-how_much_can_rag_help_the_reasoning_of_llm?",
      "type": "core",
      "title": "How much can RAG help the reasoning of llm?",
      "year": 2024,
      "authors": "Jingyu Liu, Jiaen Lin, Yong Liu",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on",
          "This finding aligns with human intuition, as standard RAG retrieves relevant knowledge only once in a problem-oriented manner, while the knowledge required for each step in complex reasoning scenarios is often varied and diverse[83,41,11]",
          "While this characteristic enhances logical coherence and interpretability of reasoning, an extended chain of thought may cause overthinking[4]and increased risks of knowledge insufficiency[60,51,2], where any knowledge gap can propagate errors and disrupt the entire reasoning chain[79,40,44,41]"
        ]
      }
    },
    {
      "id": "2024-how_easily_do_irrelevant_inputs_skew_the_responses_of_large_language_models?_2024",
      "type": "core",
      "title": "How easily do irrelevant inputs skew the responses of large language models?, 2024",
      "year": "2024",
      "authors": "Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, and Yanghua Xiao",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Retrieved documents are often lengthy and contain redundant information, directly inputting them into LRMs may disrupt the original coherence of reasoning and even introduce noise[62,72,26]"
        ]
      }
    },
    {
      "id": "2024-hopping_too_late:_exploring_the_limitations_of_large_language_models_on_multi-hop_queries",
      "type": "core",
      "title": "Hopping too late: Exploring the limitations of large language models on multi-hop queries",
      "year": 2024,
      "authors": "Eden Biran, Daniela Gottesman, Sohee Yang, Mor Geva, Amir Globerson",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Previous works mostly define latent reasoning in LLMs as the hidden computation in transformers(Yang et al,2024; Biran et al,2024)"
        ]
      }
    },
    {
      "id": "2024-gpt-4o_system_card",
      "type": "core",
      "title": "Gpt-4o system card",
      "year": 2024,
      "authors": "OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Mądry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian O'Connell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, Yury Malkov",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Closed-source non-proprietary models include DeepSeek-R1-Lite-Preview[7], OpenAI GPT-4o[21], and o1-preview[22]"
        ]
      }
    },
    {
      "id": "2024-from_matching_to_generation:_a_survey_on_generative_information_retrieval",
      "type": "core",
      "title": "From matching to generation: A survey on generative information retrieval",
      "year": 2024,
      "authors": "Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, Zhicheng Dou",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Retrieval-augmented generation (RAG) introduces retrieval mechanisms to address the limitations of static parameters in generative models, allowing access to external knowledge to solve more complex problems[30,82,35,86]"
        ]
      }
    },
    {
      "id": "2024-from_local_to_global:_a_graph_rag_approach_to_query-focused_summarization_2024",
      "type": "core",
      "title": "From local to global: A graph rag approach to query-focused summarization, 2024",
      "year": "2024",
      "authors": "Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Furthermore, some studies have explored end-to-end model training to implement RAG systems[1,36,33,34]and knowledge-graph-based RAG systems[14,37]"
        ]
      }
    },
    {
      "id": "2024-from_explicit_cot_to_implicit_cot:_learning_to_internalize_cot_step_by_step",
      "type": "core",
      "title": "From explicit cot to implicit cot: Learning to internalize cot step by step",
      "year": 2024,
      "authors": "Yuntian Deng, Yejin Choi, Stuart Shieber",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "We leverage language CoT data to supervise continuous thought by implementing a multi-stage training curriculum inspired byDeng et al (2024)",
          "To enhance the training of latent reasoning, we employ a multi-stage training strategy inspired byDeng et al (2024), which effectively utilizes language reasoning chains to guide the training process",
          "To illustrate this, we train a series of CoT models that progressively “internalize”(Deng et al,2024)the initialm={0,1,2,3,ALL}m=\\{0,1,2,3,\\text{ALL}\\}reasoning steps, and plot their accuracy versus the number of generated tokens (labeled as “language” in the figure)",
          "Previous works mostly define latent reasoning in LLMs as the hidden computation in transformers(Yang et al,2024; Biran et al,2024)",
          "∗The result is fromDeng et al (2024)",
          "Future work will explore finer-grained schedules, such as incrementally adding continuous thoughts one at a time while removing fewer language tokens, as in iCoT(Deng et al,2024)",
          "Thus, generating more tokens serves as a way to inference-time scaling for reasoning(Guo et al,2025; Snell et al,2024)",
          "(3)iCoT(Deng et al,2024): The model is trained with language reasoning chains and follows a carefully designed schedule that “internalizes” CoT"
        ]
      }
    },
    {
      "id": "2024-evaluation_of_openai_o1:_opportunities_and_challenges_of_agi",
      "type": "core",
      "title": "Evaluation of openai o1: Opportunities and challenges of AGI",
      "year": 2024,
      "authors": "Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Zeyu Zhang, Yifan Zhou, Shizhe Liang, Zihao Wu, Yanjun Lyu, Peng Shu, Xiaowei Yu, Chao Cao, Hanqi Jiang, Hanxu Chen, Yiwei Li, Junhao Chen, Huawen Hu, Yiheng Liu, Huaqin Zhao, Shaochen Xu, Haixing Dai, Lin Zhao, Ruidong Zhang, Wei Zhao, Zhenyuan Yang, Jingyuan Chen, Peilong Wang, Wei Ruan, Hui Wang, Huan Zhao, Jing Zhang, Yiming Ren, Shihuan Qin, Tong Chen, Jiaxi Li, Arif Hassan Zidan, Afrar Jahin, Minheng Chen, Sichen Xia, Jason Holmes, Yan Zhuang, Jiaqi Wang, Bochen Xu, Weiran Xia, Jichao Yu, Kaibo Tang, Yaxuan Yang, Bolun Sun, Tao Yang, Guoyu Lu, Xianqiao Wang, Lilong Chai, He Li, Jin Lu, Xin Zhang, Bao Ge, Xintao Hu, Lian Zhang, Hua Zhou, Lu Zhang, Shu Zhang, Zhen Xiang, Yudan Ren, Jun Liu, Xi Jiang, Yu Bao, Wei Zhang, Xiang Li, Gang Li, Wei Liu, Dinggang Shen, Andrea Sikora, Xiaoming Zhai, Dajiang Zhu, Tuo Zhang, Tianming Liu",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Large reasoning models focus on enhancing performance at test time by utilizing extended reasoning steps, contrasting with traditional large pre-trained models that achieve scalability during training by increasing model size or expanding training data[17,66,50,85,76]",
          "Recently emerged large reasoning models (LRMs), exemplified by OpenAI’s o1[22], Qwen-QwQ[54]and DeepSeek-R1[7], employ large-scale reinforcement learning foster impressive long-sequence stepwise reasoning capabilities, offering promising solutions to complex reasoning problems[46,31,59,84,73,74,67]"
        ]
      }
    },
    {
      "id": "2024-evaluating_mathematical_reasoning_beyond_accuracy",
      "type": "core",
      "title": "Evaluating mathematical reasoning beyond accuracy",
      "year": 2024,
      "authors": "Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, Pengfei Liu",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Notably, the high specialization of these problems also complicates manual reasoning verification, often incurring significant costs[63]"
        ]
      }
    },
    {
      "id": "2024-dualformer:_controllable_fast_and_slow_thinking_by_learning_with_randomized_reasoning_traces",
      "type": "core",
      "title": "Dualformer: Controllable fast and slow thinking by learning with randomized reasoning traces",
      "year": 2024,
      "authors": "DiJia Su, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, Qinqing Zheng",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "This includes prompting LLMs(Wei et al,2022; Khot et al,2022; Zhou et al,2022), or training LLMs to generate reasoning chains, either with supervised finetuning(Yue et al,2023; Yu et al,2023)or reinforcement learning(Wang et al,2024; Havrilla et al,2024; Shao et al,2024; Yu et al,2024a)"
        ]
      }
    },
    {
      "id": "2024-drt-o1:_optimized_deep_reasoning_translation_via_long_chain-of-thought",
      "type": "core",
      "title": "Drt-o1: Optimized deep reasoning translation via long chain-of-thought",
      "year": "2024",
      "authors": "Jiaan Wang, Fandong Meng, Yunlong Liang, and Jie Zhou",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "The o1-like reasoning paradigm has demonstrated strong performance across diverse domains, including vision-language reasoning[65,11,48,69], code generation[81,32], healthcare[3], and machine translation[57]"
        ]
      }
    },
    {
      "id": "2024-dotamath:_decomposition_of_thought_with_code_assistance_and_self-correction_for_mathematical_reasoning",
      "type": "core",
      "title": "Dotamath: Decomposition of thought with code assistance and self-correction for mathematical reasoning",
      "year": 2024,
      "authors": "Chengpeng Li, Guanting Dong, Mingfeng Xue, Ru Peng, Xiang Wang, Dayiheng Liu",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "The o1-like reasoning paradigm has demonstrated strong performance across diverse domains, including vision-language reasoning[65,11,48,69], code generation[81,32], healthcare[3], and machine translation[57]"
        ]
      }
    },
    {
      "id": "2024-do_not_think_that_much_for_2+3=?_on_the_overthinking_of_o1-like_llms_2024",
      "type": "core",
      "title": "Do not think that much for 2+3=? on the overthinking of o1-like llms, 2024",
      "year": "2024",
      "authors": "Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "While this characteristic enhances logical coherence and interpretability of reasoning, an extended chain of thought may cause overthinking[4]and increased risks of knowledge insufficiency[60,51,2], where any knowledge gap can propagate errors and disrupt the entire reasoning chain[79,40,44,41]"
        ]
      }
    },
    {
      "id": "2024-do_large_language_models_latently_perform_multi-hop_reasoning?",
      "type": "core",
      "title": "Do large language models latently perform multi-hop reasoning?",
      "year": 2024,
      "authors": "Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, Sebastian Riedel",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Previous works mostly define latent reasoning in LLMs as the hidden computation in transformers(Yang et al,2024; Biran et al,2024)"
        ]
      }
    },
    {
      "id": "2024-distributional_reasoning_in_llms:_parallel_reasoning_processes_in_multi-hop_reasoning",
      "type": "core",
      "title": "Distributional reasoning in llms: Parallel reasoning processes in multi-hop reasoning",
      "year": 2024,
      "authors": "Yuval Shalev, Amir Feder, Ariel Goldstein",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Previous works mostly define latent reasoning in LLMs as the hidden computation in transformers(Yang et al,2024; Biran et al,2024)"
        ]
      }
    },
    {
      "id": "2024-distilling_system_2_into_system_1",
      "type": "core",
      "title": "Distilling system 2 into system 1",
      "year": 2024,
      "authors": "Ping Yu, Jing Xu, Jason Weston, Ilia Kulikov",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Yu et al (2024b)also proposed to distill a model that can reason latently from data generated with complex reasoning algorithms"
        ]
      }
    },
    {
      "id": "2024-deepseekmath:_pushing_the_limits_of_mathematical_reasoning_in_open_language_models",
      "type": "core",
      "title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models",
      "year": 2024,
      "authors": "Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, Daya Guo",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "Let us look at Group Relative Policy Optimisation (GRPO)[22]"
        ],
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "This includes prompting LLMs(Wei et al,2022; Khot et al,2022; Zhou et al,2022), or training LLMs to generate reasoning chains, either with supervised finetuning(Yue et al,2023; Yu et al,2023)or reinforcement learning(Wang et al,2024; Havrilla et al,2024; Shao et al,2024; Yu et al,2024a)"
        ]
      }
    },
    {
      "id": "2024-deepseek-r1-lite-preview_is_now_live:_unleashing_supercharged_reasoning_power!_november_2024",
      "type": "core",
      "title": "Deepseek-r1-lite-preview is now live: unleashing supercharged reasoning power!, November 2024",
      "year": "2024",
      "authors": "DeepSeek-AI",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Large reasoning models focus on enhancing performance at test time by utilizing extended reasoning steps, contrasting with traditional large pre-trained models that achieve scalability during training by increasing model size or expanding training data[17,66,50,85,76]",
          "5-72B-Instruct[50], and Llama3",
          "Recently emerged large reasoning models (LRMs), exemplified by OpenAI’s o1[22], Qwen-QwQ[54]and DeepSeek-R1[7], employ large-scale reinforcement learning foster impressive long-sequence stepwise reasoning capabilities, offering promising solutions to complex reasoning problems[46,31,59,84,73,74,67]"
        ]
      }
    },
    {
      "id": "2024-corpuslm:_towards_a_unified_language_model_on_corpus_for_knowledge-intensive_tasks",
      "type": "core",
      "title": "Corpuslm: Towards a unified language model on corpus for knowledge-intensive tasks",
      "year": 2024,
      "authors": "Xiaoxi Li, Zhicheng Dou, Yujia Zhou, Fangchao Liu",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Furthermore, some studies have explored end-to-end model training to implement RAG systems[1,36,33,34]and knowledge-graph-based RAG systems[14,37]"
        ]
      }
    },
    {
      "id": "2024-chain_of_thought_empowers_transformers_to_solve_inherently_serial_problems",
      "type": "core",
      "title": "Chain of thought empowers transformers to solve inherently serial problems",
      "year": 2024,
      "authors": "Zhiyuan Li, Hong Liu, Denny Zhou, Tengyu Ma",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "This includes prompting LLMs(Wei et al,2022; Khot et al,2022; Zhou et al,2022), or training LLMs to generate reasoning chains, either with supervised finetuning(Yue et al,2023; Yu et al,2023)or reinforcement learning(Wang et al,2024; Havrilla et al,2024; Shao et al,2024; Yu et al,2024a)"
        ]
      }
    },
    {
      "id": "2024-bider:_bridging_knowledge_inconsistency_for_efficient_retrieval-augmented_llms_via_key_supporting_evidence",
      "type": "core",
      "title": "Bider: Bridging knowledge inconsistency for efficient retrieval-augmented llms via key supporting evidence",
      "year": 2024,
      "authors": "Jiajie Jin, Yutao Zhu, Yujia Zhou, Zhicheng Dou",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on"
        ]
      }
    },
    {
      "id": "2024-beyond_a*:_better_planning_with_transformers_via_search_dynamics_bootstrapping",
      "type": "core",
      "title": "Beyond a*: Better planning with transformers via search dynamics bootstrapping",
      "year": 2024,
      "authors": "Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul Mcvay, Michael Rabbat, Yuandong Tian",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "This includes prompting LLMs(Wei et al,2022; Khot et al,2022; Zhou et al,2022), or training LLMs to generate reasoning chains, either with supervised finetuning(Yue et al,2023; Yu et al,2023)or reinforcement learning(Wang et al,2024; Havrilla et al,2024; Shao et al,2024; Yu et al,2024a)"
        ]
      }
    },
    {
      "id": "2024-assistrag:_boosting_the_potential_of_large_language_models_with_an_intelligent_information_assistant",
      "type": "core",
      "title": "Assistrag: Boosting the potential of large language models with an intelligent information assistant",
      "year": 2024,
      "authors": "Yujia Zhou, Zheng Liu, Zhicheng Dou",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on"
        ]
      }
    },
    {
      "id": "2024-an_empirical_analysis_of_compute-optimal_inference_for_problem-solving_with_language_models",
      "type": "core",
      "title": "An empirical analysis of compute-optimal inference for problem-solving with language models",
      "year": 2024,
      "authors": "Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, Yiming Yang",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "Proposed methods range from collecting specialised training data to building reward models[18,11,15]and increasing the computational complexity of decoding[24,33], but none have yet achieved significant performance breakthroughs at scale",
          "To strike a balance between efficiency and effectiveness, the work[24,33]found that reasoning tasks benefit from more flexible approaches like beam search"
        ]
      }
    },
    {
      "id": "2024-aflow:_automating_agentic_workflow_generation",
      "type": "core",
      "title": "Aflow: Automating agentic workflow generation",
      "year": 2024,
      "authors": "Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, Bingnan Zheng, Bang Liu, Yuyu Luo, Chenglin Wu",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "There is also research combining agent-based systems with MCTS to optimize complex workflows, leveraging retrievers and other tools to accomplish tasks[78]"
        ]
      }
    },
    {
      "id": "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model",
      "type": "core",
      "title": "A Comparative Study on Reasoning Patterns of OpenAI’s o1 Model",
      "year": 2024,
      "authors": null,
      "name": "OpenAI o1",
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "It is noteworthy that o1-like reasoning patterns guide LRMs to engage in a slower thinking process[6,61]by implicitly breaking down complex problems, generating a long internal reasoning chain and then discovering suitable solutions step by step"
        ]
      }
    },
    {
      "id": "2023-zero-shot_next-item_recommendation_using_large_pretrained_language_models",
      "type": "core",
      "title": "Zero-Shot Next-Item Recommendation using Large Pretrained Language Models",
      "year": 2023,
      "authors": "Lei Wang, Ee-Peng Lim",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "The scaling-up of parameters and data has led to significant advancements in the capabilities of LLMs, including in-context learning(Brown et al2020; Liu et al2021; Rubin, Herzig, and Berant2021), instruction following(Ouyang et al2022; Touvron et al2023a; OpenAI2023), planning and reasoning(Wei et al2022; Wang et al2022a; Yao et al2022; Yang et al2023; Wang et al2023b)"
        ]
      }
    },
    {
      "id": "2023-why_think_step_by_step?_reasoning_emerges_from_the_locality_of_experience",
      "type": "core",
      "title": "Why think step by step? reasoning emerges from the locality of experience",
      "year": 2023,
      "authors": "Ben Prystawski, Michael Y. Li, Noah D. Goodman",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "Finally, there is a growing body of research aimed at understanding the mechanisms behind step-by-step reasoning in LLMs[26,19]"
        ]
      }
    },
    {
      "id": "2023-why_can_large_language_models_generate_correct_chain-of-thoughts?",
      "type": "core",
      "title": "Why can large language models generate correct chain-of-thoughts?",
      "year": 2023,
      "authors": "Rasul Tutunov, Antoine Grosnit, Juliusz Ziomek, Jun Wang, Haitham Bou-Ammar",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "Finally, there is a growing body of research aimed at understanding the mechanisms behind step-by-step reasoning in LLMs[26,19]"
        ]
      }
    },
    {
      "id": "2023-visual_chatgpt:_talking_drawing_and_editing_with_visual_foundation_models",
      "type": "core",
      "title": "Visual chatgpt: Talking, drawing and editing with visual foundation models",
      "year": 2023,
      "authors": "Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "As models show emergent intelligence, researchers have started exploring the potential to leverage LLMs as autonomous agents(Wang et al2023a; Zhao, Jin, and Cheng2023), augmented with memory modules, planning ability, and tool-using capabilities",
          "com/Significant-Gravitas/Auto-GPT, HuggingGPT(Shen et al2023), and Visual ChatGPT(Wu et al2023)"
        ]
      }
    },
    {
      "id": "2023-unigen:_a_unified_generative_framework_for_retrieval_and_question_answering_with_large_language_models",
      "type": "core",
      "title": "Unigen: A unified generative framework for retrieval and question answering with large language models",
      "year": 2023,
      "authors": "Xiaoxi Li, Yujia Zhou, Zhicheng Dou",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Furthermore, some studies have explored end-to-end model training to implement RAG systems[1,36,33,34]and knowledge-graph-based RAG systems[14,37]"
        ]
      }
    },
    {
      "id": "2023-uncovering_chatgpt’s_capabilities_in_recommender_systems",
      "type": "core",
      "title": "Uncovering ChatGPT’s Capabilities in Recommender Systems",
      "year": 2023,
      "authors": "Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, Jun Xu",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "The scaling-up of parameters and data has led to significant advancements in the capabilities of LLMs, including in-context learning(Brown et al2020; Liu et al2021; Rubin, Herzig, and Berant2021), instruction following(Ouyang et al2022; Touvron et al2023a; OpenAI2023), planning and reasoning(Wei et al2022; Wang et al2022a; Yao et al2022; Yang et al2023; Wang et al2023b)"
        ]
      }
    },
    {
      "id": "2023-tree_of_thoughts:_deliberate_problem_solving_with_large_language_models",
      "type": "core",
      "title": "Tree of thoughts: Deliberate problem solving with large language models",
      "year": 2023,
      "authors": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, Karthik Narasimhan",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "As models show emergent intelligence, researchers have started exploring the potential to leverage LLMs as autonomous agents(Wang et al2023a; Zhao, Jin, and Cheng2023), augmented with memory modules, planning ability, and tool-using capabilities"
        ],
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "This includes prompting LLMs(Wei et al,2022; Khot et al,2022; Zhou et al,2022), or training LLMs to generate reasoning chains, either with supervised finetuning(Yue et al,2023; Yu et al,2023)or reinforcement learning(Wang et al,2024; Havrilla et al,2024; Shao et al,2024; Yu et al,2024a)",
          "This advanced reasoning mechanism surpasses traditional CoT, even though the model is not explicitly trained or instructed to operate in this manner, as seen in previous works(Yao et al,2023; Hao et al,2023)"
        ]
      }
    },
    {
      "id": "2023-towards_revealing_the_mystery_behind_chain_of_thought:_a_theoretical_perspective",
      "type": "core",
      "title": "Towards revealing the mystery behind chain of thought: a theoretical perspective",
      "year": 2023,
      "authors": "Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, Liwei Wang",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Studies have shown that test-time scaling can improve the reasoning abilities of smaller models on complex tasks[15,75]"
        ],
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "This includes prompting LLMs(Wei et al,2022; Khot et al,2022; Zhou et al,2022), or training LLMs to generate reasoning chains, either with supervised finetuning(Yue et al,2023; Yu et al,2023)or reinforcement learning(Wang et al,2024; Havrilla et al,2024; Shao et al,2024; Yu et al,2024a)",
          "Language CoT proves to increase the effective depth of LLMs and enhance their expressiveness(Feng et al,2023)"
        ]
      }
    },
    {
      "id": "2023-toolformer:_language_models_can_teach_themselves_to_use_tools",
      "type": "core",
      "title": "Toolformer: Language models can teach themselves to use tools",
      "year": 2023,
      "authors": "Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "As models show emergent intelligence, researchers have started exploring the potential to leverage LLMs as autonomous agents(Wang et al2023a; Zhao, Jin, and Cheng2023), augmented with memory modules, planning ability, and tool-using capabilities"
        ]
      }
    },
    {
      "id": "2023-tool_learning_with_foundation_models",
      "type": "core",
      "title": "Tool learning with foundation models",
      "year": 2023,
      "authors": "Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, Maosong Sun",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "As models show emergent intelligence, researchers have started exploring the potential to leverage LLMs as autonomous agents(Wang et al2023a; Zhao, Jin, and Cheng2023), augmented with memory modules, planning ability, and tool-using capabilities"
        ]
      }
    },
    {
      "id": "2023-think-in-memory:_recalling_and_post-thinking_enable_llms_with_long-term_memory",
      "type": "core",
      "title": "Think-in-memory: Recalling and post-thinking enable llms with long-term memory",
      "year": 2023,
      "authors": "Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, Guannan Zhang",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "For example,(Wang et al2023c; Zhong et al2023; Liu et al2023b)have equipped LLMs with an external memory, empowering LLMs with growth potential Regarding the planning, CoT(Wei et al2022; Kojima et al2022)and ReAct(Yao et al2022)propose to enhance planning by step-wise reasoning; ToT(Yao et al2023)and GoT(Besta et al2023)introduce multi-path reasoning to ensure consistency and correctness; Self-Refine(Madaan et al2023)and Reflexion(Shinn et al2023)lead the LLMs to reflect on errors, with the ultimate goal of improving their subsequent problem-solving success rates"
        ]
      }
    },
    {
      "id": "2023-the_unlocking_spell_on_base_llms:_rethinking_alignment_via_in-context_learning",
      "type": "core",
      "title": "The unlocking spell on base llms: Rethinking alignment via in-context learning",
      "year": 2023,
      "authors": "Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, Yejin Choi",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "This focus has resulted in a degree of catastrophic forgetting in their general capabilities[39,10], ultimately limiting their long-context understanding of retrieved documents"
        ]
      }
    },
    {
      "id": "2023-the_expresssive_power_of_transformers_with_chain_of_thought",
      "type": "core",
      "title": "The expresssive power of transformers with chain of thought",
      "year": "2023",
      "authors": "William Merrill and Ashish Sabharwal",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "This includes prompting LLMs(Wei et al,2022; Khot et al,2022; Zhou et al,2022), or training LLMs to generate reasoning chains, either with supervised finetuning(Yue et al,2023; Yu et al,2023)or reinforcement learning(Wang et al,2024; Havrilla et al,2024; Shao et al,2024; Yu et al,2024a)"
        ]
      }
    },
    {
      "id": "2023-siren’s_song_in_the_ai_ocean:_a_survey_on_hallucination_in_large_language_models",
      "type": "core",
      "title": "Siren’s song in the AI ocean: A survey on hallucination in large language models",
      "year": 2023,
      "authors": "Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Chen Xu, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, Shuming Shi",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "While this characteristic enhances logical coherence and interpretability of reasoning, an extended chain of thought may cause overthinking[4]and increased risks of knowledge insufficiency[60,51,2], where any knowledge gap can propagate errors and disrupt the entire reasoning chain[79,40,44,41]"
        ]
      }
    },
    {
      "id": "2023-selfcheck:_using_llms_to_zero-shot_check_their_own_step-by-step_reasoning",
      "type": "core",
      "title": "Selfcheck: Using llms to zero-shot check their own step-by-step reasoning",
      "year": 2023,
      "authors": "Ning Miao, Yee Whye Teh, Tom Rainforth",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "While this characteristic enhances logical coherence and interpretability of reasoning, an extended chain of thought may cause overthinking[4]and increased risks of knowledge insufficiency[60,51,2], where any knowledge gap can propagate errors and disrupt the entire reasoning chain[79,40,44,41]"
        ]
      }
    },
    {
      "id": "2023-self-rag:_learning_to_retrieve_generate_and_critique_through_self-reflection",
      "type": "core",
      "title": "Self-rag: Learning to retrieve, generate, and critique through self-reflection",
      "year": 2023,
      "authors": "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on"
        ]
      }
    },
    {
      "id": "2023-self-evaluation_guided_beam_search_for_reasoning",
      "type": "core",
      "title": "Self-evaluation guided beam search for reasoning",
      "year": 2023,
      "authors": "Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, Qizhe Xie",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "This includes prompting LLMs(Wei et al,2022; Khot et al,2022; Zhou et al,2022), or training LLMs to generate reasoning chains, either with supervised finetuning(Yue et al,2023; Yu et al,2023)or reinforcement learning(Wang et al,2024; Havrilla et al,2024; Shao et al,2024; Yu et al,2024a)"
        ]
      }
    },
    {
      "id": "2023-scaling_relationship_on_learning_mathematical_reasoning_with_large_language_models",
      "type": "core",
      "title": "Scaling relationship on learning mathematical reasoning with large language models",
      "year": 2023,
      "authors": "Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, Jingren Zhou",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Recently emerged large reasoning models (LRMs), exemplified by OpenAI’s o1[22], Qwen-QwQ[54]and DeepSeek-R1[7], employ large-scale reinforcement learning foster impressive long-sequence stepwise reasoning capabilities, offering promising solutions to complex reasoning problems[46,31,59,84,73,74,67]"
        ]
      }
    },
    {
      "id": "2023-reflexion:_language_agents_with_verbal_reinforcement_learning",
      "type": "core",
      "title": "Reflexion: Language agents with verbal reinforcement learning",
      "year": 2023,
      "authors": "Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "As models show emergent intelligence, researchers have started exploring the potential to leverage LLMs as autonomous agents(Wang et al2023a; Zhao, Jin, and Cheng2023), augmented with memory modules, planning ability, and tool-using capabilities",
          "Despite LLM’s strong intelligence, it still exhibits occasional errors in reasoning and tool utilization(Madaan et al2023; Shinn et al2023)",
          "To reduce the occurrence of such errors, some studies have employed self-reflection(Shinn et al2023)mechanisms to enable LLM to have some error-correcting capabilities during decision-making"
        ]
      }
    },
    {
      "id": "2023-recomp:_improving_retrieval-augmented_lms_with_compression_and_selective_augmentation",
      "type": "core",
      "title": "Recomp: Improving retrieval-augmented lms with compression and selective augmentation",
      "year": 2023,
      "authors": "Fangyuan Xu, Weijia Shi, Eunsol Choi",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on"
        ]
      }
    },
    {
      "id": "2023-reasoning_with_language_model_is_planning_with_world_model",
      "type": "core",
      "title": "Reasoning with language model is planning with world model",
      "year": 2023,
      "authors": "Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "This includes prompting LLMs(Wei et al,2022; Khot et al,2022; Zhou et al,2022), or training LLMs to generate reasoning chains, either with supervised finetuning(Yue et al,2023; Yu et al,2023)or reinforcement learning(Wang et al,2024; Havrilla et al,2024; Shao et al,2024; Yu et al,2024a)",
          "This advanced reasoning mechanism surpasses traditional CoT, even though the model is not explicitly trained or instructed to operate in this manner, as seen in previous works(Yao et al,2023; Hao et al,2023)"
        ]
      }
    },
    {
      "id": "2023-query_rewriting_for_retrieval-augmented_large_language_models",
      "type": "core",
      "title": "Query rewriting for retrieval-augmented large language models",
      "year": 2023,
      "authors": "Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, Nan Duan",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on"
        ]
      }
    },
    {
      "id": "2023-query2doc:_query_expansion_with_large_language_models",
      "type": "core",
      "title": "Query2doc: Query expansion with large language models",
      "year": 2023,
      "authors": "Liang Wang, Nan Yang, Furu Wei",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on"
        ]
      }
    },
    {
      "id": "2023-plan-and-solve_prompting:_improving_zero-shot_chain-of-thought_reasoning_by_large_language_models",
      "type": "core",
      "title": "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models",
      "year": 2023,
      "authors": "Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, Ee-Peng Lim",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "The scaling-up of parameters and data has led to significant advancements in the capabilities of LLMs, including in-context learning(Brown et al2020; Liu et al2021; Rubin, Herzig, and Berant2021), instruction following(Ouyang et al2022; Touvron et al2023a; OpenAI2023), planning and reasoning(Wei et al2022; Wang et al2022a; Yao et al2022; Yang et al2023; Wang et al2023b)"
        ]
      }
    },
    {
      "id": "2023-pangu-agent:_a_fine-tunable_generalist_agent_with_structured_reasoning",
      "type": "core",
      "title": "Pangu-agent: A fine-tunable generalist agent with structured reasoning",
      "year": 2023,
      "authors": "Filippos Christianos, Georgios Papoudakis, Matthieu Zimmer, Thomas Coste, Zhihao Wu, Jingxuan Chen, Khyati Khandelwal, James Doran, Xidong Feng, Jiacheng Liu, Zheng Xiong, Yicheng Luo, Jianye Hao, Kun Shao, Haitham Bou-Ammar, Jun Wang",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "From a system perspective, the pangu-agent paper[3]introduces structured reasoning mechanisms beyond traditional models like OpenAI’s o1 model",
          "This approach[3]aligns with the concept of working memory in cognitive science, which is crucial for complex problem-solving and deliberative thinking"
        ]
      }
    },
    {
      "id": "2023-mm-react:_prompting_chatgpt_for_multimodal_reasoning_and_action",
      "type": "core",
      "title": "Mm-react: Prompting chatgpt for multimodal reasoning and action",
      "year": 2023,
      "authors": "Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "The scaling-up of parameters and data has led to significant advancements in the capabilities of LLMs, including in-context learning(Brown et al2020; Liu et al2021; Rubin, Herzig, and Berant2021), instruction following(Ouyang et al2022; Touvron et al2023a; OpenAI2023), planning and reasoning(Wei et al2022; Wang et al2022a; Yao et al2022; Yang et al2023; Wang et al2023b)"
        ]
      }
    },
    {
      "id": "2023-metamath:_bootstrap_your_own_mathematical_questions_for_large_language_models",
      "type": "core",
      "title": "Metamath: Bootstrap your own mathematical questions for large language models",
      "year": 2023,
      "authors": "Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Recently emerged large reasoning models (LRMs), exemplified by OpenAI’s o1[22], Qwen-QwQ[54]and DeepSeek-R1[7], employ large-scale reinforcement learning foster impressive long-sequence stepwise reasoning capabilities, offering promising solutions to complex reasoning problems[46,31,59,84,73,74,67]"
        ],
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "This includes prompting LLMs(Wei et al,2022; Khot et al,2022; Zhou et al,2022), or training LLMs to generate reasoning chains, either with supervised finetuning(Yue et al,2023; Yu et al,2023)or reinforcement learning(Wang et al,2024; Havrilla et al,2024; Shao et al,2024; Yu et al,2024a)"
        ]
      }
    },
    {
      "id": "2023-memorybank:_enhancing_large_language_models_with_long-term_memory",
      "type": "core",
      "title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory",
      "year": 2023,
      "authors": "Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "As models show emergent intelligence, researchers have started exploring the potential to leverage LLMs as autonomous agents(Wang et al2023a; Zhao, Jin, and Cheng2023), augmented with memory modules, planning ability, and tool-using capabilities"
        ]
      }
    },
    {
      "id": "2023-math-shepherd:_verify_and_reinforce_llms_step-by-step_without_human_annotations",
      "type": "core",
      "title": "Math-shepherd: Verify and reinforce llms step-by-step without human annotations",
      "year": 2023,
      "authors": "Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, Zhifang Sui",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "The paper[30]takes this approach further, showing how LLMs can be trained step-by-step without the need for costly human annotations, providing a more scalable solution to the reasoning data problem"
        ],
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "This includes prompting LLMs(Wei et al,2022; Khot et al,2022; Zhou et al,2022), or training LLMs to generate reasoning chains, either with supervised finetuning(Yue et al,2023; Yu et al,2023)or reinforcement learning(Wang et al,2024; Havrilla et al,2024; Shao et al,2024; Yu et al,2024a)"
        ]
      }
    },
    {
      "id": "2023-mammoth:_building_math_generalist_models_through_hybrid_instruction_tuning",
      "type": "core",
      "title": "Mammoth: Building math generalist models through hybrid instruction tuning",
      "year": 2023,
      "authors": "Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "This includes prompting LLMs(Wei et al,2022; Khot et al,2022; Zhou et al,2022), or training LLMs to generate reasoning chains, either with supervised finetuning(Yue et al,2023; Yu et al,2023)or reinforcement learning(Wang et al,2024; Havrilla et al,2024; Shao et al,2024; Yu et al,2024a)"
        ]
      }
    },
    {
      "id": "2023-making_retrieval-augmented_language_models_robust_to_irrelevant_context",
      "type": "core",
      "title": "Making retrieval-augmented language models robust to irrelevant context",
      "year": 2023,
      "authors": "Ori Yoran, Tomer Wolfson, Ori Ram, Jonathan Berant",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Retrieved documents are often lengthy and contain redundant information, directly inputting them into LRMs may disrupt the original coherence of reasoning and even introduce noise[62,72,26]"
        ]
      }
    },
    {
      "id": "2023-looped_transformers_as_programmable_computers",
      "type": "core",
      "title": "Looped transformers as programmable computers",
      "year": 2023,
      "authors": "Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D. Lee, Dimitris Papailiopoulos",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Goyal et al (2023)pretrained the model by randomly inserting a learnable<pause>tokens to the training corpus"
        ]
      }
    },
    {
      "id": "2023-longllmlingua:_accelerating_and_enhancing_llms_in_long_context_scenarios_via_prompt_compression",
      "type": "core",
      "title": "Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression",
      "year": 2023,
      "authors": "Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on"
        ]
      }
    },
    {
      "id": "2023-llama_2:_open_foundation_and_fine-tuned_chat_models",
      "type": "core",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "year": 2023,
      "authors": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "Through fine-tuning the LlaMA 2(Touvron et al2023b)model with this dataset, we have created RecLlama",
          "LlaMA-2-7B-chat,LlaMA-2-13B-chat(Touvron et al2023b): The second version of the LlaMA model released by Meta"
        ]
      }
    },
    {
      "id": "2023-let’s_verify_step_by_step",
      "type": "core",
      "title": "Let’s verify step by step",
      "year": 2023,
      "authors": "Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, Karl Cobbe",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "The follow-up work[27]expands on the concept of verifiers, integrating process-based reasoning mechanisms, and was followed by OpenAI’s work on Process Reward Models (PRMs)[12]",
          "A straightforward approach would be to label the reasoning steps manually by humans[27,12]"
        ],
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "(2)Math benchmarksincludeMATH500[38],AMC2023111https://huggingface"
        ]
      }
    },
    {
      "id": "2023-let_models_speak_ciphers:_multiagent_debate_through_embeddings",
      "type": "core",
      "title": "Let models speak ciphers: Multiagent debate through embeddings",
      "year": 2023,
      "authors": "Chau Pham, Boyi Liu, Yingxiang Yang, Zhengyu Chen, Tianyi Liu, Jianbo Yuan, Bryan A. Plummer, Zhaoran Wang, Hongxia Yang",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Goyal et al (2023)pretrained the model by randomly inserting a learnable<pause>tokens to the training corpus"
        ]
      }
    },
    {
      "id": "2023-judging_llm-as-a-judge_with_mt-bench_and_chatbot_arena",
      "type": "core",
      "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
      "year": 2023,
      "authors": "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "For the LlaMA and Vicuna models, we employ the FastChat(Zheng et al2023)package to establish local APIs, ensuring their usage is consistent with GPT-3"
        ]
      }
    },
    {
      "id": "2023-is_chatgpt_a_good_recommender?_a_preliminary_study",
      "type": "core",
      "title": "Is chatgpt a good recommender? a preliminary study",
      "year": 2023,
      "authors": "Junling Liu, Chao Liu, Peilin Zhou, Renjie Lv, Kang Zhou, Yan Zhang",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "The scaling-up of parameters and data has led to significant advancements in the capabilities of LLMs, including in-context learning(Brown et al2020; Liu et al2021; Rubin, Herzig, and Berant2021), instruction following(Ouyang et al2022; Touvron et al2023a; OpenAI2023), planning and reasoning(Wei et al2022; Wang et al2022a; Yao et al2022; Yang et al2023; Wang et al2023b)"
        ]
      }
    },
    {
      "id": "2023-implicit_chain_of_thought_reasoning_via_knowledge_distillation",
      "type": "core",
      "title": "Implicit chain of thought reasoning via knowledge distillation",
      "year": 2023,
      "authors": "Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, Stuart Shieber",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Goyal et al (2023)pretrained the model by randomly inserting a learnable<pause>tokens to the training corpus",
          "To train the model, we use a synthetic dataset generated byDeng et al (2023)"
        ]
      }
    },
    {
      "id": "2023-how_abilities_in_large_language_models_are_affected_by_supervised_fine-tuning_data_composition",
      "type": "core",
      "title": "How abilities in large language models are affected by supervised fine-tuning data composition",
      "year": 2023,
      "authors": "Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, Jingren Zhou",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "This focus has resulted in a degree of catastrophic forgetting in their general capabilities[39,10], ultimately limiting their long-context understanding of retrieved documents"
        ]
      }
    },
    {
      "id": "2023-guiding_language_model_reasoning_with_planning_tokens",
      "type": "core",
      "title": "Guiding language model reasoning with planning tokens",
      "year": 2023,
      "authors": "Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William Yang Wang, Alessandro Sordoni",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Goyal et al (2023)pretrained the model by randomly inserting a learnable<pause>tokens to the training corpus"
        ]
      }
    },
    {
      "id": "2023-graph_of_thoughts:_solving_elaborate_problems_with_large_language_models",
      "type": "core",
      "title": "Graph of thoughts: Solving elaborate problems with large language models",
      "year": 2023,
      "authors": "Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Torsten Hoefler",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "As models show emergent intelligence, researchers have started exploring the potential to leverage LLMs as autonomous agents(Wang et al2023a; Zhao, Jin, and Cheng2023), augmented with memory modules, planning ability, and tool-using capabilities"
        ]
      }
    },
    {
      "id": "2023-gpqa:_a_graduate-level_google-proof_q&a_benchmark",
      "type": "core",
      "title": "GPQA: A graduate-level google-proof q&a benchmark",
      "year": 2023,
      "authors": "David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R. Bowman",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "For all retrieval-based methods, following[52], we apply a back-off strategy where, when a final answer is not provided, we use the result from direct reasoning",
          "Performance comparison with human experts on the GPQA extended set[52]",
          "Challenging reasoning tasks:(1)GPQA[52]is a PhD-level science multiple-choice QA dataset"
        ]
      }
    },
    {
      "id": "2023-do_llms_understand_user_preferences?_evaluating_llms_on_user_rating_prediction",
      "type": "core",
      "title": "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction",
      "year": 2023,
      "authors": "Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan Hong, Ed Chi, Derek Zhiyuan Cheng",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "The scaling-up of parameters and data has led to significant advancements in the capabilities of LLMs, including in-context learning(Brown et al2020; Liu et al2021; Rubin, Herzig, and Berant2021), instruction following(Ouyang et al2022; Touvron et al2023a; OpenAI2023), planning and reasoning(Wei et al2022; Wang et al2022a; Yao et al2022; Yang et al2023; Wang et al2023b)"
        ]
      }
    },
    {
      "id": "2023-deductive_verification_of_chain-of-thought_reasoning",
      "type": "core",
      "title": "Deductive verification of chain-of-thought reasoning",
      "year": 2023,
      "authors": "Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "While this characteristic enhances logical coherence and interpretability of reasoning, an extended chain of thought may cause overthinking[4]and increased risks of knowledge insufficiency[60,51,2], where any knowledge gap can propagate errors and disrupt the entire reasoning chain[79,40,44,41]"
        ]
      }
    },
    {
      "id": "2023-critique_ability_of_large_language_models",
      "type": "core",
      "title": "Critique ability of large language models",
      "year": 2023,
      "authors": "Liangchen Luo, Zi Lin, Yinxiao Liu, Lei Shu, Yun Zhu, Jingbo Shang, Lei Meng",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "The paper[14]provides an empirical evaluation of LLMs’ ability to critique their own reasoning, showing that self-critique is often limited, and this capability often emerges only when models are sufficiently large"
        ]
      }
    },
    {
      "id": "2023-chat-rec:_towards_interactive_and_explainable_llms-augmented_recommender_system",
      "type": "core",
      "title": "Chat-rec: Towards interactive and explainable llms-augmented recommender system",
      "year": 2023,
      "authors": "Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, Jiawei Zhang",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "Chat-Rec(Gao et al2023b): A recently proposed conversational recommendation agent utilizes a text-embedding tool (OpenAI text-embedding-ada-002) to retrieve candidates"
        ]
      }
    },
    {
      "id": "2023-augmenting_language_models_with_long-term_memory",
      "type": "core",
      "title": "Augmenting Language Models with Long-Term Memory",
      "year": 2023,
      "authors": "Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, Furu Wei",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "For example,(Wang et al2023c; Zhong et al2023; Liu et al2023b)have equipped LLMs with an external memory, empowering LLMs with growth potential Regarding the planning, CoT(Wei et al2022; Kojima et al2022)and ReAct(Yao et al2022)propose to enhance planning by step-wise reasoning; ToT(Yao et al2023)and GoT(Besta et al2023)introduce multi-path reasoning to ensure consistency and correctness; Self-Refine(Madaan et al2023)and Reflexion(Shinn et al2023)lead the LLMs to reflect on errors, with the ultimate goal of improving their subsequent problem-solving success rates"
        ]
      }
    },
    {
      "id": "2023-an_in-depth_survey_of_large_language_model-based_artificial_intelligence_agents",
      "type": "core",
      "title": "An in-depth survey of large language model-based artificial intelligence agents",
      "year": 2023,
      "authors": "Pengyu Zhao, Zijian Jin, Ning Cheng",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "As models show emergent intelligence, researchers have started exploring the potential to leverage LLMs as autonomous agents(Wang et al2023a; Zhao, Jin, and Cheng2023), augmented with memory modules, planning ability, and tool-using capabilities"
        ]
      }
    },
    {
      "id": "2023-alphazero-like_tree-search_can_guide_large_language_model_decoding_and_training",
      "type": "core",
      "title": "Alphazero-like tree-search can guide large language model decoding and training",
      "year": 2023,
      "authors": "Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, Jun Wang",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "MCTS has been used for acquiring data in[6], whereas it has been extended with linear search for efficiency in[15]",
          "For longer reasoning sequences, techniques such as Monte Carlo Tree Search (MCTS)[6,15]are employed to guide the LLM policy to find correct reasoning steps efficiently in a more fine-grained manner",
          "MCTS[6]simulates multiple reasoning paths and evaluates them based on a reward system, selecting the one with the highest expected reward",
          "However, an alternative approach involves viewing the PRM as a value function that can be trained via a value iteration method, enabling it to predict cumulative rewards and guide the reasoning process through optimal action selection[6]",
          "Indeed, there is a need to implement sophisticated model-based strategies akin to Monte Carlo Tree Search (MCTS) witnin the inference and decoding stage[6]",
          "For instance, the paper[6]introduces a method that integrates Monte Carlo Tree Search (MCTS) with LLM decoding, a combination that has proven highly effective in guiding reasoning, particularly for complex, multi-step tasks"
        ]
      }
    },
    {
      "id": "2023-a_survey_on_large_language_model_based_autonomous_agents",
      "type": "core",
      "title": "A survey on large language model based autonomous agents",
      "year": 2023,
      "authors": "Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Ji-Rong Wen",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "As models show emergent intelligence, researchers have started exploring the potential to leverage LLMs as autonomous agents(Wang et al2023a; Zhao, Jin, and Cheng2023), augmented with memory modules, planning ability, and tool-using capabilities"
        ]
      }
    },
    {
      "id": "2022-towards_unified_conversational_recommender_systems_via_knowledge-enhanced_prompt_learning",
      "type": "core",
      "title": "Towards unified conversational recommender systems via knowledge-enhanced prompt learning",
      "year": 2022,
      "authors": "Xiaolei Wang, Kun Zhou, Ji-Rong Wen, Wayne Xin Zhao",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "Existing researches in conversational recommender systems (CRS) can be primarily categorized into two main areas(Gao et al2021): attribute-based question-answering(Zou and Kanoulas2019; Zou, Chen, and Kanoulas2020; Xu et al2021)and open-ended conversation(Li et al2018; Wang et al2022b,2021)"
        ]
      }
    },
    {
      "id": "2022-towards_understanding_chain-of-thought_prompting:_an_empirical_study_of_what_matters",
      "type": "core",
      "title": "Towards understanding chain-of-thought prompting: An empirical study of what matters",
      "year": 2022,
      "authors": "Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "This phenomenon is known as the unfaithfulness of CoT reasoning(Wang et al,2022; Turpin et al,2024)"
        ]
      }
    },
    {
      "id": "2022-text_and_patterns:_for_effective_chain_of_thought_it_takes_two_to_tango",
      "type": "core",
      "title": "Text and patterns: For effective chain of thought, it takes two to tango",
      "year": 2022,
      "authors": "Aman Madaan, Amir Yazdanbakhsh",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "This includes prompting LLMs(Wei et al,2022; Khot et al,2022; Zhou et al,2022), or training LLMs to generate reasoning chains, either with supervised finetuning(Yue et al,2023; Yu et al,2023)or reinforcement learning(Wang et al,2024; Havrilla et al,2024; Shao et al,2024; Yu et al,2024a)",
          "While previous work has attempted to fix these problems by prompting LLMs to generate succinct reasoning chains(Madaan and Yazdanbakhsh,2022), or performing additional reasoning before generating some critical tokens(Zelikman et al,2024), these solutions remain constrained within the language space and do not solve the fundamental problems"
        ]
      }
    },
    {
      "id": "2022-star:_bootstrapping_reasoning_with_reasoning",
      "type": "core",
      "title": "Star: Bootstrapping reasoning with reasoning",
      "year": 2022,
      "authors": "Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D. Goodman",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "The acquisition of reasoning data has been another area of focus, particularly in papers like[34], which explores methods for automatically obtaining data related to reasoning steps",
          "However, a particularly effective method for collecting data and improving LLM reasoning without requiring human supervision is the Self-Taught Reasoner (STaR) technique[34], among others"
        ]
      }
    },
    {
      "id": "2022-solving_math_word_problems_with_process-and_outcome-based_feedback",
      "type": "core",
      "title": "Solving math word problems with process-and outcome-based feedback",
      "year": 2022,
      "authors": "Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "The follow-up work[27]expands on the concept of verifiers, integrating process-based reasoning mechanisms, and was followed by OpenAI’s work on Process Reward Models (PRMs)[12]",
          "A straightforward approach would be to label the reasoning steps manually by humans[27,12]"
        ]
      }
    },
    {
      "id": "2022-react:_synergizing_reasoning_and_acting_in_language_models",
      "type": "core",
      "title": "React: Synergizing reasoning and acting in language models",
      "year": 2022,
      "authors": "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "The scaling-up of parameters and data has led to significant advancements in the capabilities of LLMs, including in-context learning(Brown et al2020; Liu et al2021; Rubin, Herzig, and Berant2021), instruction following(Ouyang et al2022; Touvron et al2023a; OpenAI2023), planning and reasoning(Wei et al2022; Wang et al2022a; Yao et al2022; Yang et al2023; Wang et al2023b)",
          "For example,(Wang et al2023c; Zhong et al2023; Liu et al2023b)have equipped LLMs with an external memory, empowering LLMs with growth potential Regarding the planning, CoT(Wei et al2022; Kojima et al2022)and ReAct(Yao et al2022)propose to enhance planning by step-wise reasoning; ToT(Yao et al2023)and GoT(Besta et al2023)introduce multi-path reasoning to ensure consistency and correctness; Self-Refine(Madaan et al2023)and Reflexion(Shinn et al2023)lead the LLMs to reflect on errors, with the ultimate goal of improving their subsequent problem-solving success rates",
          "Typically, the tool augmentation is implemented via ReAct(Yao et al2022), where LLMs generate reasoning traces, actions, and observations in an interleaved manner"
        ],
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Recently, agentic RAG systems empower models to autonomously determine when and what knowledge to retrieve as needed, showcasing enhanced planning and problem-solving capabilities[5,56,70]",
          "To manage the length of retrieved documents, inspired by ReAct[70], we first retrieve the top-10 snippets during reasoning"
        ]
      }
    },
    {
      "id": "2022-program_of_thoughts_prompting:_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks",
      "type": "core",
      "title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
      "year": 2022,
      "authors": "Wenhu Chen, Xueguang Ma, Xinyi Wang, William W. Cohen",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "For example,(Wang et al2023c; Zhong et al2023; Liu et al2023b)have equipped LLMs with an external memory, empowering LLMs with growth potential Regarding the planning, CoT(Wei et al2022; Kojima et al2022)and ReAct(Yao et al2022)propose to enhance planning by step-wise reasoning; ToT(Yao et al2023)and GoT(Besta et al2023)introduce multi-path reasoning to ensure consistency and correctness; Self-Refine(Madaan et al2023)and Reflexion(Shinn et al2023)lead the LLMs to reflect on errors, with the ultimate goal of improving their subsequent problem-solving success rates"
        ]
      }
    },
    {
      "id": "2022-measuring_and_narrowing_the_compositionality_gap_in_language_models",
      "type": "core",
      "title": "Measuring and narrowing the compositionality gap in language models",
      "year": 2022,
      "authors": "Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, Mike Lewis",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Bamboogle[47]collects complex questions that Google answers incorrectly to evaluate models’ compositional reasoning across various domains"
        ]
      }
    },
    {
      "id": "2022-making_large_language_models_better_reasoners_with_step-aware_verifier",
      "type": "core",
      "title": "Making large language models better reasoners with step-aware verifier",
      "year": 2022,
      "authors": "Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "Proposed methods range from collecting specialised training data to building reward models[18,11,15]and increasing the computational complexity of decoding[24,33], but none have yet achieved significant performance breakthroughs at scale",
          "A more recent addition to this line of research is[11], which combines verifier models with majority voting to produce more reliable outputs in reasoning tasks"
        ]
      }
    },
    {
      "id": "2022-least-to-most_prompting_enables_complex_reasoning_in_large_language_models",
      "type": "core",
      "title": "Least-to-most prompting enables complex reasoning in large language models",
      "year": 2022,
      "authors": "Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, Ed Chi",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "This includes prompting LLMs(Wei et al,2022; Khot et al,2022; Zhou et al,2022), or training LLMs to generate reasoning chains, either with supervised finetuning(Yue et al,2023; Yu et al,2023)or reinforcement learning(Wang et al,2024; Havrilla et al,2024; Shao et al,2024; Yu et al,2024a)"
        ]
      }
    },
    {
      "id": "2022-large_language_models_are_zero-shot_reasoners",
      "type": "core",
      "title": "Large language models are zero-shot reasoners",
      "year": 2022,
      "authors": "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "For example,(Wang et al2023c; Zhong et al2023; Liu et al2023b)have equipped LLMs with an external memory, empowering LLMs with growth potential Regarding the planning, CoT(Wei et al2022; Kojima et al2022)and ReAct(Yao et al2022)propose to enhance planning by step-wise reasoning; ToT(Yao et al2023)and GoT(Besta et al2023)introduce multi-path reasoning to ensure consistency and correctness; Self-Refine(Madaan et al2023)and Reflexion(Shinn et al2023)lead the LLMs to reflect on errors, with the ultimate goal of improving their subsequent problem-solving success rates"
        ]
      }
    },
    {
      "id": "2022-language_models_are_greedy_reasoners:_a_systematic_formal_analysis_of_chain-of-thought",
      "type": "core",
      "title": "Language models are greedy reasoners: A systematic formal analysis of chain-of-thought",
      "year": 2022,
      "authors": "Abulhair Saparov, He He",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Unlike previous logical reasoning datasets like ProntoQA(Saparov and He,2022), ProsQA’s DAG structure introduces complex exploration paths, making it particularly challenging for models to identify the correct reasoning chain",
          "On logical reasoning including ProntoQA(Saparov and He,2022), and our newly proposed ProsQA (Section4) which requires stronger planning ability,Coconutand some of its variants even surpasses language-based CoT methods, while generating significantly fewer tokens during inference",
          "We use the ProntoQA(Saparov and He,2022)dataset, and our newly proposed ProsQA dataset, which is more challenging due to more distracting branches",
          "To construct the dataset, we first compile a set of typical entity names, such as “Alex” and “Jack,” along with fictional concept names like “lorpus” and “rorpus,” following the setting of ProntoQA(Saparov and He,2022)"
        ]
      }
    },
    {
      "id": "2022-decomposed_prompting:_a_modular_approach_for_solving_complex_tasks",
      "type": "core",
      "title": "Decomposed prompting: A modular approach for solving complex tasks",
      "year": 2022,
      "authors": "Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "This includes prompting LLMs(Wei et al,2022; Khot et al,2022; Zhou et al,2022), or training LLMs to generate reasoning chains, either with supervised finetuning(Yue et al,2023; Yu et al,2023)or reinforcement learning(Wang et al,2024; Havrilla et al,2024; Shao et al,2024; Yu et al,2024a)"
        ]
      }
    },
    {
      "id": "2022-comparison-based_conversational_recommender_system_with_relative_bandit_feedback",
      "type": "core",
      "title": "Comparison-based conversational recommender system with relative bandit feedback",
      "year": 2022,
      "authors": "Zhihui Xie, Tong Yu, Canzhe Zhao, Shuai Li",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "Existing researches in conversational recommender systems (CRS) can be primarily categorized into two main areas(Gao et al2021): attribute-based question-answering(Zou and Kanoulas2019; Zou, Chen, and Kanoulas2020; Xu et al2021)and open-ended conversation(Li et al2018; Wang et al2022b,2021)"
        ]
      }
    },
    {
      "id": "2022-chain_of_thought_prompting_elicits_reasoning_in_large_language_models",
      "type": "core",
      "title": "Chain of thought prompting elicits reasoning in large language models",
      "year": 2022,
      "authors": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "1) and subsequently improve problem-solving, especially in tasks like math and coding[32,16]",
          "However, the ”chain of thoughts” concept offers a potential mitigation to this constraint[32]"
        ]
      }
    },
    {
      "id": "2022-chain-of-thought_prompting_elicits_reasoning_in_large_language_models",
      "type": "core",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "year": 2022,
      "authors": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "The scaling-up of parameters and data has led to significant advancements in the capabilities of LLMs, including in-context learning(Brown et al2020; Liu et al2021; Rubin, Herzig, and Berant2021), instruction following(Ouyang et al2022; Touvron et al2023a; OpenAI2023), planning and reasoning(Wei et al2022; Wang et al2022a; Yao et al2022; Yang et al2023; Wang et al2023b)",
          "For example,(Wang et al2023c; Zhong et al2023; Liu et al2023b)have equipped LLMs with an external memory, empowering LLMs with growth potential Regarding the planning, CoT(Wei et al2022; Kojima et al2022)and ReAct(Yao et al2022)propose to enhance planning by step-wise reasoning; ToT(Yao et al2023)and GoT(Besta et al2023)introduce multi-path reasoning to ensure consistency and correctness; Self-Refine(Madaan et al2023)and Reflexion(Shinn et al2023)lead the LLMs to reflect on errors, with the ultimate goal of improving their subsequent problem-solving success rates"
        ],
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Recently, models like OpenAI-o1[22], Qwen-QwQ[54]and DeepSeek-R1[7]explicitly demonstrate chain-of-thought reasoning[59], mimicking human problem-solving approaches in domains such as mathematics, coding, and so on",
          "Recently emerged large reasoning models (LRMs), exemplified by OpenAI’s o1[22], Qwen-QwQ[54]and DeepSeek-R1[7], employ large-scale reinforcement learning foster impressive long-sequence stepwise reasoning capabilities, offering promising solutions to complex reasoning problems[46,31,59,84,73,74,67]",
          "For baseline models not specifically trained for o1-like reasoning, we apply Chain-of-Thought (CoT)[59]prompting to perform reasoning before generating answers"
        ],
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "This includes prompting LLMs(Wei et al,2022; Khot et al,2022; Zhou et al,2022), or training LLMs to generate reasoning chains, either with supervised finetuning(Yue et al,2023; Yu et al,2023)or reinforcement learning(Wang et al,2024; Havrilla et al,2024; Shao et al,2024; Yu et al,2024a)",
          "For example, a prevalent approach, known as chain-of-thought (CoT) reasoning(Wei et al,2022), involves prompting or training LLMs to generate solutions step-by-step using natural language"
        ]
      }
    },
    {
      "id": "2022-blenderbot_3:_a_deployed_conversational_agent_that_continually_learns_to_responsibly_engage",
      "type": "core",
      "title": "Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage",
      "year": 2022,
      "authors": "Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, Jason Weston",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "For example,(Wang et al2023c; Zhong et al2023; Liu et al2023b)have equipped LLMs with an external memory, empowering LLMs with growth potential Regarding the planning, CoT(Wei et al2022; Kojima et al2022)and ReAct(Yao et al2022)propose to enhance planning by step-wise reasoning; ToT(Yao et al2023)and GoT(Besta et al2023)introduce multi-path reasoning to ensure consistency and correctness; Self-Refine(Madaan et al2023)and Reflexion(Shinn et al2023)lead the LLMs to reflect on errors, with the ultimate goal of improving their subsequent problem-solving success rates"
        ]
      }
    },
    {
      "id": "2022-barcor:_towards_a_unified_framework_for_conversational_recommendation_systems",
      "type": "core",
      "title": "BARCOR: Towards A Unified Framework for Conversational Recommendation Systems",
      "year": 2022,
      "authors": "Ting-Chun Wang, Shang-Yu Su, Yun-Nung Chen",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "Existing researches in conversational recommender systems (CRS) can be primarily categorized into two main areas(Gao et al2021): attribute-based question-answering(Zou and Kanoulas2019; Zou, Chen, and Kanoulas2020; Xu et al2021)and open-ended conversation(Li et al2018; Wang et al2022b,2021)"
        ]
      }
    },
    {
      "id": "2022-a_path_towards_autonomous_machine_intelligence_version_0.9._2_2022-06-27",
      "type": "core",
      "title": "A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27",
      "year": "2022",
      "authors": "Yann LeCun",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "This includes prompting LLMs(Wei et al,2022; Khot et al,2022; Zhou et al,2022), or training LLMs to generate reasoning chains, either with supervised finetuning(Yue et al,2023; Yu et al,2023)or reinforcement learning(Wang et al,2024; Havrilla et al,2024; Shao et al,2024; Yu et al,2024a)"
        ]
      }
    },
    {
      "id": "2021-♫_musique:_multihop_questions_via_single-hop_question_composition",
      "type": "core",
      "title": "♫ musique: Multihop questions via single-hop question composition",
      "year": 2021,
      "authors": "Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "MuSiQue[55]features 2-4 hop questions built from five existing single-hop datasets"
        ]
      }
    },
    {
      "id": "2021-webgpt:_browser-assisted_question-answering_with_human_feedback",
      "type": "core",
      "title": "Webgpt: Browser-assisted question-answering with human feedback",
      "year": 2021,
      "authors": "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "To possess domain-specific skills, some works(Qin et al2023a)study guiding LLMs to use external tools, such as a web search engine(Nakano et al2021; Shuster et al2022), mathematical tools(Schick et al2023; Thoppilan et al2022), code interpreters(Gao et al2023a; Chen et al2022)and visual models(Wu et al2023; Shen et al2023)"
        ]
      }
    },
    {
      "id": "2021-show_your_work:_scratchpads_for_intermediate_computation_with_language_models",
      "type": "core",
      "title": "Show your work: Scratchpads for intermediate computation with language models",
      "year": 2021,
      "authors": "Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, Augustus Odena",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "1) and subsequently improve problem-solving, especially in tasks like math and coding[32,16]"
        ]
      }
    },
    {
      "id": "2021-recindial:_a_unified_framework_for_conversational_recommendation_with_pretrained_language_models",
      "type": "core",
      "title": "Recindial: A unified framework for conversational recommendation with pretrained language models",
      "year": 2021,
      "authors": "Lingzhi Wang, Huang Hu, Lei Sha, Can Xu, Kam-Fai Wong, Daxin Jiang",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "Existing researches in conversational recommender systems (CRS) can be primarily categorized into two main areas(Gao et al2021): attribute-based question-answering(Zou and Kanoulas2019; Zou, Chen, and Kanoulas2020; Xu et al2021)and open-ended conversation(Li et al2018; Wang et al2022b,2021)"
        ]
      }
    },
    {
      "id": "2021-learning_to_retrieve_prompts_for_in-context_learning",
      "type": "core",
      "title": "Learning to retrieve prompts for in-context learning",
      "year": 2021,
      "authors": "Ohad Rubin, Jonathan Herzig, Jonathan Berant",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "The scaling-up of parameters and data has led to significant advancements in the capabilities of LLMs, including in-context learning(Brown et al2020; Liu et al2021; Rubin, Herzig, and Berant2021), instruction following(Ouyang et al2022; Touvron et al2023a; OpenAI2023), planning and reasoning(Wei et al2022; Wang et al2022a; Yao et al2022; Yang et al2023; Wang et al2023b)"
        ]
      }
    },
    {
      "id": "2021-advances_and_challenges_in_conversational_recommender_systems:_a_survey",
      "type": "core",
      "title": "Advances and challenges in conversational recommender systems: A survey",
      "year": 2021,
      "authors": "Chongming Gao, Wenqiang Lei, Xiangnan He, Maarten de Rijke, Tat-Seng Chua",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "Existing researches in conversational recommender systems (CRS) can be primarily categorized into two main areas(Gao et al2021): attribute-based question-answering(Zou and Kanoulas2019; Zou, Chen, and Kanoulas2020; Xu et al2021)and open-ended conversation(Li et al2018; Wang et al2022b,2021)"
        ]
      }
    },
    {
      "id": "2021-addressing_deep_learning_model_uncertainty_in_long-range_climate_forecasting_with_late_fusion",
      "type": "core",
      "title": "Addressing deep learning model uncertainty in long-range climate forecasting with late fusion",
      "year": 2021,
      "authors": "Ken C. L. Wong, Hongzhi Wang, Etienne E. Vos, Bianca Zadrozny, Campbell D. Watson, Tanveer Syeda-Mahmood",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "While this characteristic enhances logical coherence and interpretability of reasoning, an extended chain of thought may cause overthinking[4]and increased risks of knowledge insufficiency[60,51,2], where any knowledge gap can propagate errors and disrupt the entire reasoning chain[79,40,44,41]"
        ]
      }
    },
    {
      "id": "2020-what_does_bert_know_about_books_movies_and_music?_probing_bert_for_conversational_recommendation",
      "type": "core",
      "title": "What does bert know about books, movies and music? probing bert for conversational recommendation",
      "year": 2020,
      "authors": "Gustavo Penha, Claudia Hauff",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "Existing researches in conversational recommender systems (CRS) can be primarily categorized into two main areas(Gao et al2021): attribute-based question-answering(Zou and Kanoulas2019; Zou, Chen, and Kanoulas2020; Xu et al2021)and open-ended conversation(Li et al2018; Wang et al2022b,2021)"
        ]
      }
    },
    {
      "id": "2020-towards_question-based_recommender_systems",
      "type": "core",
      "title": "Towards question-based recommender systems",
      "year": 2020,
      "authors": "Jie Zou, Yifan Chen, Evangelos Kanoulas",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "Existing researches in conversational recommender systems (CRS) can be primarily categorized into two main areas(Gao et al2021): attribute-based question-answering(Zou and Kanoulas2019; Zou, Chen, and Kanoulas2020; Xu et al2021)and open-ended conversation(Li et al2018; Wang et al2022b,2021)"
        ]
      }
    },
    {
      "id": "2020-thinking_fast_and_slow",
      "type": "core",
      "title": "Thinking, Fast and Slow",
      "year": 2020,
      "authors": "Grady Booch, Francesco Fabiano, Lior Horesh, Kiran Kate, Jon Lenchner, Nick Linck, Andrea Loreggia, Keerthiram Murugesan, Nicholas Mattei, Francesca Rossi, Biplav Srivastava",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "This direction, which we refer to as LLM-Native Chain-of-Thought (NativeCoT), should be able to inherently mirror the deliberate, analytical process possessed by human’s System 2 thinking[8]",
          "Interestingly, in human cognition, two correlated yet distinct modes of cognitive processing are presented to guide human decision-making and behaviours[8], each of which has partially distinction brain circuits and neural pathways ( Fig"
        ],
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "It is noteworthy that o1-like reasoning patterns guide LRMs to engage in a slower thinking process[6,61]by implicitly breaking down complex problems, generating a long internal reasoning chain and then discovering suitable solutions step by step"
        ]
      }
    },
    {
      "id": "2020-retrieval-augmented_generation_for_knowledge-intensive_nlp_tasks",
      "type": "core",
      "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
      "year": 2020,
      "authors": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Retrieval-augmented generation (RAG) introduces retrieval mechanisms to address the limitations of static parameters in generative models, allowing access to external knowledge to solve more complex problems[30,82,35,86]"
        ]
      }
    },
    {
      "id": "2020-offline_reinforcement_learning:_tutorial_review_and_perspectives_on_open_problems",
      "type": "core",
      "title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems",
      "year": 2020,
      "authors": "Sergey Levine, Aviral Kumar, George Tucker, Justin Fu",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "This phenomenon can be characterised as what we called an ”intelligence upper bound,” a concept that can be rigorously derived from recent research in offline reinforcement learning and imitation learning[10]"
        ]
      }
    },
    {
      "id": "2020-limitations_of_autoregressive_models_and_their_alternatives",
      "type": "core",
      "title": "Limitations of autoregressive models and their alternatives",
      "year": 2020,
      "authors": "Chu-Cheng Lin, Aaron Jaech, Xin Li, Matthew R. Gormley, Jason Eisner",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "The second challenge, from a computational complexity perspective, is that Large Language Models (LLMs) inherently operate within the constraints of quadratic computational complexity[13]"
        ]
      }
    },
    {
      "id": "2020-constructing_a_multi-hop_qa_dataset_for_comprehensive_evaluation_of_reasoning_steps",
      "type": "core",
      "title": "Constructing A multi-hop QA dataset for comprehensive evaluation of reasoning steps",
      "year": 2020,
      "authors": "Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, Akiko Aizawa",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "2WikiMultihopQA (2WIKI)[18]provides explicit reasoning paths for multi-hop questions"
        ]
      }
    },
    {
      "id": "2019-xlnet:_generalized_autoregressive_pretraining_for_language_understanding",
      "type": "core",
      "title": "XLNet: Generalized autoregressive pretraining for language understanding",
      "year": 2019,
      "authors": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2019-well-read_students_learn_better:_the_impact_of_student_initialization_on_knowledge_distillation",
      "type": "core",
      "title": "Well-read students learn better: The impact of student initialization on knowledge distillation",
      "year": "2019",
      "authors": "Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2019-towards_knowledge-based_recommender_dialog_system",
      "type": "core",
      "title": "Towards knowledge-based recommender dialog system",
      "year": 2019,
      "authors": "Qibin Chen, Junyang Lin, Yichang Zhang, Ming Ding, Yukuo Cen, Hongxia Yang, Jie Tang",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "Existing researches in conversational recommender systems (CRS) can be primarily categorized into two main areas(Gao et al2021): attribute-based question-answering(Zou and Kanoulas2019; Zou, Chen, and Kanoulas2020; Xu et al2021)and open-ended conversation(Li et al2018; Wang et al2022b,2021)"
        ]
      }
    },
    {
      "id": "2019-structbert:_incorporating_language_structures_into_pre-training_for_deep_language_understanding",
      "type": "core",
      "title": "StructBERT: Incorporating language structures into pre-training for deep language understanding",
      "year": 2019,
      "authors": "Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Jiangnan Xia, Liwei Peng, Luo Si",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2019-stochastic_shared_embeddings:_data-driven_regularization_of_embedding_layers",
      "type": "core",
      "title": "Stochastic shared embeddings: Data-driven regularization of embedding layers",
      "year": "2019",
      "authors": "Liwei Wu, Shuqing Li, Cho-Jui Hsieh, James Sharpnack",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Specifically, SSE-SE replaces one embedding with another embedding stochastically with probabilityp𝑝p, which is called SSE probability in[41]",
          "Very recently, a new regularization technique called Stochastic Shared Embeddings (SSE)[41]is proposed as a new means of regularizing embedding layers",
          "Unlike previous SASRec model[16], we use one more regularization technique in our SSE-PT model specifically for embedding layer in addition to the ones listed earlier: the Stochastic Shared Embeddings (SSE)[41]",
          "Although introducing user embeddings into the model is indeed difficult with existing regularization techniques for embeddings, we show that personalization can greatly improve ranking performances with recent regularization technique called Stochastic Shared Embeddings (SSE)[41]",
          "SASRec paper[16]also does not utilize SSE[41]for further regularization: only dropout and weight decay are used"
        ]
      }
    },
    {
      "id": "2019-spanbert:_improving_pre-training_by_representing_and_predicting_spans",
      "type": "core",
      "title": "SpanBERT: Improving pre-training by representing and predicting spans",
      "year": 2019,
      "authors": "Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, Omer Levy",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2019-roberta:_a_robustly_optimized_bert_pretraining_approach",
      "type": "core",
      "title": "RoBERTa: A robustly optimized BERT pretraining approach",
      "year": 2019,
      "authors": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2019-reducing_bert_pre-training_time_from_3_days_to_76_minutes",
      "type": "core",
      "title": "Reducing BERT pre-training time from 3 days to 76 minutes",
      "year": "2019",
      "authors": "Yang You, Jing Li, Jonathan Hseu, Xiaodan Song, James Demmel, and Cho-Jui Hsieh",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2019-patient_knowledge_distillation_for_bert_model_compression",
      "type": "core",
      "title": "Patient knowledge distillation for BERT model compression",
      "year": 2019,
      "authors": "Siqi Sun, Yu Cheng, Zhe Gan, Jingjing Liu",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2019-natural_questions:_a_benchmark_for_question_answering_research",
      "type": "core",
      "title": "Natural questions: a benchmark for question answering research",
      "year": "2019",
      "authors": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al",
      "name": null,
      "parents": {
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Open-domain QA tasks:(1)Single-hop QA datasets:Natural Questions (NQ)[29]contains questions from real Google search queries with answers from Wikipedia articles"
        ]
      }
    },
    {
      "id": "2019-modeling_recurrence_for_transformer",
      "type": "core",
      "title": "Modeling recurrence for transformer",
      "year": 2019,
      "authors": "Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, Zhaopeng Tu",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2019-megatron-lm:_training_multi-billion_parameter_language_models_using_model_parallelism_2019",
      "type": "core",
      "title": "Megatron-LM: Training multi-billion parameter language models using model parallelism, 2019",
      "year": "2019",
      "authors": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2019-learning_to_ask:_question-based_sequential_bayesian_product_search",
      "type": "core",
      "title": "Learning to ask: Question-based sequential Bayesian product search",
      "year": 2019,
      "authors": "Jie Zou, Evangelos Kanoulas",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "Existing researches in conversational recommender systems (CRS) can be primarily categorized into two main areas(Gao et al2021): attribute-based question-answering(Zou and Kanoulas2019; Zou, Chen, and Kanoulas2020; Xu et al2021)and open-ended conversation(Li et al2018; Wang et al2022b,2021)"
        ]
      }
    },
    {
      "id": "2019-graph_dna:_deep_neighborhood_aware_graph_encoding_for_collaborative_filtering",
      "type": "core",
      "title": "Graph dna: Deep neighborhood aware graph encoding for collaborative filtering",
      "year": "2019",
      "authors": "Liwei Wu, Hsiang-Fu Yu, Nikhil Rao, James Sharpnack, Cho-Jui Hsieh",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "These relationships can also be viewed as graphs[42]"
        ]
      }
    },
    {
      "id": "2019-generating_long_sequences_with_sparse_transformers",
      "type": "core",
      "title": "Generating long sequences with sparse transformers",
      "year": 2019,
      "authors": "Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2019-efficient_training_of_bert_by_progressively_stacking",
      "type": "core",
      "title": "Efficient training of bert by progressively stacking",
      "year": "2019",
      "authors": "Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2019-dissent:_learning_sentence_representations_from_explicit_discourse_relations",
      "type": "core",
      "title": "DisSent: Learning sentence representations from explicit discourse relations",
      "year": "2019",
      "authors": "Allen Nie, Erin Bennett, and Noah Goodman",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2019-deep_learning_based_recommender_system:_a_survey_and_new_perspectives",
      "type": "core",
      "title": "Deep learning based recommender system: A survey and new perspectives",
      "year": "2019",
      "authors": "Shuai Zhang, Lina Yao, Aixin Sun, Yi Tay",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Although personalization is not needed for the original Transformer model[36]in natural languages understandings or translations, personalization plays a crucial role throughout recommender system literature[43]ever since the matrix factorization approach to the Netflix prize[19]"
        ]
      }
    },
    {
      "id": "2019-deep_equilibrium_models",
      "type": "core",
      "title": "Deep equilibrium models",
      "year": 2019,
      "authors": "Shaojie Bai, J. Zico Kolter, Vladlen Koltun",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2019-dcmn+:_dual_co-matching_network_for_multi-choice_reading_comprehension",
      "type": "core",
      "title": "DCMN+: Dual co-matching network for multi-choice reading comprehension",
      "year": 2019,
      "authors": "Shuailiang Zhang, Hai Zhao, Yuwei Wu, Zhuosheng Zhang, Xi Zhou, Xiang Zhou",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2019-bam!_born-again_multi-task_networks_for_natural_language_understanding",
      "type": "core",
      "title": "Bam! born-again multi-task networks for natural language understanding",
      "year": 2019,
      "authors": "Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D. Manning, Quoc V. Le",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2019-a_distinct_cortical_network_for_mathematical_knowledge_in_the_human_brain",
      "type": "core",
      "title": "A distinct cortical network for mathematical knowledge in the human brain",
      "year": "2019",
      "authors": "Marie Amalric and Stanislas Dehaene",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Neuroimaging studies have consistently shown that the language network – a set of brain regions responsible for language comprehension and production – remains largely inactive during various reasoning tasks(Amalric and Dehaene,2019; Monti et al,2012,2007,2009; Fedorenko et al,2011)"
        ]
      }
    },
    {
      "id": "2018-universal_transformers",
      "type": "core",
      "title": "Universal transformers",
      "year": 2018,
      "authors": "Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, Łukasz Kaiser",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2018-understanding_the_disharmony_between_dropout_and_batch_normalization_by_variance_shift",
      "type": "core",
      "title": "Understanding the disharmony between dropout and batch normalization by variance shift",
      "year": 2018,
      "authors": "Xiang Li, Shuo Chen, Xiaolin Hu, Jian Yang",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2018-towards_deep_conversational_recommendations",
      "type": "core",
      "title": "Towards deep conversational recommendations",
      "year": 2018,
      "authors": "Raymond Li, Samira Kahou, Hannes Schulz, Vincent Michalski, Laurent Charlin, Chris Pal",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "Following the settings of traditional conversational recommender systems on ReDial(Li et al2018), we also adopt the one-turn recommendation strategy",
          "Existing researches in conversational recommender systems (CRS) can be primarily categorized into two main areas(Gao et al2021): attribute-based question-answering(Zou and Kanoulas2019; Zou, Chen, and Kanoulas2020; Xu et al2021)and open-ended conversation(Li et al2018; Wang et al2022b,2021)"
        ]
      }
    },
    {
      "id": "2018-stamp:_short-term_attention/memory_priority_model_for_session-based_recommendation",
      "type": "core",
      "title": "Stamp: short-term attention/memory priority model for session-based recommendation",
      "year": "2018",
      "authors": "Qiao Liu, Yifu Zeng, Refuoe Mokhosi, and Haibin Zhang",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "STAMP: a session-based recommendation algorithm[23]using attention mechanism",
          "Later on, attention models are getting more and more attention in both NLP,[36,4], and recommender systems,[23,16]"
        ]
      }
    },
    {
      "id": "2018-sql-rank:_a_listwise_approach_to_collaborative_ranking",
      "type": "core",
      "title": "Sql-rank: A listwise approach to collaborative ranking",
      "year": "2018",
      "authors": "Liwei Wu, Cho-Jui Hsieh, James Sharpnack",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Listwise methods[40], on the other hand, consider a user’s entire engagement history as independent observations",
          "SQL-Rank[40]and item-based recommendations[28]are omitted because the former is similar to BPR[26]except using the listwise loss function instead of the pairwise loss function and the latter has been shown inferior to TransRec[8]"
        ]
      }
    },
    {
      "id": "2018-self-attentive_sequential_recommendation",
      "type": "core",
      "title": "Self-Attentive Sequential Recommendation",
      "year": "2018",
      "authors": null,
      "name": "SASRec",
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "We use 5 datasets, the first 4 of them have exactly the same train/dev/test splits as in[16]:",
          "Later on, attention models are getting more and more attention in both NLP,[36,4], and recommender systems,[23,16]",
          "Unlike previous SASRec model[16], we use one more regularization technique in our SSE-PT model specifically for embedding layer in addition to the ones listed earlier: the Stochastic Shared Embeddings (SSE)[41]",
          "[16]found that adding additional personalized embeddings did not improve the performance of their Transformer model, and postulate that this is due to the fact that they already use the user history and the embeddings only contribute to overfitting",
          "In particular, the SASRec model[16], inspired by the popular Transformer model in natural languages processing, has achieved state-of-art results in the temporal collaborative ranking problem and enjoyed more than 10x speed-up compared to earlier RNN[10]/ CNN[34]-based methods",
          "On most datasets, our SSE-PT improves NDCG by more than 4% when compared with SASRec[16]and more than 20% when compared to non-deep-learning methods",
          "In[16], it has been shown that SASRec is about 11 times faster than Caser and 17 times faster than GRU4Rec+and achieves much better NDCG@10 results so we did not include Caser and GRU4Rec+in our comparisons",
          "Our model is motivated by the Transformer model in[36]and[16]",
          "Note that the main difference between our model and[16]is that we introduce the user embeddingsuisubscript𝑢𝑖u_{i}, making our model personalized",
          "We use the latter, which is the same setting as[16]",
          "The main difference between session-based recommendations[10]and sequential recommendations[16]is that the former assumes that the user ids are not recorded and therefore the length of engagement sequences are relatively short",
          "Steam dataset introduced in[16]",
          "SASRec: a self-attentive sequential recommendation method[16]motivated by Transformer in NLP[36]",
          "We use the same datasets as in[16]and follow the same procedure in the paper: use last items for each user as test data, second-to-last as validation data and the rest as training data",
          "This may prevent us from adding user embeddings as additional parameters into complicated models like the Transformer model[16], which can easily have 20 layers with 6 self-attention blocks and millions of parameters for a medium-sized dataset like Movielens10M[6]"
        ],
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "Regarding tools, we use SQL as information query tool, SQL and ItemCF(Linden, Smith, and York2003)as hard condition and soft condition item retrieval tools, respectively, and SASRec(Kang and McAuley2018)without position embedding as the ranking tool"
        ]
      }
    },
    {
      "id": "2018-recurrent_neural_networks_with_top-k_gains_for_session-based_recommendations",
      "type": "core",
      "title": "Recurrent neural networks with top-k gains for session-based recommendations",
      "year": "2018",
      "authors": "Balázs Hidasi, Alexandros Karatzoglou",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Like sentences in NLP, sequence data in recommendations can be similarly modelled by recurrent neural networks (RNN)[10,9]and convolutional neural network (CNN)[34]models",
          "GRU4Rec+: follow-up work of GRU4Rec by the same authors: the model has a very similar architecture to GRU4Rec but has a more complicated loss function[9]",
          "There are multiple ways to define the loss of our model, previously a popular loss is the BPR loss[26,9]:"
        ]
      }
    },
    {
      "id": "2018-mesh-tensorflow:_deep_learning_for_supercomputers",
      "type": "core",
      "title": "Mesh-tensorflow: Deep learning for supercomputers",
      "year": 2018,
      "authors": "Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, Blake Hechtman",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2018-know_what_you_don’t_know:_unanswerable_questions_for_squad",
      "type": "core",
      "title": "Know what you don’t know: Unanswerable questions for SQuAD",
      "year": "2018",
      "authors": "Pranav Rajpurkar, Robin Jia, and Percy Liang",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2018-bi-directional_block_self-attention_for_fast_and_memory-efficient_sequence_modeling",
      "type": "core",
      "title": "Bi-directional block self-attention for fast and memory-efficient sequence modeling",
      "year": 2018,
      "authors": "Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Chengqi Zhang",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2018-adaptive_input_representations_for_neural_language_modeling",
      "type": "core",
      "title": "Adaptive input representations for neural language modeling",
      "year": 2018,
      "authors": "Alexei Baevski, Michael Auli",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2017-the_reversible_residual_network:_backpropagation_without_storing_activations",
      "type": "core",
      "title": "The reversible residual network: Backpropagation without storing activations",
      "year": 2017,
      "authors": "Aidan N. Gomez, Mengye Ren, Raquel Urtasun, Roger B. Grosse",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2017-semeval-2017_task_1:_semantic_textual_similarity_multilingual_and_crosslingual_focused_evaluation",
      "type": "core",
      "title": "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
      "year": "2017",
      "authors": "Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2017-proximal_policy_optimization_algorithms",
      "type": "core",
      "title": "Proximal policy optimization algorithms",
      "year": 2017,
      "authors": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "The policy (language model) is then optimised using methods like Proximal Policy Optimisation (PPO)[21]:",
          "ϵitalic-ϵ\\epsilonitalic_ϵis the clipping parameter that prevents excessive updates (as in PPO[21]),"
        ]
      }
    },
    {
      "id": "2017-mastering_chess_and_shogi_by_self-play_with_a_general_reinforcement_learning_algorithm",
      "type": "core",
      "title": "Mastering chess and shogi by self-play with a general reinforcement learning algorithm",
      "year": 2017,
      "authors": "David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, Demis Hassabis",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "Model-based strategies like Monte Carlo Tree Search (MCTS) serve as classic illustrations of this approach[23]"
        ]
      }
    },
    {
      "id": "2017-large-scale_collaborative_ranking_in_near-linear_time",
      "type": "core",
      "title": "Large-scale collaborative ranking in near-linear time",
      "year": "2017",
      "authors": "Liwei Wu, Cho-Jui Hsieh, and James Sharpnack",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Pairwise methods[26,39]consider each pairwise comparison for a user as a label, which implicitly models the pairwise comparisons as independent observations"
        ]
      }
    },
    {
      "id": "2016-training_deep_nets_with_sublinear_memory_cost",
      "type": "core",
      "title": "Training deep nets with sublinear memory cost",
      "year": 2016,
      "authors": "Tianqi Chen, Bing Xu, Chiyuan Zhang, Carlos Guestrin",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2016-the_movielens_datasets:_history_and_context",
      "type": "core",
      "title": "The movielens datasets: History and context",
      "year": "2016",
      "authors": "F Maxwell Harper and Joseph A Konstan",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Movielens1M dataset[6], a widely used benchmark datasets containing one million user movie ratings",
          "This may prevent us from adding user embeddings as additional parameters into complicated models like the Transformer model[16], which can easily have 20 layers with 6 self-attention blocks and millions of parameters for a medium-sized dataset like Movielens10M[6]"
        ]
      }
    },
    {
      "id": "2016-learning_generic_sentence_representations_using_convolutional_neural_networks",
      "type": "core",
      "title": "Learning generic sentence representations using convolutional neural networks",
      "year": 2016,
      "authors": "Zhe Gan, Yunchen Pu, Ricardo Henao, Chunyuan Li, Xiaodong He, Lawrence Carin",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2016-inception-v4_inception-resnet_and_the_impact_of_residual_connections_on_learning",
      "type": "core",
      "title": "Inception-v4, inception-resnet and the impact of residual connections on learning",
      "year": 2016,
      "authors": "Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2016-gaussian_error_linear_units_(gelus)",
      "type": "core",
      "title": "Gaussian Error Linear Units (GELUs)",
      "year": 2016,
      "authors": "Dan Hendrycks, Kevin Gimpel",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2016-efficient_softmax_approximation_for_gpus",
      "type": "core",
      "title": "Efficient softmax approximation for gpus",
      "year": 2016,
      "authors": "Edouard Grave, Armand Joulin, Moustapha Cissé, David Grangier, Hervé Jégou",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2016-deep_learning_volume_1",
      "type": "core",
      "title": "Deep learning, volume 1",
      "year": "2016",
      "authors": "Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "ℓ2subscriptℓ2\\ell_{2}regularization[13]is the most widely used approach and has been used in many matrix factorization models in recommender systems;ℓ1subscriptℓ1\\ell_{1}regularization[35]is used when a sparse model is preferred"
        ]
      }
    },
    {
      "id": "2015-session-based_recommendations_with_recurrent_neural_networks",
      "type": "core",
      "title": "Session-based recommendations with recurrent neural networks",
      "year": "2015",
      "authors": "Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos Tikk",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Like sentences in NLP, sequence data in recommendations can be similarly modelled by recurrent neural networks (RNN)[10,9]and convolutional neural network (CNN)[34]models",
          "In particular, the SASRec model[16], inspired by the popular Transformer model in natural languages processing, has achieved state-of-art results in the temporal collaborative ranking problem and enjoyed more than 10x speed-up compared to earlier RNN[10]/ CNN[34]-based methods",
          "The main difference between session-based recommendations[10]and sequential recommendations[16]is that the former assumes that the user ids are not recorded and therefore the length of engagement sequences are relatively short",
          "GRU4Rec: the first RNN-based method proposed for the session-based recommendation problem[10]"
        ]
      }
    },
    {
      "id": "2014-glove:_global_vectors_for_word_representation",
      "type": "core",
      "title": "Glove: Global vectors for word representation",
      "year": "2014",
      "authors": "Jeffrey Pennington, Richard Socher, and Christopher Manning",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2014-adam:_a_method_for_stochastic_optimization",
      "type": "core",
      "title": "Adam: A method for stochastic optimization",
      "year": "2014",
      "authors": "Diederik P. Kingma, Jimmy Ba",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "We implemented our method in Tensorflow and solve it with Adam Optimizer[17]with a learning rate of0"
        ]
      }
    },
    {
      "id": "2013-recursive_deep_models_for_semantic_compositionality_over_a_sentiment_treebank",
      "type": "core",
      "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
      "year": "2013",
      "authors": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2013-on_the_difficulty_of_training_recurrent_neural_networks",
      "type": "core",
      "title": "On the difficulty of training recurrent neural networks",
      "year": "2013",
      "authors": "Razvan Pascanu, Tomas Mikolov, Yoshua Bengio",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "There are many other regularization techniques, including parameter sharing[5], max-norm regularization[31], gradient clipping[25], etc"
        ]
      }
    },
    {
      "id": "2012-thought_beyond_language:_neural_dissociation_of_algebra_and_natural_language",
      "type": "core",
      "title": "Thought beyond language: neural dissociation of algebra and natural language",
      "year": "2012",
      "authors": "Martin M Monti, Lawrence M Parsons, and Daniel N Osherson",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Neuroimaging studies have consistently shown that the language network – a set of brain regions responsible for language comprehension and production – remains largely inactive during various reasoning tasks(Amalric and Dehaene,2019; Monti et al,2012,2007,2009; Fedorenko et al,2011)"
        ]
      }
    },
    {
      "id": "2012-factorization_machines_with_libfm",
      "type": "core",
      "title": "Factorization machines with libFM",
      "year": "2012",
      "authors": "S. Rendle",
      "name": null,
      "parents": {
        "2016-wide_&_deep_learning_for_recommender_systems": [
          "Embedding-based models, such as factorization machines[5]or deep neural networks, can generalize to previously unseen query-item feature pairs by learning a low-dimensional dense embedding vector for each query and item feature, with less burden of feature engineering",
          "The idea of combining wide linear models with cross-product feature transformations and deep neural networks with dense embeddings is inspired by previous work, such as factorization machines[5]which add generalization to linear models by factorizing the interactions between two variables as a dot product between two low-dimensional embedding vectors"
        ]
      }
    },
    {
      "id": "2011-strategies_for_training_large_scale_neural_network_language_models",
      "type": "core",
      "title": "Strategies for training large scale neural network language models",
      "year": "2011",
      "authors": "T. Mikolov, A. Deoras, D. Povey, L. Burget, and J. H. Cernocky",
      "name": null,
      "parents": {
        "2016-wide_&_deep_learning_for_recommender_systems": [
          ", hidden layer sizes) by learning direct weights between inputs and outputs[4]"
        ]
      }
    },
    {
      "id": "2011-functional_specificity_for_high-level_linguistic_processing_in_the_human_brain",
      "type": "core",
      "title": "Functional specificity for high-level linguistic processing in the human brain",
      "year": "2011",
      "authors": "Evelina Fedorenko, Michael K Behr, and Nancy Kanwisher",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Neuroimaging studies have consistently shown that the language network – a set of brain regions responsible for language comprehension and production – remains largely inactive during various reasoning tasks(Amalric and Dehaene,2019; Monti et al,2012,2007,2009; Fedorenko et al,2011)"
        ]
      }
    },
    {
      "id": "2011-follow-the-regularized-leader_and_mirror_descent:_equivalence_theorems_and_l1_regularization",
      "type": "core",
      "title": "Follow-the-regularized-leader and mirror descent: Equivalence theorems and l1 regularization",
      "year": "2011",
      "authors": "H. B. McMahan",
      "name": null,
      "parents": {
        "2016-wide_&_deep_learning_for_recommender_systems": [
          "In the experiments, we used Follow-the-regularized-leader (FTRL) algorithm[3]withL1subscript𝐿1L_{1}regularization as the optimizer for the wide part of the model, and AdaGrad[1]for the deep part"
        ]
      }
    },
    {
      "id": "2011-appjoy:_personalized_mobile_application_discovery",
      "type": "core",
      "title": "AppJoy: Personalized mobile application discovery",
      "year": "2011",
      "authors": "B. Yan and G. Chen",
      "name": null,
      "parents": {
        "2016-wide_&_deep_learning_for_recommender_systems": [
          "There has also been previous work on mobile app recommender systems, such as AppJoy which used CF on users’ app usage records[8]"
        ]
      }
    },
    {
      "id": "2011-adaptive_subgradient_methods_for_online_learning_and_stochastic_optimization",
      "type": "core",
      "title": "Adaptive subgradient methods for online learning and stochastic optimization",
      "year": "2011",
      "authors": "J. Duchi, E. Hazan, and Y. Singer",
      "name": null,
      "parents": {
        "2016-wide_&_deep_learning_for_recommender_systems": [
          "In the experiments, we used Follow-the-regularized-leader (FTRL) algorithm[3]withL1subscript𝐿1L_{1}regularization as the optimizer for the wide part of the model, and AdaGrad[1]for the deep part"
        ]
      }
    },
    {
      "id": "2010-unconscious_activation_of_the_prefrontal_no-go_network",
      "type": "core",
      "title": "Unconscious activation of the prefrontal no-go network",
      "year": "2010",
      "authors": "S. Van Gaal, K. R. Ridderinkhof, H. S. Scholte, and V. A. Lamme",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "2and also see[28])",
          "(b) while voluntary control engages a broader network, activating many regions within the parietal and prefrontal lobes[28]"
        ]
      }
    },
    {
      "id": "2009-the_fifth_pascal_recognizing_textual_entailment_challenge",
      "type": "core",
      "title": "The fifth PASCAL recognizing textual entailment challenge",
      "year": "2009",
      "authors": "Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2009-the_boundaries_of_language_and_thought_in_deductive_inference",
      "type": "core",
      "title": "The boundaries of language and thought in deductive inference",
      "year": "2009",
      "authors": "Martin M Monti, Lawrence M Parsons, and Daniel N Osherson",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Neuroimaging studies have consistently shown that the language network – a set of brain regions responsible for language comprehension and production – remains largely inactive during various reasoning tasks(Amalric and Dehaene,2019; Monti et al,2012,2007,2009; Fedorenko et al,2011)"
        ]
      }
    },
    {
      "id": "2009-the_bellkor_solution_to_the_netflix_grand_prize",
      "type": "core",
      "title": "The bellkor solution to the netflix grand prize",
      "year": "2009",
      "authors": "Yehuda Koren",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Although personalization is not needed for the original Transformer model[36]in natural languages understandings or translations, personalization plays a crucial role throughout recommender system literature[43]ever since the matrix factorization approach to the Netflix prize[19]"
        ]
      }
    },
    {
      "id": "2009-collaborative_filtering_with_temporal_dynamics",
      "type": "core",
      "title": "Collaborative filtering with temporal dynamics",
      "year": "2009",
      "authors": "Yehuda Koren",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Both settings, do not explicitly require time-stamps: only the relative temporal orderings are assumed known (in contrast to, for example, timeSVD++[20])"
        ]
      }
    },
    {
      "id": "2008-probabilistic_matrix_factorization",
      "type": "core",
      "title": "Probabilistic matrix factorization",
      "year": "2008",
      "authors": "Tom Vander Aa, Imen Chakroun, Tom Haber",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Collaborative filtering algorithms including matrix factorization,[11,29,18,24,14], which predict the feedback in a pointwise fashion as if it were a supervised learning problem, fall into the first category"
        ]
      }
    },
    {
      "id": "2008-mining_recommendations_from_the_web",
      "type": "core",
      "title": "Mining recommendations from the web",
      "year": "2008",
      "authors": "Guy Shani, Max Chickering, and Christopher Meek",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Recommendation systems are increasingly prevalent due to content delivery platforms, e-commerce websites, and mobile apps[30]"
        ]
      }
    },
    {
      "id": "2008-cofi_rank-maximum_margin_matrix_factorization_for_collaborative_ranking",
      "type": "core",
      "title": "Cofi rank-maximum margin matrix factorization for collaborative ranking",
      "year": "2008",
      "authors": "Markus Weimer, Alexandros Karatzoglou, Quoc V Le, and Alex J Smola",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "In literature, this is formulated as the collaborative ranking problem[38]"
        ]
      }
    },
    {
      "id": "2007-the_third_pascal_recognizing_textual_entailment_challenge",
      "type": "core",
      "title": "The third PASCAL recognizing textual entailment challenge",
      "year": "2007",
      "authors": "Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2007-functional_neuroanatomy_of_deductive_inference:_a_language-independent_distributed_network",
      "type": "core",
      "title": "Functional neuroanatomy of deductive inference: a language-independent distributed network",
      "year": "2007",
      "authors": "Martin M Monti, Daniel N Osherson, Michael J Martinez, and Lawrence M Parsons",
      "name": null,
      "parents": {
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Neuroimaging studies have consistently shown that the language network – a set of brain regions responsible for language comprehension and production – remains largely inactive during various reasoning tasks(Amalric and Dehaene,2019; Monti et al,2012,2007,2009; Fedorenko et al,2011)"
        ]
      }
    },
    {
      "id": "2007-collaborative_filtering_recommender_systems",
      "type": "core",
      "title": "Collaborative filtering recommender systems",
      "year": "2007",
      "authors": "Zhihai Yang",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Collaborative filtering algorithms including matrix factorization,[11,29,18,24,14], which predict the feedback in a pointwise fashion as if it were a supervised learning problem, fall into the first category"
        ]
      }
    },
    {
      "id": "2006-unifying_user-based_and_item-based_collaborative_filtering_approaches_by_similarity_fusion",
      "type": "core",
      "title": "Unifying user-based and item-based collaborative filtering approaches by similarity fusion",
      "year": "2006",
      "authors": "Jun Wang, Arjen P De Vries, and Marcel JT Reinders",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Item-to-item[28], user-to-user[37], user-to-item[21]are 3 different angles of utilizing user engagement data"
        ]
      }
    },
    {
      "id": "2006-the_second_pascal_recognising_textual_entailment_challenge",
      "type": "core",
      "title": "The second PASCAL recognising textual entailment challenge",
      "year": "2006",
      "authors": "Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2005-the_pascal_recognising_textual_entailment_challenge",
      "type": "core",
      "title": "The PASCAL recognising textual entailment challenge",
      "year": "2005",
      "authors": "Ido Dagan, Oren Glickman, and Bernardo Magnini",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2005-maximum-margin_matrix_factorization",
      "type": "core",
      "title": "Maximum-margin matrix factorization",
      "year": "2005",
      "authors": "Shamal Shaikh, Venkateswara Rao Kagita, Vikas Kumar, Arun K Pujari",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "There are many other regularization techniques, including parameter sharing[5], max-norm regularization[31], gradient clipping[25], etc"
        ]
      }
    },
    {
      "id": "2005-automatically_constructing_a_corpus_of_sentential_paraphrases",
      "type": "core",
      "title": "Automatically constructing a corpus of sentential paraphrases",
      "year": "2005",
      "authors": "William B. Dolan and Chris Brockett",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "2001-item-based_collaborative_filtering_recommendation_algorithms",
      "type": "core",
      "title": "Item-based collaborative filtering recommendation algorithms",
      "year": "2001",
      "authors": "Badrul Munir Sarwar, George Karypis, Joseph A Konstan, John Riedl, et al",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "SQL-Rank[40]and item-based recommendations[28]are omitted because the former is similar to BPR[26]except using the listwise loss function instead of the pairwise loss function and the latter has been shown inferior to TransRec[8]",
          "Item-to-item[28], user-to-user[37], user-to-item[21]are 3 different angles of utilizing user engagement data"
        ]
      }
    },
    {
      "id": "1996-regression_shrinkage_and_selection_via_the_lasso",
      "type": "core",
      "title": "Regression shrinkage and selection via the lasso",
      "year": "1996",
      "authors": "Robert Tibshirani",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "ℓ2subscriptℓ2\\ell_{2}regularization[13]is the most widely used approach and has been used in many matrix factorization models in recommender systems;ℓ1subscriptℓ1\\ell_{1}regularization[35]is used when a sparse model is preferred"
        ]
      }
    },
    {
      "id": "1995-recommending_and_evaluating_choices_in_a_virtual_community_of_use",
      "type": "core",
      "title": "Recommending and evaluating choices in a virtual community of use",
      "year": "1995",
      "authors": "Will Hill, Larry Stead, Mark Rosenstein, and George Furnas",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Collaborative filtering algorithms including matrix factorization,[11,29,18,24,14], which predict the feedback in a pointwise fashion as if it were a supervised learning problem, fall into the first category"
        ]
      }
    },
    {
      "id": "1995-centering:_a_framework_for_modeling_the_local_coherence_of_discourse",
      "type": "core",
      "title": "Centering: A framework for modeling the local coherence of discourse",
      "year": "1995",
      "authors": "Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "1992-a_simple_weight_decay_can_improve_generalization",
      "type": "core",
      "title": "A simple weight decay can improve generalization",
      "year": "1992",
      "authors": "Anders Krogh and John A Hertz",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Weight decay[22], also known asl2subscript𝑙2l_{2}regularization[13], is applied to all embeddings, including both user and item embeddings"
        ]
      }
    },
    {
      "id": "1979-coherence_and_coreference",
      "type": "core",
      "title": "Coherence and coreference",
      "year": "1979",
      "authors": "Jerry R. Hobbs",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "1978-the_rating_of_chessplayers_past_and_present",
      "type": "core",
      "title": "The rating of chessplayers, past and present",
      "year": "1978",
      "authors": "A. E. Elo",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "Suppose we have access to an extensive dataset of chess games, but all from players with Elo ratings below 2000 (a standardised measure of player skill)[5]"
        ]
      }
    },
    {
      "id": "1976-cohesion_in_english",
      "type": "core",
      "title": "Cohesion in English",
      "year": "1976",
      "authors": "M.A.K. Halliday and Ruqaiya Hasan",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ]
      }
    },
    {
      "id": "1970-ridge_regression:_biased_estimation_for_nonorthogonal_problems",
      "type": "core",
      "title": "Ridge regression: Biased estimation for nonorthogonal problems",
      "year": "1970",
      "authors": "Arthur E Hoerl and Robert W Kennard",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "ℓ2subscriptℓ2\\ell_{2}regularization[13]is the most widely used approach and has been used in many matrix factorization models in recommender systems;ℓ1subscriptℓ1\\ell_{1}regularization[35]is used when a sparse model is preferred",
          "Weight decay[22], also known asl2subscript𝑙2l_{2}regularization[13], is applied to all embeddings, including both user and item embeddings"
        ]
      }
    },
    {
      "id": "1958-dynamic_programming_and_stochastic_control_processes",
      "type": "core",
      "title": "Dynamic programming and stochastic control processes",
      "year": "1958",
      "authors": "R. Bellman",
      "name": null,
      "parents": {
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "The Bellman equation[1]for the PRM is:",
          "We can define the reasoning process as a Markov Decision Process (MDP)[1]"
        ]
      }
    },
    {
      "id": "1799-joint_training_of_a_convolutional_network_and_a_graphical_model_for_human_pose_estimation",
      "type": "core",
      "title": "Joint training of a convolutional network and a graphical model for human pose estimation",
      "year": "1799",
      "authors": "Jonathan Tompson, Arjun Jain, Yann LeCun, Christoph Bregler",
      "name": null,
      "parents": {
        "2016-wide_&_deep_learning_for_recommender_systems": [
          "Joint training of neural networks with graphical models has also been applied to human pose estimation from images[6]"
        ]
      }
    }
  ],
  [
    {
      "id": "2410-uncertainty-aware_reward_model:_teaching_reward_models_to_know_what_is_unknown_2024",
      "type": "core",
      "title": "Uncertainty-aware reward model: Teaching reward models to know what is unknown, 2024",
      "year": "2410",
      "authors": "Xingzhou Lou, Dong Yan, Wei Shen, Yuzi Yan, Jian Xie, and Junge Zhang",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "Skywork-Reward-Gemma-2-27B(Liu & Zeng,2024)and URM-LLaMa-3"
        ]
      }
    },
    {
      "id": "2024-yi:_open_foundation_models_by_01.ai_2024",
      "type": "core",
      "title": "Yi: Open foundation models by 01.ai, 2024",
      "year": "2024",
      "authors": "01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "Therefore, we select four benchmarks—HotpotQA(Yang et al,2018), Collie(Yao et al,2023), USACO(Shi et al,2024), and AIME222https://huggingface",
          "Inspired by the scaling law, the rapid development of open-source models has also been achieved through scaling up the size of parameters and collecting huge data for pre-training, such as Qwen(Bai et al,2023; Yang et al,2024), Yi(AI et al,2024), Llama(Touvron et al,2023; Dubey et al,2024), and Deepseek(Bi et al,2024)"
        ]
      }
    },
    {
      "id": "2024-xcot:_cross-lingual_instruction_tuning_for_cross-lingual_chain-of-thought_reasoning",
      "type": "core",
      "title": "xcot: Cross-lingual instruction tuning for cross-lingual chain-of-thought reasoning",
      "year": 2024,
      "authors": "Linzheng Chai, Jian Yang, Tao Sun, Hongcheng Guo, Jiaheng Liu, Bing Wang, Xiannian Liang, Jiaqi Bai, Tongliang Li, Qiyao Peng, Zhoujun Li",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          ", Commonsense Reasoning(Yang et al,2018), Coding(Jain et al,2024; Chai et al,2024a), Math(Satpute et al,2024; Chai et al,2024b), and Dialogue(Young & Shishido,2023))"
        ]
      }
    },
    {
      "id": "2024-tool_learning_with_large_language_models:_a_survey",
      "type": "core",
      "title": "Tool learning with large language models: A survey",
      "year": 2024,
      "authors": "Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "Snell et al (2024)propose that scaling LLMs Test-time Compute optimally can be more effective than scaling model parameters"
        ]
      }
    },
    {
      "id": "2024-the_llama_3_herd_of_models",
      "type": "core",
      "title": "The llama 3 herd of models",
      "year": 2024,
      "authors": "Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, Zhiyu Ma",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "According to LIME(Zhu et al,2024), we design adata filteringmodule to show the performance differences among different models",
          "Therefore, we select four benchmarks—HotpotQA(Yang et al,2018), Collie(Yao et al,2023), USACO(Shi et al,2024), and AIME222https://huggingface",
          "Inspired by the scaling law, the rapid development of open-source models has also been achieved through scaling up the size of parameters and collecting huge data for pre-training, such as Qwen(Bai et al,2023; Yang et al,2024), Yi(AI et al,2024), Llama(Touvron et al,2023; Dubey et al,2024), and Deepseek(Bi et al,2024)"
        ],
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "3-70B-Instruct[13]"
        ],
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Large language models (LLMs) have demonstrated remarkable reasoning abilities, emerging from extensive pretraining on human languages(Dubey et al,2024; Achiam et al,2023)",
          "2-3B and Llama 3-8B(Dubey et al,2024)withc=1c=1"
        ]
      }
    },
    {
      "id": "2024-symbolic_learning_enables_self-evolving_agents",
      "type": "core",
      "title": "Symbolic learning enables self-evolving agents",
      "year": 2024,
      "authors": "Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, Yuchen Eleanor Jiang",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "For the commonsense reasoning datasets, we leverage the existing state-of-the-art agent framework(Zhou et al,2023;2024)for evaluation",
          "Therefore, we select four benchmarks—HotpotQA(Yang et al,2018), Collie(Yao et al,2023), USACO(Shi et al,2024), and AIME222https://huggingface"
        ]
      }
    },
    {
      "id": "2024-skywork_reward_model_series",
      "type": "core",
      "title": "Skywork reward model series",
      "year": "2024",
      "authors": "Chris Yuhao Liu and Liang Zeng",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "Skywork-Reward-Gemma-2-27B(Liu & Zeng,2024)and URM-LLaMa-3"
        ]
      }
    },
    {
      "id": "2024-scaling_llm_test-time_compute_optimally_can_be_more_effective_than_scaling_model_parameters",
      "type": "core",
      "title": "Scaling llm test-time compute optimally can be more effective than scaling model parameters",
      "year": 2024,
      "authors": "Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "Snell et al (2024)propose that scaling LLMs Test-time Compute optimally can be more effective than scaling model parameters",
          "Inspired by the scaling law, the rapid development of open-source models has also been achieved through scaling up the size of parameters and collecting huge data for pre-training, such as Qwen(Bai et al,2023; Yang et al,2024), Yi(AI et al,2024), Llama(Touvron et al,2023; Dubey et al,2024), and Deepseek(Bi et al,2024)"
        ],
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "Similarly, the paper[24]emphasises the importance of optimising test-time computation, empirically showing that inference-time reasoning enhancements can often yield more substantial improvements than simply scaling model parameters",
          "Building on substantial performance gains, OpenAI o1 has shown that the scaling principles traditionally applied during training[9,24]are now relevant to the inference phase",
          "Proposed methods range from collecting specialised training data to building reward models[18,11,15]and increasing the computational complexity of decoding[24,33], but none have yet achieved significant performance breakthroughs at scale",
          "To strike a balance between efficiency and effectiveness, the work[24,33]found that reasoning tasks benefit from more flexible approaches like beam search"
        ],
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Thus, generating more tokens serves as a way to inference-time scaling for reasoning(Guo et al,2025; Snell et al,2024)"
        ]
      }
    },
    {
      "id": "2024-rewardbench:_evaluating_reward_models_for_language_modeling",
      "type": "core",
      "title": "Rewardbench: Evaluating reward models for language modeling",
      "year": 2024,
      "authors": "Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "Skywork-Reward-Gemma-2-27B(Liu & Zeng,2024)and URM-LLaMa-3"
        ]
      }
    },
    {
      "id": "2024-qwen2_technical_report",
      "type": "core",
      "title": "Qwen2 technical report",
      "year": 2024,
      "authors": "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, Zhihao Fan",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "Therefore, we select four benchmarks—HotpotQA(Yang et al,2018), Collie(Yao et al,2023), USACO(Shi et al,2024), and AIME222https://huggingface",
          "Inspired by the scaling law, the rapid development of open-source models has also been achieved through scaling up the size of parameters and collecting huge data for pre-training, such as Qwen(Bai et al,2023; Yang et al,2024), Yi(AI et al,2024), Llama(Touvron et al,2023; Dubey et al,2024), and Deepseek(Bi et al,2024)"
        ],
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Large reasoning models focus on enhancing performance at test time by utilizing extended reasoning steps, contrasting with traditional large pre-trained models that achieve scalability during training by increasing model size or expanding training data[17,66,50,85,76]"
        ]
      }
    },
    {
      "id": "2024-quiet-star:_language_models_can_teach_themselves_to_think_before_speaking",
      "type": "core",
      "title": "Quiet-star: Language models can teach themselves to think before speaking",
      "year": 2024,
      "authors": "Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, Noah D. Goodman",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "Snell et al (2024)propose that scaling LLMs Test-time Compute optimally can be more effective than scaling model parameters"
        ],
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Studies have shown that test-time scaling can improve the reasoning abilities of smaller models on complex tasks[15,75]"
        ],
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "While previous work has attempted to fix these problems by prompting LLMs to generate succinct reasoning chains(Madaan and Yazdanbakhsh,2022), or performing additional reasoning before generating some critical tokens(Zelikman et al,2024), these solutions remain constrained within the language space and do not solve the fundamental problems"
        ]
      }
    },
    {
      "id": "2024-mceval:_massively_multilingual_code_evaluation",
      "type": "core",
      "title": "Mceval: Massively multilingual code evaluation",
      "year": 2024,
      "authors": "Linzheng Chai, Shukai Liu, Jian Yang, Yuwei Yin, Ke Jin, Jiaheng Liu, Tao Sun, Ge Zhang, Changyu Ren, Hongcheng Guo, Zekun Wang, Boyang Wang, Xianjie Wu, Bing Wang, Tongliang Li, Liqun Yang, Sufeng Duan, Zhoujun Li",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          ", Commonsense Reasoning(Yang et al,2018), Coding(Jain et al,2024; Chai et al,2024a), Math(Satpute et al,2024; Chai et al,2024b), and Dialogue(Young & Shishido,2023))"
        ]
      }
    },
    {
      "id": "2024-livecodebench:_holistic_and_contamination_free_evaluation_of_large_language_models_for_code",
      "type": "core",
      "title": "Livecodebench: Holistic and contamination free evaluation of large language models for code",
      "year": 2024,
      "authors": "Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, Ion Stoica",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          ", Commonsense Reasoning(Yang et al,2018), Coding(Jain et al,2024; Chai et al,2024a), Math(Satpute et al,2024; Chai et al,2024b), and Dialogue(Young & Shishido,2023))"
        ],
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "(2)Math benchmarksincludeMATH500[38],AMC2023111https://huggingface"
        ]
      }
    },
    {
      "id": "2024-lime-m:_less_is_more_for_evaluation_of_mllms",
      "type": "core",
      "title": "Lime-m: Less is more for evaluation of mllms",
      "year": "2024",
      "authors": "Kang Zhu, Qianbo Zang, Shian Jia, Siwei Wu, Feiteng Fang, Yizhi Li, Shuyue Guo, Tianyu Zheng, Bo Li, Haoning Wu, et al",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "According to LIME(Zhu et al,2024), we design adata filteringmodule to show the performance differences among different models",
          "Therefore, we select four benchmarks—HotpotQA(Yang et al,2018), Collie(Yao et al,2023), USACO(Shi et al,2024), and AIME222https://huggingface"
        ]
      }
    },
    {
      "id": "2024-deepseek_llm:_scaling_open-source_language_models_with_longtermism",
      "type": "core",
      "title": "Deepseek llm: Scaling open-source language models with longtermism",
      "year": 2024,
      "authors": "DeepSeek-AI, :, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, Yuheng Zou",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "Inspired by the scaling law, the rapid development of open-source models has also been achieved through scaling up the size of parameters and collecting huge data for pre-training, such as Qwen(Bai et al,2023; Yang et al,2024), Yi(AI et al,2024), Llama(Touvron et al,2023; Dubey et al,2024), and Deepseek(Bi et al,2024)"
        ]
      }
    },
    {
      "id": "2024-can_llms_master_math?_investigating_large_language_models_on_math_stack_exchange",
      "type": "core",
      "title": "Can llms master math? investigating large language models on math stack exchange",
      "year": 2024,
      "authors": "Ankit Satpute, Noah Giessing, Andre Greiner-Petter, Moritz Schubotz, Olaf Teschke, Akiko Aizawa, Bela Gipp",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          ", Commonsense Reasoning(Yang et al,2018), Coding(Jain et al,2024; Chai et al,2024a), Math(Satpute et al,2024; Chai et al,2024b), and Dialogue(Young & Shishido,2023))"
        ]
      }
    },
    {
      "id": "2024-can_language_models_solve_olympiad_programming?",
      "type": "core",
      "title": "Can language models solve olympiad programming?",
      "year": 2024,
      "authors": "Quan Shi, Michael Tang, Karthik Narasimhan, Shunyu Yao",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "Therefore, we select four benchmarks—HotpotQA(Yang et al,2018), Collie(Yao et al,2023), USACO(Shi et al,2024), and AIME222https://huggingface",
          "We are using the bronze level of theUSACO(Shi et al,2024)competition to test the coding skills of LLMs"
        ]
      }
    },
    {
      "id": "2023-toolllm:_facilitating_large_language_models_to_master_16000+_real-world_apis",
      "type": "core",
      "title": "Toolllm: Facilitating large language models to master 16000+ real-world apis",
      "year": 2023,
      "authors": "Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, Maosong Sun",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "ToolLlaMA2-7B(Qin et al2023b)is another fine-tuned model designed to interact with external APIs in response to human instructions"
        ],
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "Wang et al (2023)conduct a hierarchical hypothesis search to enable inductive reasoning capabilities"
        ]
      }
    },
    {
      "id": "2023-think_before_you_speak:_training_language_models_with_pause_tokens",
      "type": "core",
      "title": "Think before you speak: Training language models with pause tokens",
      "year": 2023,
      "authors": "Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, Vaishnavh Nagarajan",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "Wang et al (2023)conduct a hierarchical hypothesis search to enable inductive reasoning capabilities"
        ],
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "Another approach is presented in[7], which suggests using pause tokens to force models to pause and “think” during reasoning"
        ],
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "(4)Pause token(Goyal et al,2023): The model is trained using only the question and answer without a reasoning chain",
          "Goyal et al (2023)pretrained the model by randomly inserting a learnable<pause>tokens to the training corpus"
        ]
      }
    },
    {
      "id": "2023-self-refine:_iterative_refinement_with_self-feedback",
      "type": "core",
      "title": "Self-refine: Iterative refinement with self-feedback",
      "year": 2023,
      "authors": "Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "As models show emergent intelligence, researchers have started exploring the potential to leverage LLMs as autonomous agents(Wang et al2023a; Zhao, Jin, and Cheng2023), augmented with memory modules, planning ability, and tool-using capabilities",
          "Inspired by Self-Instruct(Madaan et al2023), we use LLM to generate demonstrations of tool-using plans in the form of(x,𝒑)𝑥𝒑(x,\\boldsymbol{p})( italic_x , bold_italic_p )",
          "Despite LLM’s strong intelligence, it still exhibits occasional errors in reasoning and tool utilization(Madaan et al2023; Shinn et al2023)"
        ],
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "It improves initial outputs from LLMs through iterative feedback and refinement(Madaan et al,2024)",
          "Therefore, we select four benchmarks—HotpotQA(Yang et al,2018), Collie(Yao et al,2023), USACO(Shi et al,2024), and AIME222https://huggingface",
          "Recently, Test-time Compute methods, such as Best-of-N (BoN) and Self-Refine(Madaan et al,2024), have been proposed to enhance model performance during the inference phase and have shown to be more efficient than simply increasing model parameters"
        ]
      }
    },
    {
      "id": "2023-qwen_technical_report",
      "type": "core",
      "title": "Qwen technical report",
      "year": 2023,
      "authors": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "Therefore, we select four benchmarks—HotpotQA(Yang et al,2018), Collie(Yao et al,2023), USACO(Shi et al,2024), and AIME222https://huggingface",
          "We selectHotpotQA(Yang et al,2018)andCollie(Yao et al,2023)to evaluate the commonsense reasoning ability of LLMs",
          "As a result, OpenAI’s GPT series models(Radford,2018; Radford et al,2019; Brown,2020; Achiam et al,2023)have achieved remarkable success in the NLP field"
        ]
      }
    },
    {
      "id": "2023-llama:_open_and_efficient_foundation_language_models",
      "type": "core",
      "title": "LLaMA: Open and Efficient Foundation Language Models",
      "year": 2023,
      "authors": null,
      "name": "LLaMA",
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "The scaling-up of parameters and data has led to significant advancements in the capabilities of LLMs, including in-context learning(Brown et al2020; Liu et al2021; Rubin, Herzig, and Berant2021), instruction following(Ouyang et al2022; Touvron et al2023a; OpenAI2023), planning and reasoning(Wei et al2022; Wang et al2022a; Yao et al2022; Yang et al2023; Wang et al2023b)"
        ],
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "As a result, OpenAI’s GPT series models(Radford,2018; Radford et al,2019; Brown,2020; Achiam et al,2023)have achieved remarkable success in the NLP field"
        ],
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "The work in[25]focused its analysis from graphical models for the chain of thought mechanism"
        ]
      }
    },
    {
      "id": "2023-investigating_openai’s_chatgpt_potentials_in_generating_chatbot’s_dialogue_for_english_as_a_foreign_language_learning",
      "type": "core",
      "title": "Investigating openai’s chatgpt potentials in generating chatbot’s dialogue for english as a foreign language learning",
      "year": "2023",
      "authors": "Julio Christian Young and Makoto Shishido",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          ", Commonsense Reasoning(Yang et al,2018), Coding(Jain et al,2024; Chai et al,2024a), Math(Satpute et al,2024; Chai et al,2024b), and Dialogue(Young & Shishido,2023))"
        ]
      }
    },
    {
      "id": "2023-hypothesis_search:_inductive_reasoning_with_language_models",
      "type": "core",
      "title": "Hypothesis search: Inductive reasoning with language models",
      "year": 2023,
      "authors": "Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, Noah D. Goodman",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "Wang et al (2023)conduct a hierarchical hypothesis search to enable inductive reasoning capabilities"
        ]
      }
    },
    {
      "id": "2023-gpt-4_technical_report",
      "type": "core",
      "title": "GPT-4 Technical Report",
      "year": 2023,
      "authors": "OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, Barret Zoph",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "The scaling-up of parameters and data has led to significant advancements in the capabilities of LLMs, including in-context learning(Brown et al2020; Liu et al2021; Rubin, Herzig, and Berant2021), instruction following(Ouyang et al2022; Touvron et al2023a; OpenAI2023), planning and reasoning(Wei et al2022; Wang et al2022a; Yao et al2022; Yang et al2023; Wang et al2023b)",
          "5,GPT-4(OpenAI2023): We access these LLMs from OpenAI by API service"
        ],
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "As a result, OpenAI’s GPT series models(Radford,2018; Radford et al,2019; Brown,2020; Achiam et al,2023)have achieved remarkable success in the NLP field"
        ],
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "Large language models (LLMs) have demonstrated remarkable reasoning abilities, emerging from extensive pretraining on human languages(Dubey et al,2024; Achiam et al,2023)"
        ]
      }
    },
    {
      "id": "2023-collie:_systematic_construction_of_constrained_text_generation_tasks",
      "type": "core",
      "title": "Collie: Systematic construction of constrained text generation tasks",
      "year": 2023,
      "authors": "Shunyu Yao, Howard Chen, Austin W. Hanjie, Runzhe Yang, Karthik Narasimhan",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "Therefore, we select four benchmarks—HotpotQA(Yang et al,2018), Collie(Yao et al,2023), USACO(Shi et al,2024), and AIME222https://huggingface",
          "We selectHotpotQA(Yang et al,2018)andCollie(Yao et al,2023)to evaluate the commonsense reasoning ability of LLMs"
        ]
      }
    },
    {
      "id": "2023-agents:_an_open-source_framework_for_autonomous_language_agents",
      "type": "core",
      "title": "Agents: An open-source framework for autonomous language agents",
      "year": 2023,
      "authors": "Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Xiangru Tang, Ningyu Zhang, Huajun Chen, Peng Cui, Mrinmaya Sachan",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "For the commonsense reasoning datasets, we leverage the existing state-of-the-art agent framework(Zhou et al,2023;2024)for evaluation"
        ]
      }
    },
    {
      "id": "2022-pal:_program-aided_language_models",
      "type": "core",
      "title": "Pal: Program-aided language models",
      "year": 2022,
      "authors": "Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig",
      "name": null,
      "parents": {
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "As models show emergent intelligence, researchers have started exploring the potential to leverage LLMs as autonomous agents(Wang et al2023a; Zhao, Jin, and Cheng2023), augmented with memory modules, planning ability, and tool-using capabilities"
        ],
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "Wang et al (2023)conduct a hierarchical hypothesis search to enable inductive reasoning capabilities"
        ]
      }
    },
    {
      "id": "2020-scaling_laws_for_autoregressive_generative_modeling",
      "type": "core",
      "title": "Scaling laws for autoregressive generative modeling",
      "year": 2020,
      "authors": "Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, Sam McCandlish",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "With the emergence of Transformers(Vaswani,2017)and the scaling laws(Henighan et al,2020), the researchers try to scale up the parameters of the generative language model"
        ],
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Large reasoning models focus on enhancing performance at test time by utilizing extended reasoning steps, contrasting with traditional large pre-trained models that achieve scalability during training by increasing model size or expanding training data[17,66,50,85,76]"
        ]
      }
    },
    {
      "id": "2019-language_models_are_unsupervised_multitask_learners",
      "type": "core",
      "title": "Language models are unsupervised multitask learners",
      "year": "2019",
      "authors": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ],
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "As a result, OpenAI’s GPT series models(Radford,2018; Radford et al,2019; Brown,2020; Achiam et al,2023)have achieved remarkable success in the NLP field"
        ]
      }
    },
    {
      "id": "2018-personalized_top-n_sequential_recommendation_via_convolutional_sequence_embedding",
      "type": "core",
      "title": "Personalized top-n sequential recommendation via convolutional sequence embedding",
      "year": "2018",
      "authors": "Jiaxi Tang, Ke Wang",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Like sentences in NLP, sequence data in recommendations can be similarly modelled by recurrent neural networks (RNN)[10,9]and convolutional neural network (CNN)[34]models",
          "In particular, the SASRec model[16], inspired by the popular Transformer model in natural languages processing, has achieved state-of-art results in the temporal collaborative ranking problem and enjoyed more than 10x speed-up compared to earlier RNN[10]/ CNN[34]-based methods",
          "Caser: a CNN-based method[34]which embeds a sequence of recent items in both time and latent spaces forming an ‘image’ before learning local features through horizontal and vertical convolutional filters"
        ],
        "2018-self-attentive_sequential_recommendation": [
          "In particular, Convolutional Sequence Embedding (Caser), a CNN-based method, views the embedding matrix ofL𝐿Lprevious items as an ‘image’ and applies convolutional operations to extract transitions[22]",
          "Fossil[21], Vista[20], Caser[22])",
          "Convolutional Sequence Embeddings (Caser)[22]",
          "A few options are promising to investigate in the future: 1) using restricted self-attention[42]which only attends on recent actions rather than all actions, and distant actions can be considered in higher layers; 2) splitting long sequences into short segments as in[22]",
          "MF[40], FPMC[1]and Caser[22]); 2) consider the user’s previous actions, and induce animplicituser embedding from embeddings of visited items (e"
        ]
      }
    },
    {
      "id": "2018-latent_cross:_making_use_of_context_in_recurrent_recommender_systems",
      "type": "core",
      "title": "Latent cross: Making use of context in recurrent recommender systems,",
      "year": "2018",
      "authors": "[25]  A. Beutel, P. Covington, S. Jain, C. Xu, J. Li, V. Gatto, and E. H. Chi,",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "Other than MC-based methods, another line of work adopts RNNs to model user sequences[23,2,24,25]"
        ]
      }
    },
    {
      "id": "2018-improving_language_understanding_by_generative_pre-training",
      "type": "core",
      "title": "Improving language understanding by generative pre-training",
      "year": "2018",
      "authors": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ],
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "As a result, OpenAI’s GPT series models(Radford,2018; Radford et al,2019; Brown,2020; Achiam et al,2023)have achieved remarkable success in the NLP field"
        ]
      }
    },
    {
      "id": "2018-hotpotqa:_a_dataset_for_diverse_explainable_multi-hop_question_answering",
      "type": "core",
      "title": "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
      "year": 2018,
      "authors": "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, Christopher D. Manning",
      "name": null,
      "parents": {
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          ", Commonsense Reasoning(Yang et al,2018), Coding(Jain et al,2024; Chai et al,2024a), Math(Satpute et al,2024; Chai et al,2024b), and Dialogue(Young & Shishido,2023))",
          "Therefore, we select four benchmarks—HotpotQA(Yang et al,2018), Collie(Yao et al,2023), USACO(Shi et al,2024), and AIME222https://huggingface",
          "We selectHotpotQA(Yang et al,2018)andCollie(Yao et al,2023)to evaluate the commonsense reasoning ability of LLMs"
        ],
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "(2)Multi-hop QA datasets:HotpotQA[68]is the first large-scale dataset requiring reasoning across multiple Wikipedia paragraphs"
        ]
      }
    },
    {
      "id": "2018-attention-based_transactional_context_embedding_for_next-item_recommendation",
      "type": "core",
      "title": "Attention-based transactional context embedding for next-item recommendation,",
      "year": "2018",
      "authors": "[31]  S. Wang, L. Hu, L. Cao, X. Huang, D. Lian, and W. Liu,",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "Recently, attention mechanisms have been incorporated into recommender systems[29,30,31]"
        ]
      }
    },
    {
      "id": "2018-an_empirical_evaluation_of_generic_convolutional_and_recurrent_networks_for_sequence_modeling",
      "type": "core",
      "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling,",
      "year": "2018",
      "authors": "Shaojie Bai, J. Zico Kolter, Vladlen Koltun",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "RNNs are generally suitable for modeling sequences, though recent studies show CNNs and self-attention can be stronger in some sequential settings[3,44]"
        ]
      }
    },
    {
      "id": "2018-a_time-restricted_self-attention_layer_for_asr",
      "type": "core",
      "title": "A time-restricted self-attention layer for asr,",
      "year": "2018",
      "authors": "[42]  D. Povey, H. Hadian, P. Ghahremani, K. Li, and S. Khudanpur,",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "A few options are promising to investigate in the future: 1) using restricted self-attention[42]which only attends on recent actions rather than all actions, and distant actions can be considered in higher layers; 2) splitting long sequences into short segments as in[22]"
        ]
      }
    },
    {
      "id": "2017-what_your_images_reveal:_exploiting_visual_contents_for_point-of-interest_recommendation",
      "type": "core",
      "title": "What your images reveal: Exploiting visual contents for point-of-interest recommendation,",
      "year": "2017",
      "authors": "[10]  S. Wang, Y. Wang, J. Tang, K. Shu, S. Ranganath, and H. Liu,",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "images[10,11], text[12,13], etc"
        ]
      }
    },
    {
      "id": "2017-visually-aware_fashion_recommendation_and_design_with_generative_image_models",
      "type": "core",
      "title": "Visually-aware fashion recommendation and design with generative image models,",
      "year": "2017",
      "authors": "Wang-Cheng Kang, Chen Fang, Zhaowen Wang, Julian McAuley",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "images[10,11], text[12,13], etc"
        ]
      }
    },
    {
      "id": "2017-translation-based_recommendation",
      "type": "core",
      "title": "Translation-based recommendation",
      "year": "2017",
      "authors": "Ruining He, Wang-Cheng Kang, Julian McAuley",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "SQL-Rank[40]and item-based recommendations[28]are omitted because the former is similar to BPR[26]except using the listwise loss function instead of the pairwise loss function and the latter has been shown inferior to TransRec[8]",
          "TransRec: a first-order sequential recommendation method[8]in which items are embedded into a transition space and users are modelled as translation vectors operating on item sequences"
        ],
        "2018-self-attentive_sequential_recommendation": [
          "Since the last visited item is often the key factor affecting the user’s next action (essentially providing ‘context’), first-order MC based methods show strong performance, especially on sparse datasets[19]",
          "We adopt two common Top-N metrics, Hit Rate@10 and NDCG@10, to evaluate recommendation performance[14,19]",
          "For example, existing sequential recommendation methods have shown that the last visited item plays a key role on predicting the next item[1,21,19]",
          "FPMC[1], HRM[43], TransRec[19]) or high-order MCs (e",
          "We followed the same preprocessing procedure from[19,21,1]",
          "Translation-based Recommendation (TransRec)[19]"
        ]
      }
    },
    {
      "id": "2017-recurrent_recommender_networks",
      "type": "core",
      "title": "Recurrent recommender networks,",
      "year": "2017",
      "authors": "[17]  C. Wu, A. Ahmed, A. Beutel, A. J. Smola, and H. Jing,",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          ")[17,18,16]",
          "Another line of work seeks to use RNNs to model user action sequences[2,26,17]",
          "We also don’t include temporal recommendation methods like TimeSVD++[16]and RRN[17], which differ in setting from what we consider here"
        ]
      }
    },
    {
      "id": "2017-recurrent_neural_networks_with_top-k_gains_for_session-based_recommendations",
      "type": "core",
      "title": "Recurrent neural networks with top-k gains for session-based recommendations,",
      "year": "2017",
      "authors": "Balázs Hidasi, Alexandros Karatzoglou",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "GRU4Rec++{}^{\\text{+}}[26]",
          "Another line of work seeks to use RNNs to model user action sequences[2,26,17]",
          "For example, GRU4Rec uses Gated Recurrent Units (GRU) to model click sequences for session-based recommendation[2], and an improved version further boosts its Top-N recommendation performance[26]"
        ]
      }
    },
    {
      "id": "2017-neural_survival_recommender",
      "type": "core",
      "title": "Neural survival recommender,",
      "year": "2017",
      "authors": "[23]  H. Jing and A. J. Smola,",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "Other than MC-based methods, another line of work adopts RNNs to model user sequences[23,2,24,25]"
        ]
      }
    },
    {
      "id": "2017-neural_collaborative_filtering",
      "type": "core",
      "title": "Neural collaborative filtering,",
      "year": "2017",
      "authors": "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "We adopt two common Top-N metrics, Hit Rate@10 and NDCG@10, to evaluate recommendation performance[14,19]",
          "For example, NeuMF[14]estimates user preferences via Multi-Layer Perceptions (MLP), and AutoRec[15]predicts ratings using autoencoders",
          "To avoid heavy computation on all user-item pairs, we followed the strategy in[48,14]"
        ]
      }
    },
    {
      "id": "2017-deep_learning_based_recommender_system:_a_survey_and_new_perspectives",
      "type": "core",
      "title": "Deep learning based recommender system: A survey and new perspectives,",
      "year": "2017",
      "authors": "Shuai Zhang, Lina Yao, Aixin Sun, Yi Tay",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "Recently, due to their success in related problems, various deep learning techniques have been introduced for recommendation[9]"
        ]
      }
    },
    {
      "id": "2017-attentive_collaborative_filtering:_multimedia_recommendation_with_item-_and_component-level_attention",
      "type": "core",
      "title": "Attentive collaborative filtering: Multimedia recommendation with item- and component-level attention,",
      "year": "2017",
      "authors": "[29]  J. Chen, H. Zhang, X. He, L. Nie, W. Liu, and T. Chua,",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "Recently, attention mechanisms have been incorporated into recommender systems[29,30,31]"
        ]
      }
    },
    {
      "id": "2017-attentional_factorization_machines:_learning_the_weight_of_feature_interactions_via_attention_networks",
      "type": "core",
      "title": "Attentional factorization machines: Learning the weight of feature interactions via attention networks,",
      "year": "2017",
      "authors": "Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, Tat-Seng Chua",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "Recently, attention mechanisms have been incorporated into recommender systems[29,30,31]"
        ]
      }
    },
    {
      "id": "2016-vista:_a_visually_socially_and_temporally-aware_model_for_artistic_recommendation",
      "type": "core",
      "title": "Vista: A visually, socially, and temporally-aware model for artistic recommendation,",
      "year": "2016",
      "authors": "Ruining He, Chen Fang, Zhaowen Wang, Julian McAuley",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "Fossil[21], Vista[20], Caser[22])",
          "There are also methods adopting high-order MCs that consider more previous items[20,21]"
        ]
      }
    },
    {
      "id": "2016-session-based_recommendations_with_recurrent_neural_networks",
      "type": "core",
      "title": "Session-based recommendations with recurrent neural networks,",
      "year": "2016",
      "authors": "Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos Tikk",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "GRU4Rec[2]",
          "Other than MC-based methods, another line of work adopts RNNs to model user sequences[23,2,24,25]",
          "The computational complexity of our model is mainly due to the self-attention layer and the feed-forward network, which isO​(n2​d+n​d2)𝑂superscript𝑛2𝑑𝑛superscript𝑑2O(n^{2}d+nd^{2})",
          "Another line of work seeks to use RNNs to model user action sequences[2,26,17]",
          "MF[40], FPMC[1]and Caser[22]); 2) consider the user’s previous actions, and induce animplicituser embedding from embeddings of visited items (e",
          "Another line of work uses Recurrent Neural Networks (RNNs) to summarize all previous actions via a hidden state, which is used to predict the next action[2]"
        ]
      }
    },
    {
      "id": "2016-fusing_similarity_models_with_markov_chains_for_sparse_sequential_recommendation",
      "type": "core",
      "title": "Fusing similarity models with markov chains for sparse sequential recommendation,",
      "year": "2016",
      "authors": "Ruining He, Julian McAuley",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "PRME[47], HRM[43], Fossil[21]) have been outperformed on similar datasets by baselines among those above, we omit comparison against them",
          "Fossil[21], Vista[20], Caser[22])",
          "For example, existing sequential recommendation methods have shown that the last visited item plays a key role on predicting the next item[1,21,19]",
          "We followed the same preprocessing procedure from[19,21,1]",
          "There are also methods adopting high-order MCs that consider more previous items[20,21]",
          "FSIM[8], Fossil[21], GRU4Rec[2])"
        ]
      }
    },
    {
      "id": "2016-convolutional_matrix_factorization_for_document_context-aware_recommendation",
      "type": "core",
      "title": "Convolutional matrix factorization for document context-aware recommendation,",
      "year": "2016",
      "authors": "[13]  D. H. Kim, C. Park, J. Oh, S. Lee, and H. Yu,",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "images[10,11], text[12,13], etc"
        ]
      }
    },
    {
      "id": "2016-context-aware_sequential_recommendation",
      "type": "core",
      "title": "Context-aware sequential recommendation,",
      "year": "2016",
      "authors": "Qiang Liu, Shu Wu, Diyi Wang, Zhaokang Li, Liang Wang",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "Other than MC-based methods, another line of work adopts RNNs to model user sequences[23,2,24,25]"
        ]
      }
    },
    {
      "id": "2015-show_attend_and_tell:_neural_image_caption_generation_with_visual_attention",
      "type": "core",
      "title": "Show, attend and tell: Neural image caption generation with visual attention,",
      "year": "2015",
      "authors": "Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "Attention mechanisms have been shown to be effective in various tasks such as image captioning[27]and machine translation[28], among others"
        ]
      }
    },
    {
      "id": "2015-personalized_ranking_metric_embedding_for_next_new_poi_recommendation",
      "type": "core",
      "title": "Personalized ranking metric embedding for next new poi recommendation,",
      "year": "2015",
      "authors": "[47]  S. Feng, X. Li, Y. Zeng, G. Cong, Y. M. Chee, and Q. Yuan,",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "PRME[47], HRM[43], Fossil[21]) have been outperformed on similar datasets by baselines among those above, we omit comparison against them"
        ]
      }
    },
    {
      "id": "2015-neural_machine_translation_by_jointly_learning_to_align_and_translate",
      "type": "core",
      "title": "Neural machine translation by jointly learning to align and translate,",
      "year": "2015",
      "authors": "Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "using an RNN encoder-decoder for translation: the encoder’s hidden states are keys and values, and the decoder’s hidden states are queries)[28]",
          "Attention mechanisms have been shown to be effective in various tasks such as image captioning[27]and machine translation[28], among others"
        ]
      }
    },
    {
      "id": "2015-learning_hierarchical_representation_model_for_next_basket_recommendation",
      "type": "core",
      "title": "Learning hierarchical representation model for next basket recommendation,",
      "year": "2015",
      "authors": "[43]  P. Wang, J. Guo, Y. Lan, J. Xu, S. Wan, and X. Cheng,",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "PRME[47], HRM[43], Fossil[21]) have been outperformed on similar datasets by baselines among those above, we omit comparison against them",
          "FPMC[1], HRM[43], TransRec[19]) or high-order MCs (e"
        ]
      }
    },
    {
      "id": "2015-image-based_recommendations_on_styles_and_substitutes",
      "type": "core",
      "title": "Image-based recommendations on styles and substitutes,",
      "year": "2015",
      "authors": "Julian McAuley, Christopher Targett, Qinfeng Shi, Anton van den Hengel",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "Amazon:A series of datasets introduced in[46], comprising large corpora of product reviews crawled fromAmazon"
        ]
      }
    },
    {
      "id": "2015-collaborative_deep_learning_for_recommender_systems",
      "type": "core",
      "title": "Collaborative deep learning for recommender systems",
      "year": "2015",
      "authors": "Hao Wang, Naiyan Wang, Dit-Yan Yeung",
      "name": null,
      "parents": {
        "2016-wide_&_deep_learning_for_recommender_systems": [
          "In the recommender systems literature, collaborative deep learning has been explored by coupling deep learning for content information and collaborative filtering (CF) for the ratings matrix[7]"
        ],
        "2018-self-attentive_sequential_recommendation": [
          "images[10,11], text[12,13], etc"
        ]
      }
    },
    {
      "id": "2015-autorec:_autoencoders_meet_collaborative_filtering",
      "type": "core",
      "title": "Autorec: Autoencoders meet collaborative filtering,",
      "year": "2015",
      "authors": "[15]  S. Sedhain, A. K. Menon, S. Sanner, and L. Xie,",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "For example, NeuMF[14]estimates user preferences via Multi-Layer Perceptions (MLP), and AutoRec[15]predicts ratings using autoencoders"
        ]
      }
    },
    {
      "id": "2014-visualizing_and_understanding_convolutional_networks",
      "type": "core",
      "title": "Visualizing and understanding convolutional networks,",
      "year": "2014",
      "authors": "Matthew D Zeiler, Rob Fergus",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "In some cases, multi-layer neural networks have demonstrated the ability to learn meaningful features hierarchically[34]"
        ]
      }
    },
    {
      "id": "2013-fism:_factored_item_similarity_models_for_top-n_recommender_systems",
      "type": "core",
      "title": "Fism: factored item similarity models for top-n recommender systems,",
      "year": "2013",
      "authors": "[8]  S. Kabbur, X. Ning, and G. Karypis,",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "FISM[8])",
          "FSIM[8], Fossil[21], GRU4Rec[2])",
          "Factorized Item Similarity Models[8]"
        ]
      }
    },
    {
      "id": "2013-an_empirical_analysis_of_dropout_in_piecewise_linear_networks",
      "type": "core",
      "title": "An empirical analysis of dropout in piecewise linear networks,",
      "year": "2013",
      "authors": "David Warde-Farley, Ian J. Goodfellow, Aaron Courville, Yoshua Bengio",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "Further analysis points out that dropout can be viewed as a form of ensemble learning which considers an enormous number of models (exponential in the number of neurons and input features) that share parameters[39]"
        ]
      }
    },
    {
      "id": "2011-advances_in_collaborative_filtering",
      "type": "core",
      "title": "Advances in collaborative filtering,",
      "year": "2011",
      "authors": "Liwei Wu",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "Matrix Factorization (MF) methods seek to uncover latent dimensions to represent users’ preferences and items’ properties, and estimate interactions through the inner product between the user and item embeddings[6,7]"
        ]
      }
    },
    {
      "id": "2010-temporal_collaborative_filtering_with_bayesian_probabilistic_tensor_factorization",
      "type": "core",
      "title": "Temporal collaborative filtering with bayesian probabilistic tensor factorization,",
      "year": "2010",
      "authors": "[18]  L. Xiong, X. Chen, T.-K. Huang, J. Schneider, and J. G. Carbonell,",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          ")[17,18,16]"
        ]
      }
    },
    {
      "id": "2010-factorizing_personalized_markov_chains_for_next-basket_recommendation",
      "type": "core",
      "title": "Factorizing personalized markov chains for next-basket recommendation",
      "year": "2010",
      "authors": "Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "PFMC: a personalized Markov chain model[27]that combines matrix factorization and first-order Markov Chain to take advantage of both users’ latent long-term preferences as well as short-term item transitions",
          "Initially, sequence data in temporal order are usually modelled with Markov models, in which future observation is conditioned on last few observed items[27]"
        ],
        "2018-self-attentive_sequential_recommendation": [
          "Factorized Personalized Markov Chains (FPMC)[1]",
          "For example, existing sequential recommendation methods have shown that the last visited item plays a key role on predicting the next item[1,21,19]",
          "Furthermore, SASRec is also closely related to Factorized Personalized Markov Chains (FPMC)[1], which fuse MF with FMC to capture user preferences and short-term dynamics respectively:",
          "FPMC[1], HRM[43], TransRec[19]) or high-order MCs (e",
          "To provide personalized recommendations, existing methods often take one of two approaches: 1) learn anexplicituser embedding representing user preferences (e",
          "We followed the same preprocessing procedure from[19,21,1]",
          "Markov Chains (MCs) are a classic example, which assume that the next action is conditioned on only the previous action (or previous few), and have been successfully adopted to characterize short-range item transitions for recommendation[1]",
          "For instance, FPMC fuses an MF term and an item-item transition term to capture long-term preferences and short-term transitions respectively[1]"
        ]
      }
    },
    {
      "id": "2010-collaborative_filtering_with_temporal_dynamics",
      "type": "core",
      "title": "Collaborative filtering with temporal dynamics,",
      "year": "2010",
      "authors": "[16]  Y. Koren,",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "TimeSVD++[16]achieved strong results by splitting time into several segments and modeling users and items separately in each",
          "We also don’t include temporal recommendation methods like TimeSVD++[16]and RRN[17], which differ in setting from what we consider here"
        ]
      }
    },
    {
      "id": "2009-matrix_factorization_techniques_for_recommender_systems",
      "type": "core",
      "title": "Matrix factorization techniques for recommender systems",
      "year": "2009",
      "authors": "Jennifer Nguyen, Mu Zhu",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Recommender systems can be divided into those designed for explicit feedback, such as ratings[21], and those for implicit feedback, based on user engagement[14]"
        ],
        "2018-self-attentive_sequential_recommendation": [
          "MF[40], FPMC[1]and Caser[22]); 2) consider the user’s previous actions, and induce animplicituser embedding from embeddings of visited items (e"
        ]
      }
    },
    {
      "id": "2009-bpr:_bayesian_personalized_ranking_from_implicit_feedback",
      "type": "core",
      "title": "Bpr: Bayesian personalized ranking from implicit feedback",
      "year": "2009",
      "authors": "Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars Schmidt-Thieme",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Pairwise methods[26,39]consider each pairwise comparison for a user as a label, which implicitly models the pairwise comparisons as independent observations",
          "BPR: Bayesian personalized ranking for implicit feedback setting[26]",
          "SQL-Rank[40]and item-based recommendations[28]are omitted because the former is similar to BPR[26]except using the listwise loss function instead of the pairwise loss function and the latter has been shown inferior to TransRec[8]",
          "There are multiple ways to define the loss of our model, previously a popular loss is the BPR loss[26,9]:"
        ],
        "2018-self-attentive_sequential_recommendation": [
          "clicks, purchases, comments)[4,5]",
          "Bayesian Personalized Ranking (BPR)[5]"
        ]
      }
    },
    {
      "id": "2008-factorization_meets_the_neighborhood:_a_multifaceted_collaborative_filtering_model",
      "type": "core",
      "title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model",
      "year": "2008",
      "authors": "Yehuda Koren",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Collaborative filtering algorithms including matrix factorization,[11,29,18,24,14], which predict the feedback in a pointwise fashion as if it were a supervised learning problem, fall into the first category"
        ],
        "2018-self-attentive_sequential_recommendation": [
          "To avoid heavy computation on all user-item pairs, we followed the strategy in[48,14]"
        ]
      }
    },
    {
      "id": "2008-collaborative_filtering_for_implicit_feedback_datasets",
      "type": "core",
      "title": "Collaborative filtering for implicit feedback datasets",
      "year": "2008",
      "authors": "Yifan Hu, Yehuda Koren, and Chris Volinsky",
      "name": null,
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Recommender systems can be divided into those designed for explicit feedback, such as ratings[21], and those for implicit feedback, based on user engagement[14]",
          "Collaborative filtering algorithms including matrix factorization,[11,29,18,24,14], which predict the feedback in a pointwise fashion as if it were a supervised learning problem, fall into the first category"
        ],
        "2018-self-attentive_sequential_recommendation": [
          "clicks, purchases, comments)[4,5]"
        ]
      }
    },
    {
      "id": "2001-gradient_flow_in_recurrent_nets:_the_difficulty_of_learning_long-term_dependencies",
      "type": "core",
      "title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies,",
      "year": "2001",
      "authors": "[45]  S. Hochreiter, Y. Bengio, P. Frasconi, J. Schmidhuber et al.,",
      "name": null,
      "parents": {
        "2018-self-attentive_sequential_recommendation": [
          "In contrast, our model hasO​(1)𝑂1O(1)maximum path length, which can be beneficial for learning long-range dependencies[45]"
        ]
      }
    }
  ],
  [
    {
      "id": "2112-self-attention_does_not_need_o​(n2)𝑜superscript𝑛2o(n^{2})_memory",
      "type": "core",
      "title": "Self-attention does not need o​(n2)𝑜superscript𝑛2o(n^{2}) memory",
      "year": "2112",
      "authors": "Markus N Rabe and Charles Staats. 2021",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "com/facebookresearch/xformersis inspired byRabe and Staats (2021)and uses the backward fromDao et al (2022)"
        ]
      }
    },
    {
      "id": "2025-large_language_models_in_machine_translation",
      "type": "core",
      "title": "Large language models in machine translation",
      "year": 2025,
      "authors": "Daniel Scalena, Gabriele Sarti, Arianna Bisazza, Elisabetta Fersini, Malvina Nissim",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "Brants et al (2007)showed the benefits of using language models trained on 2 trillion tokens, resulting in 300 billionn𝑛n-grams, on the quality of machine translation"
        ]
      }
    },
    {
      "id": "2022-using_deepspeed_and_megatron_to_train_megatron-turing_nlg_530b_a_large-scale_generative_language_model",
      "type": "core",
      "title": "Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model",
      "year": 2022,
      "authors": "Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, Bryan Catanzaro",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "This lead to a series ofLarge Language Models, such as Jurassic-1(Lieber et al,2021), Megatron-Turing NLG(Smith et al,2022), Gopher(Rae et al,2021), Chinchilla(Hoffmann et al,2022), PaLM(Chowdhery et al,2022), OPT(Zhang et al,2022), and GLM(Zeng et al,2022)"
        ]
      }
    },
    {
      "id": "2022-training_language_models_to_follow_instructions_with_human_feedback",
      "type": "core",
      "title": "Training language models to follow instructions with human feedback",
      "year": 2022,
      "authors": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "We follow the QA prompt style used inOuyang et al (2022), and report the performance of GPT-3 from the same paper"
        ],
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "The scaling-up of parameters and data has led to significant advancements in the capabilities of LLMs, including in-context learning(Brown et al2020; Liu et al2021; Rubin, Herzig, and Berant2021), instruction following(Ouyang et al2022; Touvron et al2023a; OpenAI2023), planning and reasoning(Wei et al2022; Wang et al2022a; Yao et al2022; Yang et al2023; Wang et al2023b)"
        ],
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "Next, Reinforcement Learning from Human Feedback (RLHF)[18]is then applied to further improve the model’s instruction-following ability",
          "PRM plays an essential role in this process by incorporating online reinforcement learning to optimise reasoning tasks[18]",
          "Proposed methods range from collecting specialised training data to building reward models[18,11,15]and increasing the computational complexity of decoding[24,33], but none have yet achieved significant performance breakthroughs at scale"
        ]
      }
    },
    {
      "id": "2022-training_compute-optimal_large_language_models",
      "type": "core",
      "title": "Training compute-optimal large language models",
      "year": 2022,
      "authors": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "The objective of the scaling laws fromHoffmann et al (2022)is to determine how to best scale the dataset and model sizes for a particulartrainingcompute budget",
          "This was also observed in previous workZhang et al (2022), with the notable exception ofHoffmann et al (2022)where they do not see a difference between Chinchilla and Gopher, despite different sizes",
          "Our training approach is similar to the methods described in previous workBrown et al (2020); Chowdhery et al (2022), and is inspired by the Chinchilla scaling lawsHoffmann et al (2022)",
          "These few-shot properties first appeared when scaling models to a sufficient sizeKaplan et al (2020), resulting in a line of work that focuses on further scaling these modelsChowdhery et al (2022); Rae et al (2021)",
          "Several recent workZhang et al (2022); Hoffmann et al (2022)have considered the RealToxicityPrompts benchmarkGehman et al (2020)as an indicator of how toxic is their model",
          "This lead to a series ofLarge Language Models, such as Jurassic-1(Lieber et al,2021), Megatron-Turing NLG(Smith et al,2022), Gopher(Rae et al,2021), Chinchilla(Hoffmann et al,2022), PaLM(Chowdhery et al,2022), OPT(Zhang et al,2022), and GLM(Zeng et al,2022)",
          "A similar observation was made in previous workRae et al (2021); Hoffmann et al (2022), and is likely indicative of gender bias",
          "We compare LLaMA with other foundation models, namely the non-publicly available language models GPT-3(Brown et al,2020), Gopher(Rae et al,2021), Chinchilla(Hoffmann et al,2022)and PaLM(Chowdhery et al,2022), as well as the open-sourced OPT models(Zhang et al,2022), GPT-J(Wang and Komatsuzaki,2021), and GPT-Neo(Black et al,2022)"
        ]
      }
    },
    {
      "id": "2022-solving_quantitative_reasoning_problems_with_language_models",
      "type": "core",
      "title": "Solving quantitative reasoning problems with language models",
      "year": 2022,
      "authors": "Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "In Table7, we compare with PaLM and MinervaLewkowycz et al (2022)",
          "FollowingLewkowycz et al (2022), we removed everything before the first section, as well as the bibliography"
        ],
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "Recently emerged large reasoning models (LRMs), exemplified by OpenAI’s o1[22], Qwen-QwQ[54]and DeepSeek-R1[7], employ large-scale reinforcement learning foster impressive long-sequence stepwise reasoning capabilities, offering promising solutions to complex reasoning problems[46,31,59,84,73,74,67]"
        ]
      }
    },
    {
      "id": "2022-self-consistency_improves_chain_of_thought_reasoning_in_language_models",
      "type": "core",
      "title": "Self-consistency improves chain of thought reasoning in language models",
      "year": 2022,
      "authors": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "In Table7, we compare with PaLM and MinervaLewkowycz et al (2022)"
        ],
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "The scaling-up of parameters and data has led to significant advancements in the capabilities of LLMs, including in-context learning(Brown et al2020; Liu et al2021; Rubin, Herzig, and Berant2021), instruction following(Ouyang et al2022; Touvron et al2023a; OpenAI2023), planning and reasoning(Wei et al2022; Wang et al2022a; Yao et al2022; Yang et al2023; Wang et al2023b)"
        ]
      }
    },
    {
      "id": "2022-scaling_instruction-finetuned_language_models",
      "type": "core",
      "title": "Scaling instruction-finetuned language models",
      "year": 2022,
      "authors": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, Jason Wei",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "LLaMA-65B fine-tuned with the protocol and instruction dataset fromChung et al (2022)",
          "Additionally, we observed likeChung et al (2022)that finetuning these models on instructions lead to promising results, and we plan to further investigate this in future work",
          "In Table10, we report the results of our instruct model LLaMA-I on MMLU and compare with existing instruction finetuned models of moderate sizes, namely, OPT-IMLIyer et al (2022)and the Flan-PaLM seriesChung et al (2022)",
          "Since this is not the focus of this paper, we only conducted a single experiment following the same protocol asChung et al (2022)to train an instruct model, LLaMA-I",
          "We compare LLaMA with other foundation models, namely the non-publicly available language models GPT-3(Brown et al,2020), Gopher(Rae et al,2021), Chinchilla(Hoffmann et al,2022)and PaLM(Chowdhery et al,2022), as well as the open-sourced OPT models(Zhang et al,2022), GPT-J(Wang and Komatsuzaki,2021), and GPT-Neo(Black et al,2022)"
        ]
      }
    },
    {
      "id": "2022-reducing_activation_recomputation_in_large_transformer_models",
      "type": "core",
      "title": "Reducing activation recomputation in large transformer models",
      "year": 2022,
      "authors": "Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, Bryan Catanzaro",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "To fully benefit from this optimization, we need to reduce the memory usage of the model by using model and sequence parallelism, as described byKorthikanti et al (2022)"
        ]
      }
    },
    {
      "id": "2022-palm:_scaling_language_modeling_with_pathways",
      "type": "core",
      "title": "Palm: Scaling language modeling with pathways",
      "year": 2022,
      "authors": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "Our training approach is similar to the methods described in previous workBrown et al (2020); Chowdhery et al (2022), and is inspired by the Chinchilla scaling lawsHoffmann et al (2022)",
          "These few-shot properties first appeared when scaling models to a sufficient sizeKaplan et al (2020), resulting in a line of work that focuses on further scaling these modelsChowdhery et al (2022); Rae et al (2021)",
          "For instance, PaLM-Coder(Chowdhery et al,2022)increases the pass@1 score of PaLM on HumanEval from 26",
          "The values marked with∗are read from figures inChowdhery et al (2022)",
          "This lead to a series ofLarge Language Models, such as Jurassic-1(Lieber et al,2021), Megatron-Turing NLG(Smith et al,2022), Gopher(Rae et al,2021), Chinchilla(Hoffmann et al,2022), PaLM(Chowdhery et al,2022), OPT(Zhang et al,2022), and GLM(Zeng et al,2022)",
          "We compare LLaMA with other foundation models, namely the non-publicly available language models GPT-3(Brown et al,2020), Gopher(Rae et al,2021), Chinchilla(Hoffmann et al,2022)and PaLM(Chowdhery et al,2022), as well as the open-sourced OPT models(Zhang et al,2022), GPT-J(Wang and Komatsuzaki,2021), and GPT-Neo(Black et al,2022)"
        ],
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "Large language models (LLMs), such as GPT-3(Brown et al2020)and PaLM(Chowdhery et al2022), have made significant strides in recent years, demonstrating remarkable capabilities in artificial general intelligence and revolutionizing the field of natural language processing"
        ]
      }
    },
    {
      "id": "2022-opt:_open_pre-trained_transformer_language_models",
      "type": "core",
      "title": "Opt: Open pre-trained transformer language models",
      "year": 2022,
      "authors": "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "This was also observed in previous workZhang et al (2022), with the notable exception ofHoffmann et al (2022)where they do not see a difference between Chinchilla and Gopher, despite different sizes",
          "Several recent workZhang et al (2022); Hoffmann et al (2022)have considered the RealToxicityPrompts benchmarkGehman et al (2020)as an indicator of how toxic is their model",
          "There exist some exceptions, notably OPT(Zhang et al,2022), GPT-NeoX(Black et al,2022), BLOOM(Scao et al,2022)and GLM(Zeng et al,2022), but none that are competitive with PaLM-62B or Chinchilla",
          "This lead to a series ofLarge Language Models, such as Jurassic-1(Lieber et al,2021), Megatron-Turing NLG(Smith et al,2022), Gopher(Rae et al,2021), Chinchilla(Hoffmann et al,2022), PaLM(Chowdhery et al,2022), OPT(Zhang et al,2022), and GLM(Zeng et al,2022)",
          "We compare LLaMA with other foundation models, namely the non-publicly available language models GPT-3(Brown et al,2020), Gopher(Rae et al,2021), Chinchilla(Hoffmann et al,2022)and PaLM(Chowdhery et al,2022), as well as the open-sourced OPT models(Zhang et al,2022), GPT-J(Wang and Komatsuzaki,2021), and GPT-Neo(Black et al,2022)"
        ]
      }
    },
    {
      "id": "2022-opt-iml:_scaling_language_model_instruction_meta_learning_through_the_lens_of_generalization",
      "type": "core",
      "title": "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
      "year": 2022,
      "authors": "Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li, Brian O'Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli Celikyilmaz, Luke Zettlemoyer, Ves Stoyanov",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "We compare LLaMA with other foundation models, namely the non-publicly available language models GPT-3(Brown et al,2020), Gopher(Rae et al,2021), Chinchilla(Hoffmann et al,2022)and PaLM(Chowdhery et al,2022), as well as the open-sourced OPT models(Zhang et al,2022), GPT-J(Wang and Komatsuzaki,2021), and GPT-Neo(Black et al,2022)",
          "In Table10, we report the results of our instruct model LLaMA-I on MMLU and compare with existing instruction finetuned models of moderate sizes, namely, OPT-IMLIyer et al (2022)and the Flan-PaLM seriesChung et al (2022)"
        ]
      }
    },
    {
      "id": "2022-lamda:_language_models_for_dialog_applications",
      "type": "core",
      "title": "Lamda: Language models for dialog applications",
      "year": 2022,
      "authors": "Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, Quoc Le",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "In Table8, we compare the pass@1 scores of our models with existing language models that have not been finetuned on code, namely PaLM and LaMDAThoppilan et al (2022)"
        ],
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "For example,(Wang et al2023c; Zhong et al2023; Liu et al2023b)have equipped LLMs with an external memory, empowering LLMs with growth potential Regarding the planning, CoT(Wei et al2022; Kojima et al2022)and ReAct(Yao et al2022)propose to enhance planning by step-wise reasoning; ToT(Yao et al2023)and GoT(Besta et al2023)introduce multi-path reasoning to ensure consistency and correctness; Self-Refine(Madaan et al2023)and Reflexion(Shinn et al2023)lead the LLMs to reflect on errors, with the ultimate goal of improving their subsequent problem-solving success rates"
        ]
      }
    },
    {
      "id": "2022-incoder:_a_generative_model_for_code_infilling_and_synthesis",
      "type": "core",
      "title": "Incoder: A generative model for code infilling and synthesis",
      "year": 2022,
      "authors": "Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, Mike Lewis",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "For instance, PaLM-Coder(Chowdhery et al,2022)increases the pass@1 score of PaLM on HumanEval from 26"
        ]
      }
    },
    {
      "id": "2022-gpt-neox-20b:_an_open-source_autoregressive_language_model",
      "type": "core",
      "title": "Gpt-neox-20b: An open-source autoregressive language model",
      "year": 2022,
      "authors": "Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "There exist some exceptions, notably OPT(Zhang et al,2022), GPT-NeoX(Black et al,2022), BLOOM(Scao et al,2022)and GLM(Zeng et al,2022), but none that are competitive with PaLM-62B or Chinchilla",
          "We compare LLaMA with other foundation models, namely the non-publicly available language models GPT-3(Brown et al,2020), Gopher(Rae et al,2021), Chinchilla(Hoffmann et al,2022)and PaLM(Chowdhery et al,2022), as well as the open-sourced OPT models(Zhang et al,2022), GPT-J(Wang and Komatsuzaki,2021), and GPT-Neo(Black et al,2022)"
        ]
      }
    },
    {
      "id": "2022-glm-130b:_an_open_bilingual_pre-trained_model",
      "type": "core",
      "title": "Glm-130b: An open bilingual pre-trained model",
      "year": 2022,
      "authors": "Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, Jie Tang",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "This lead to a series ofLarge Language Models, such as Jurassic-1(Lieber et al,2021), Megatron-Turing NLG(Smith et al,2022), Gopher(Rae et al,2021), Chinchilla(Hoffmann et al,2022), PaLM(Chowdhery et al,2022), OPT(Zhang et al,2022), and GLM(Zeng et al,2022)",
          "There exist some exceptions, notably OPT(Zhang et al,2022), GPT-NeoX(Black et al,2022), BLOOM(Scao et al,2022)and GLM(Zeng et al,2022), but none that are competitive with PaLM-62B or Chinchilla"
        ]
      }
    },
    {
      "id": "2022-flashattention:_fast_and_memory-efficient_exact_attention_with_io-awareness",
      "type": "core",
      "title": "Flashattention: Fast and memory-efficient exact attention with io-awareness",
      "year": 2022,
      "authors": "Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Ré",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "com/facebookresearch/xformersis inspired byRabe and Staats (2021)and uses the backward fromDao et al (2022)"
        ]
      }
    },
    {
      "id": "2022-emergent_abilities_of_large_language_models",
      "type": "core",
      "title": "Emergent abilities of large language models",
      "year": 2022,
      "authors": "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "This lead to a series ofLarge Language Models, such as Jurassic-1(Lieber et al,2021), Megatron-Turing NLG(Smith et al,2022), Gopher(Rae et al,2021), Chinchilla(Hoffmann et al,2022), PaLM(Chowdhery et al,2022), OPT(Zhang et al,2022), and GLM(Zeng et al,2022)"
        ]
      }
    },
    {
      "id": "2022-codegen:_an_open_large_language_model_for_code_with_multi-turn_program_synthesis",
      "type": "core",
      "title": "Codegen: An open large language model for code with multi-turn program synthesis",
      "year": 2022,
      "authors": "Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "For instance, PaLM-Coder(Chowdhery et al,2022)increases the pass@1 score of PaLM on HumanEval from 26"
        ]
      }
    },
    {
      "id": "2022-bloom:_a_176b-parameter_open-access_multilingual_language_model",
      "type": "core",
      "title": "Bloom: A 176b-parameter open-access multilingual language model",
      "year": 2022,
      "authors": "BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Taşar, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre François Lavallée, Rémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aurélie Névéol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdeněk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Muñoz Ferrandis, Daniel McDuff, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clémentine Fourrier, Daniel León Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria A Castillo, Marianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, Thomas Wolf",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "There exist some exceptions, notably OPT(Zhang et al,2022), GPT-NeoX(Black et al,2022), BLOOM(Scao et al,2022)and GLM(Zeng et al,2022), but none that are competitive with PaLM-62B or Chinchilla"
        ]
      }
    },
    {
      "id": "2021-truthfulqa:_measuring_how_models_mimic_human_falsehoods",
      "type": "core",
      "title": "Truthfulqa: Measuring how models mimic human falsehoods",
      "year": 2021,
      "authors": "Stephanie Lin, Jacob Hilton, Owain Evans",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "TruthfulQA(Lin et al,2021)aims to measure the truthfulness of a model, i"
        ]
      }
    },
    {
      "id": "2021-training_verifiers_to_solve_math_word_problems",
      "type": "core",
      "title": "Training verifiers to solve math word problems",
      "year": 2021,
      "authors": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "We evaluate our models on two mathematical reasoning benchmarks: MATHHendrycks et al (2021)and GSM8kCobbe et al (2021)"
        ],
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "Papers like[4]introduced the earliest formal attempt (outcome reward only) at using verifiers in mathematical reasoning tasks, laying the groundwork for subsequent research"
        ],
        "2025-training_large_language_models_to_reason_in_a_continuous_latent_space": [
          "For math reasoning (GSM8k,Cobbe et al,2021), using continuous thoughts is shown to be beneficial to reasoning accuracy, mirroring the effects of language reasoning chains",
          "We use GSM8k(Cobbe et al,2021)as the dataset for math reasoning"
        ]
      }
    },
    {
      "id": "2021-sustainable_ai:_environmental_implications_challenges_and_opportunities",
      "type": "core",
      "title": "Sustainable ai: Environmental implications, challenges and opportunities",
      "year": 2021,
      "authors": "Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang, Charles Bai, Michael Gschwind, Anurag Gupta, Myle Ott, Anastasia Melnikov, Salvatore Candido, David Brooks, Geeta Chauhan, Benjamin Lee, Hsien-Hsin S. Lee, Bugra Akyildiz, Maximilian Balandat, Joe Spisak, Ravi Jain, Mike Rabbat, Kim Hazelwood",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "We followWu et al (2022)to compute carbon emission of training OPT, BLOOM and our models in the same data center",
          "We follow a formula forWu et al (2022)to estimate the Watt-hour, Wh, needed to train a model, as well as the tons of carbon emissions, tCO2eq"
        ]
      }
    },
    {
      "id": "2021-scaling_language_models:_methods_analysis_&_insights_from_training_gopher",
      "type": "core",
      "title": "Scaling language models: Methods, analysis & insights from training gopher",
      "year": 2021,
      "authors": "Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, Geoffrey Irving",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "This lead to a series ofLarge Language Models, such as Jurassic-1(Lieber et al,2021), Megatron-Turing NLG(Smith et al,2022), Gopher(Rae et al,2021), Chinchilla(Hoffmann et al,2022), PaLM(Chowdhery et al,2022), OPT(Zhang et al,2022), and GLM(Zeng et al,2022)",
          "A similar observation was made in previous workRae et al (2021); Hoffmann et al (2022), and is likely indicative of gender bias",
          "These few-shot properties first appeared when scaling models to a sufficient sizeKaplan et al (2020), resulting in a line of work that focuses on further scaling these modelsChowdhery et al (2022); Rae et al (2021)",
          "We compare LLaMA with other foundation models, namely the non-publicly available language models GPT-3(Brown et al,2020), Gopher(Rae et al,2021), Chinchilla(Hoffmann et al,2022)and PaLM(Chowdhery et al,2022), as well as the open-sourced OPT models(Zhang et al,2022), GPT-J(Wang and Komatsuzaki,2021), and GPT-Neo(Black et al,2022)"
        ]
      }
    },
    {
      "id": "2021-roformer:_enhanced_transformer_with_rotary_position_embedding",
      "type": "core",
      "title": "Roformer: Enhanced transformer with rotary position embedding",
      "year": 2021,
      "authors": "Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "We remove the absolute positional embeddings, and instead, add rotary positional embeddings (RoPE), introduced bySu et al (2021), at each layer of the network"
        ]
      }
    },
    {
      "id": "2021-program_synthesis_with_large_language_models",
      "type": "core",
      "title": "Program synthesis with large language models",
      "year": 2021,
      "authors": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, Charles Sutton",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "HumanEval generations are done in zero-shot and MBBP with 3-shot prompts similar toAustin et al (2021)",
          "We evaluate the ability of our models to write code from a natural language description on two benchmarks: HumanEvalChen et al (2021)and MBPPAustin et al (2021)"
        ]
      }
    },
    {
      "id": "2021-measuring_mathematical_problem_solving_with_the_math_dataset",
      "type": "core",
      "title": "Measuring mathematical problem solving with the math dataset",
      "year": 2021,
      "authors": "Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "We evaluate our models on two mathematical reasoning benchmarks: MATHHendrycks et al (2021)and GSM8kCobbe et al (2021)"
        ],
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "MATH500 consists of 500 questions from the MATH test set[16]"
        ]
      }
    },
    {
      "id": "2021-evaluating_large_language_models_trained_on_code",
      "type": "core",
      "title": "Evaluating large language models trained on code",
      "year": 2021,
      "authors": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "Other models trained specifically for code also perform better than general models on these tasks(Chen et al,2021; Nijkamp et al,2022; Fried et al,2022)",
          "We use the same method asChen et al (2021)to obtain unbiased estimates of the pass@k",
          "We evaluate the ability of our models to write code from a natural language description on two benchmarks: HumanEvalChen et al (2021)and MBPPAustin et al (2021)"
        ]
      }
    },
    {
      "id": "2021-a_neural_probabilistic_language_model",
      "type": "core",
      "title": "A neural probabilistic language model",
      "year": 2021,
      "authors": "Simeng Sun, Mohit Iyyer",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "In the past two decades, neural networks have been successfully applied to the language modelling task, starting from feed forward models(Bengio et al,2000), recurrent neural networks(Elman,1990; Mikolov et al,2010)and LSTMs(Hochreiter and Schmidhuber,1997; Graves,2013)"
        ]
      }
    },
    {
      "id": "2021-a_mathematical_theory_of_communication",
      "type": "core",
      "title": "A mathematical theory of communication",
      "year": 2021,
      "authors": "Hsin-Po Wang",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "are probability distributions over sequences of words, tokens or characters(Shannon,1948,1951)"
        ]
      }
    },
    {
      "id": "2020-the_pile:_an_800gb_dataset_of_diverse_text_for_language_modeling",
      "type": "core",
      "title": "The Pile: An 800gb dataset of diverse text for language modeling",
      "year": 2020,
      "authors": "Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "We include two book corpora in our training dataset: the Gutenberg Project, which contains books that are in the public domain, and the Books3 section of ThePile(Gao et al,2020), a publicly available dataset for training large language models"
        ]
      }
    },
    {
      "id": "2020-scaling_laws_for_neural_language_models",
      "type": "core",
      "title": "Scaling laws for neural language models",
      "year": 2020,
      "authors": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "Large Languages Models (LLMs) trained on massive corpora of texts have shown their ability to perform new tasks from textual instructions or from a few examplesBrown et al (2020)",
          "Notable models include BERT(Devlin et al,2018), GPT-2(Radford et al,2019), Megatron-LM(Shoeybi et al,2019), and T5(Raffel et al,2020)"
        ],
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "Building on substantial performance gains, OpenAI o1 has shown that the scaling principles traditionally applied during training[9,24]are now relevant to the inference phase"
        ]
      }
    },
    {
      "id": "2020-realtoxicityprompts:_evaluating_neural_toxic_degeneration_in_language_models",
      "type": "core",
      "title": "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
      "year": 2020,
      "authors": "Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, Noah A. Smith",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "Several recent workZhang et al (2022); Hoffmann et al (2022)have considered the RealToxicityPrompts benchmarkGehman et al (2020)as an indicator of how toxic is their model",
          "Large language models have been showed to reproduce and amplify biases that are existing in the training data(Sheng et al,2019; Kurita et al,2019), and to generate toxic or offensive content(Gehman et al,2020)"
        ]
      }
    },
    {
      "id": "2020-measuring_massive_multitask_language_understanding",
      "type": "core",
      "title": "Measuring massive multitask language understanding",
      "year": 2020,
      "authors": "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "The massive multitask language understanding benchmark, or MMLU, introduced byHendrycks et al (2020)consists of multiple choice questions covering various domains of knowledge, including humanities, STEM and social sciences"
        ]
      }
    },
    {
      "id": "2020-language_models_are_few-shot_learners",
      "type": "core",
      "title": "Language models are few-shot learners",
      "year": 2020,
      "authors": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "Our training approach is similar to the methods described in previous workBrown et al (2020); Chowdhery et al (2022), and is inspired by the Chinchilla scaling lawsHoffmann et al (2022)",
          "We follow the evaluation setup fromBrown et al (2020)and report results in Table6",
          "Notable models include BERT(Devlin et al,2018), GPT-2(Radford et al,2019), Megatron-LM(Shoeybi et al,2019), and T5(Raffel et al,2020)",
          "We followGao et al (2021)and use the likelihood normalized by the number of characters in the completion, except for certain datasets (OpenBookQA, BoolQ), for which we followBrown et al (2020), and select a completion based on the likelihood normalized by the likelihood of the completion given “Answer:” as context:P(𝚌𝚘𝚖𝚙𝚕𝚎𝚝𝚒𝚘𝚗∣𝚌𝚘𝚗𝚝𝚎𝚡𝚝)/P(𝚌𝚘𝚖𝚙𝚕𝚎𝚝𝚒𝚘𝚗∣``Answer:\"){\\scriptstyle P(\\mathtt{completion}\\mid\\mathtt{context})/P(\\mathtt{completion}\\mid``Answer:\")}",
          "Large Languages Models (LLMs) trained on massive corpora of texts have shown their ability to perform new tasks from textual instructions or from a few examplesBrown et al (2020)",
          "We compare LLaMA with other foundation models, namely the non-publicly available language models GPT-3(Brown et al,2020), Gopher(Rae et al,2021), Chinchilla(Hoffmann et al,2022)and PaLM(Chowdhery et al,2022), as well as the open-sourced OPT models(Zhang et al,2022), GPT-J(Wang and Komatsuzaki,2021), and GPT-Neo(Black et al,2022)",
          "Following previous work(Brown et al,2020), we consider zero-shot and few-shot tasks, and report results on a total of 20 benchmarks:"
        ],
        "2023-recommender_ai_agent:_integrating_large_language_models_for_interactive_recommendations": [
          "The scaling-up of parameters and data has led to significant advancements in the capabilities of LLMs, including in-context learning(Brown et al2020; Liu et al2021; Rubin, Herzig, and Berant2021), instruction following(Ouyang et al2022; Touvron et al2023a; OpenAI2023), planning and reasoning(Wei et al2022; Wang et al2022a; Yao et al2022; Yang et al2023; Wang et al2023b)",
          "Large language models (LLMs), such as GPT-3(Brown et al2020)and PaLM(Chowdhery et al2022), have made significant strides in recent years, demonstrating remarkable capabilities in artificial general intelligence and revolutionizing the field of natural language processing"
        ],
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "With the emergence of Transformers(Vaswani,2017)and the scaling laws(Henighan et al,2020), the researchers try to scale up the parameters of the generative language model"
        ],
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "wherextsubscript𝑥𝑡x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTis thet𝑡titalic_t-th token,x<tsubscript𝑥absent𝑡x_{<t}italic_x start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPTrepresents all tokens beforet𝑡titalic_t,θ𝜃\\thetaitalic_θare the model parameters, andP𝑃Pitalic_Pis the probability distribution over the vocabulary[2]"
        ],
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "While this characteristic enhances logical coherence and interpretability of reasoning, an extended chain of thought may cause overthinking[4]and increased risks of knowledge insufficiency[60,51,2], where any knowledge gap can propagate errors and disrupt the entire reasoning chain[79,40,44,41]"
        ]
      }
    },
    {
      "id": "2020-glu_variants_improve_transformer",
      "type": "core",
      "title": "Glu variants improve transformer",
      "year": 2020,
      "authors": "Noam Shazeer",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "We replace the ReLU non-linearity by the SwiGLU activation function, introduced byShazeer (2020)to improve the performance"
        ]
      }
    },
    {
      "id": "2020-crows-pairs:_a_challenge_dataset_for_measuring_social_biases_in_masked_language_models",
      "type": "core",
      "title": "CrowS-pairs: A challenge dataset for measuring social biases in masked language models",
      "year": 2020,
      "authors": "Nikita Nangia, Clara Vania, Rasika Bhalerao, Samuel R. Bowman",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "We evaluate the biases in our model on the CrowS-PairsNangia et al (2020)"
        ]
      }
    },
    {
      "id": "2019-winogrande:_an_adversarial_winograd_schema_challenge_at_scale",
      "type": "core",
      "title": "Winogrande: An adversarial winograd schema challenge at scale",
      "year": 2019,
      "authors": "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "We consider eight standard common sense reasoning benchmarks: BoolQ(Clark et al,2019), PIQA(Bisk et al,2020), SIQA(Sap et al,2019), HellaSwag(Zellers et al,2019), WinoGrande(Sakaguchi et al,2021), ARC easy and challenge(Clark et al,2018)and OpenBookQA(Mihaylov et al,2018)"
        ]
      }
    },
    {
      "id": "2019-transformer-xl:_attentive_language_models_beyond_a_fixed-length_context",
      "type": "core",
      "title": "Transformer-xl: Attentive language models beyond a fixed-length context",
      "year": 2019,
      "authors": "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ],
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "More recently, transformer networks, based on self-attention, have led to important improvements, especially for capturing long range dependencies(Vaswani et al,2017; Radford et al,2018; Dai et al,2019)"
        ]
      }
    },
    {
      "id": "2019-the_woman_worked_as_a_babysitter:_on_biases_in_language_generation",
      "type": "core",
      "title": "The woman worked as a babysitter: On biases in language generation",
      "year": 2019,
      "authors": "Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, Nanyun Peng",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "Large language models have been showed to reproduce and amplify biases that are existing in the training data(Sheng et al,2019; Kurita et al,2019), and to generate toxic or offensive content(Gehman et al,2020)"
        ]
      }
    },
    {
      "id": "2019-socialiqa:_commonsense_reasoning_about_social_interactions",
      "type": "core",
      "title": "Socialiqa: Commonsense reasoning about social interactions",
      "year": 2019,
      "authors": "Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, Yejin Choi",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "We consider eight standard common sense reasoning benchmarks: BoolQ(Clark et al,2019), PIQA(Bisk et al,2020), SIQA(Sap et al,2019), HellaSwag(Zellers et al,2019), WinoGrande(Sakaguchi et al,2021), ARC easy and challenge(Clark et al,2018)and OpenBookQA(Mihaylov et al,2018)"
        ]
      }
    },
    {
      "id": "2019-root_mean_square_layer_normalization",
      "type": "core",
      "title": "Root mean square layer normalization",
      "year": 2019,
      "authors": "Biao Zhang, Rico Sennrich",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "We use the RMSNorm normalizing function, introduced byZhang and Sennrich (2019)"
        ]
      }
    },
    {
      "id": "2019-piqa:_reasoning_about_physical_commonsense_in_natural_language",
      "type": "core",
      "title": "Piqa: Reasoning about physical commonsense in natural language",
      "year": 2019,
      "authors": "Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, Yejin Choi",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "We consider eight standard common sense reasoning benchmarks: BoolQ(Clark et al,2019), PIQA(Bisk et al,2020), SIQA(Sap et al,2019), HellaSwag(Zellers et al,2019), WinoGrande(Sakaguchi et al,2021), ARC easy and challenge(Clark et al,2018)and OpenBookQA(Mihaylov et al,2018)"
        ]
      }
    },
    {
      "id": "2019-megatron-lm:_training_multi-billion_parameter_language_models_using_model_parallelism",
      "type": "core",
      "title": "Megatron-lm: Training multi-billion parameter language models using model parallelism",
      "year": 2019,
      "authors": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "Notable models include BERT(Devlin et al,2018), GPT-2(Radford et al,2019), Megatron-LM(Shoeybi et al,2019), and T5(Raffel et al,2020)"
        ]
      }
    },
    {
      "id": "2019-hellaswag:_can_a_machine_really_finish_your_sentence?",
      "type": "core",
      "title": "Hellaswag: Can a machine really finish your sentence?",
      "year": 2019,
      "authors": "Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "We consider eight standard common sense reasoning benchmarks: BoolQ(Clark et al,2019), PIQA(Bisk et al,2020), SIQA(Sap et al,2019), HellaSwag(Zellers et al,2019), WinoGrande(Sakaguchi et al,2021), ARC easy and challenge(Clark et al,2018)and OpenBookQA(Mihaylov et al,2018)"
        ]
      }
    },
    {
      "id": "2019-exploring_the_limits_of_transfer_learning_with_a_unified_text-to-text_transformer",
      "type": "core",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "year": 2019,
      "authors": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ],
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "Notable models include BERT(Devlin et al,2018), GPT-2(Radford et al,2019), Megatron-LM(Shoeybi et al,2019), and T5(Raffel et al,2020)",
          "We thus included the publicly available C4 dataset(Raffel et al,2020)in our data"
        ],
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          "whereQisubscript𝑄𝑖Q_{i}italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTandAisubscript𝐴𝑖A_{i}italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTare thei𝑖iitalic_i-th question and answer pair, respectively[20]"
        ],
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "While this characteristic enhances logical coherence and interpretability of reasoning, an extended chain of thought may cause overthinking[4]and increased risks of knowledge insufficiency[60,51,2], where any knowledge gap can propagate errors and disrupt the entire reasoning chain[79,40,44,41]"
        ]
      }
    },
    {
      "id": "2019-ccnet:_extracting_high_quality_monolingual_datasets_from_web_crawl_data",
      "type": "core",
      "title": "CCNet: Extracting high quality monolingual datasets from web crawl data",
      "year": 2019,
      "authors": "Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, Edouard Grave",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "We preprocess five CommonCrawl dumps, ranging from 2017 to 2020, with the CCNet pipelineWenzek et al (2020)"
        ]
      }
    },
    {
      "id": "2019-boolq:_exploring_the_surprising_difficulty_of_natural_yes/no_questions",
      "type": "core",
      "title": "Boolq: Exploring the surprising difficulty of natural yes/no questions",
      "year": 2019,
      "authors": "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "We consider eight standard common sense reasoning benchmarks: BoolQ(Clark et al,2019), PIQA(Bisk et al,2020), SIQA(Sap et al,2019), HellaSwag(Zellers et al,2019), WinoGrande(Sakaguchi et al,2021), ARC easy and challenge(Clark et al,2018)and OpenBookQA(Mihaylov et al,2018)"
        ]
      }
    },
    {
      "id": "2019-a_constructive_prediction_of_the_generalization_error_across_scales",
      "type": "core",
      "title": "A constructive prediction of the generalization error across scales",
      "year": 2019,
      "authors": "Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, Nir Shavit",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "Notable models include BERT(Devlin et al,2018), GPT-2(Radford et al,2019), Megatron-LM(Shoeybi et al,2019), and T5(Raffel et al,2020)"
        ]
      }
    },
    {
      "id": "2018-think_you_have_solved_question_answering?_try_arc_the_ai2_reasoning_challenge",
      "type": "core",
      "title": "Think you have solved question answering? try arc, the ai2 reasoning challenge",
      "year": 2018,
      "authors": "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "We consider eight standard common sense reasoning benchmarks: BoolQ(Clark et al,2019), PIQA(Bisk et al,2020), SIQA(Sap et al,2019), HellaSwag(Zellers et al,2019), WinoGrande(Sakaguchi et al,2021), ARC easy and challenge(Clark et al,2018)and OpenBookQA(Mihaylov et al,2018)"
        ]
      }
    },
    {
      "id": "2018-sentencepiece:_a_simple_and_language_independent_subword_tokenizer_and_detokenizer_for_neural_text_processing",
      "type": "core",
      "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "year": 2018,
      "authors": "Taku Kudo, John Richardson",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ],
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "We tokenize the data with the byte-pair encoding (BPE) algorithm(Sennrich et al,2015), using the implementation from SentencePiece(Kudo and Richardson,2018)"
        ]
      }
    },
    {
      "id": "2018-gender_bias_in_coreference_resolution",
      "type": "core",
      "title": "Gender bias in coreference resolution",
      "year": 2018,
      "authors": "Rachel Rudinger, Jason Naradowsky, Brian Leonard, Benjamin Van Durme",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "To further investigate the biases of our model on the gender category, we look at the WinoGender benchmark(Rudinger et al,2018), a co-reference resolution dataset"
        ]
      }
    },
    {
      "id": "2018-can_a_suit_of_armor_conduct_electricity?_a_new_dataset_for_open_book_question_answering",
      "type": "core",
      "title": "Can a suit of armor conduct electricity? a new dataset for open book question answering",
      "year": 2018,
      "authors": "Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "We consider eight standard common sense reasoning benchmarks: BoolQ(Clark et al,2019), PIQA(Bisk et al,2020), SIQA(Sap et al,2019), HellaSwag(Zellers et al,2019), WinoGrande(Sakaguchi et al,2021), ARC easy and challenge(Clark et al,2018)and OpenBookQA(Mihaylov et al,2018)"
        ]
      }
    },
    {
      "id": "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding",
      "type": "core",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for  Language Understanding",
      "year": 2018,
      "authors": null,
      "name": "BERT",
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Like sentences in NLP, sequence data in recommendations can be similarly modelled by recurrent neural networks (RNN)[10,9]and convolutional neural network (CNN)[34]models",
          "Recent advances in deep learning, especially the discovery of various attention mechanisms[2,33]and newer architectures[36,4]in addition to classical RNN and CNN architecture in natural language processing, have allowed us to make better use of the temporal ordering of items that each user has engaged with"
        ],
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ],
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "Notable models include BERT(Devlin et al,2018), GPT-2(Radford et al,2019), Megatron-LM(Shoeybi et al,2019), and T5(Raffel et al,2020)"
        ]
      }
    },
    {
      "id": "2017-race:_large-scale_reading_comprehension_dataset_from_examinations",
      "type": "core",
      "title": "RACE: Large-scale ReAding comprehension dataset from examinations",
      "year": 2017,
      "authors": "Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Eduard Hovy",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ],
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "We evaluate our models on the RACE reading comprehension benchmark(Lai et al,2017)"
        ]
      }
    },
    {
      "id": "2017-deep_learning_scaling_is_predictable_empirically",
      "type": "core",
      "title": "Deep learning scaling is predictable, empirically",
      "year": 2017,
      "authors": "Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, Yanqi Zhou",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "Hestness et al (2017)andRosenfeld et al (2019)studied the impact of scaling on the performance of deep learning models, showing the existence of power laws between the model and dataset sizes and the performance of the system"
        ]
      }
    },
    {
      "id": "2017-decoupled_weight_decay_regularization",
      "type": "core",
      "title": "Decoupled weight decay regularization",
      "year": 2017,
      "authors": "Ilya Loshchilov, Frank Hutter",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "Our models are trained using the AdamW optimizer(Loshchilov and Hutter,2017), with the following hyper-parameters:β1=0"
        ]
      }
    },
    {
      "id": "2016-long_short-term_memory",
      "type": "core",
      "title": "Long short-term memory",
      "year": 2016,
      "authors": "Ivo Danihelka, Greg Wayne, Benigno Uria, Nal Kalchbrenner, Alex Graves",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "In the past two decades, neural networks have been successfully applied to the language modelling task, starting from feed forward models(Bengio et al,2000), recurrent neural networks(Elman,1990; Mikolov et al,2010)and LSTMs(Hochreiter and Schmidhuber,1997; Graves,2013)"
        ]
      }
    },
    {
      "id": "1045-recurrent_neural_network_based_language_model",
      "type": "core",
      "title": "Recurrent neural network based language model",
      "year": "1045",
      "authors": "Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernockỳ, and Sanjeev Khudanpur. 2010",
      "name": null,
      "parents": {
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "In the past two decades, neural networks have been successfully applied to the language modelling task, starting from feed forward models(Bengio et al,2000), recurrent neural networks(Elman,1990; Mikolov et al,2010)and LSTMs(Hochreiter and Schmidhuber,1997; Graves,2013)"
        ]
      }
    }
  ],
  [
    {
      "id": "2022-the_winograd_schema_challenge",
      "type": "core",
      "title": "The Winograd schema challenge",
      "year": 2022,
      "authors": "Vid Kocijan, Ernest Davis, Thomas Lukasiewicz, Gary Marcus, Leora Morgenstern",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ],
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2020-quora_question_pairs",
      "type": "core",
      "title": "Quora question pairs",
      "year": 2020,
      "authors": "Andreas Chandra, Ruben Stefanus",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2020-cloze_procedure",
      "type": "core",
      "title": "Cloze procedure",
      "year": 2020,
      "authors": "Dezhao Luo, Chang Liu, Yu Zhou, Dongbao Yang, Can Ma, Qixiang Ye, Weiping Wang",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          "BERT alleviates the previously mentioned unidirectionality constraint by using a “masked language model” (MLM) pre-training objective, inspired by the Cloze taskTaylor (1953)",
          "We refer to this procedure as a “masked LM” (MLM), although it is often referred to as aClozetask in the literatureTaylor (1953)"
        ]
      }
    },
    {
      "id": "2018-universal_language_model_fine-tuning_for_text_classification",
      "type": "core",
      "title": "Universal language model fine-tuning for text classification",
      "year": 2018,
      "authors": "Jeremy Howard, Sebastian Ruder",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ],
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2018-u-net:_machine_reading_comprehension_with_unanswerable_questions",
      "type": "core",
      "title": "U-net: Machine reading comprehension with unanswerable questions",
      "year": 2018,
      "authors": "Fu Sun, Linyang Li, Xipeng Qiu, Yang Liu",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2018-swag:_a_large-scale_adversarial_dataset_for_grounded_commonsense_inference",
      "type": "core",
      "title": "Swag: A large-scale adversarial dataset for grounded commonsense inference",
      "year": 2018,
      "authors": "Rowan Zellers, Yonatan Bisk, Roy Schwartz, Yejin Choi",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2018-semi-supervised_sequence_modeling_with_cross-view_training",
      "type": "core",
      "title": "Semi-supervised sequence modeling with cross-view training",
      "year": 2018,
      "authors": "Kevin Clark, Minh-Thang Luong, Christopher D. Manning, Quoc V. Le",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2018-neural_network_acceptability_judgments",
      "type": "core",
      "title": "Neural network acceptability judgments",
      "year": 2018,
      "authors": "Alex Warstadt, Amanpreet Singh, Samuel R. Bowman",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ],
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2018-multi-granularity_hierarchical_attention_fusion_networks_for_reading_comprehension_and_question_answering",
      "type": "core",
      "title": "Multi-granularity hierarchical attention fusion networks for reading comprehension and question answering",
      "year": 2018,
      "authors": "Wei Wang, Ming Yan, Chen Wu",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2018-glue:_a_multi-task_benchmark_and_analysis_platform_for_natural_language_understanding",
      "type": "core",
      "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "year": 2018,
      "authors": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R. Bowman",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ],
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2018-dissecting_contextual_word_embeddings:_architecture_and_representation",
      "type": "core",
      "title": "Dissecting contextual word embeddings: Architecture and representation",
      "year": 2018,
      "authors": "Matthew E. Peters, Mark Neumann, Luke Zettlemoyer, Wen-tau Yih",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2018-deep_contextualized_word_representations",
      "type": "core",
      "title": "Deep contextualized word representations",
      "year": 2018,
      "authors": "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ],
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2018-character-level_language_modeling_with_deeper_self-attention",
      "type": "core",
      "title": "Character-level language modeling with deeper self-attention",
      "year": 2018,
      "authors": "Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, Llion Jones",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2018-an_efficient_framework_for_learning_sentence_representations",
      "type": "core",
      "title": "An efficient framework for learning sentence representations",
      "year": 2018,
      "authors": "Lajanugen Logeswaran, Honglak Lee",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          "(2017)andLogeswaran and Lee (2018)",
          null
        ]
      }
    },
    {
      "id": "2017-triviaqa:_a_large_scale_distantly_supervised_challenge_dataset_for_reading_comprehension",
      "type": "core",
      "title": "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
      "year": 2017,
      "authors": "Mandar Joshi, Eunsol Choi, Daniel S. Weld, Luke Zettlemoyer",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ],
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "We compare LLaMA to existing large language models on two closed-book question answering benchmarks: Natural Questions(Kwiatkowski et al,2019)and TriviaQAJoshi et al (2017)"
        ],
        "2025-search-o1:_agentic_search-enhanced_large_reasoning_models": [
          "TriviaQA[28]is a large-scale dataset with questions from trivia websites and competitions, featuring complex entity relationships"
        ]
      }
    },
    {
      "id": "2017-supervised_learning_of_universal_sentence_representations_from_natural_language_inference_data",
      "type": "core",
      "title": "Supervised learning of universal sentence representations from natural language inference data",
      "year": 2017,
      "authors": "Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, Antoine Bordes",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2017-simple_and_effective_multi-paragraph_reading_comprehension",
      "type": "core",
      "title": "Simple and effective multi-paragraph reading comprehension",
      "year": 2017,
      "authors": "Christopher Clark, Matt Gardner",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2017-semi-supervised_sequence_tagging_with_bidirectional_language_models",
      "type": "core",
      "title": "Semi-supervised sequence tagging with bidirectional language models",
      "year": 2017,
      "authors": "Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula, Russell Power",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2017-reinforced_mnemonic_reader_for_machine_reading_comprehension",
      "type": "core",
      "title": "Reinforced mnemonic reader for machine reading comprehension",
      "year": 2017,
      "authors": "Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, Ming Zhou",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2017-learned_in_translation:_contextualized_word_vectors",
      "type": "core",
      "title": "Learned in translation: Contextualized word vectors",
      "year": 2017,
      "authors": "Bryan McCann, James Bradbury, Caiming Xiong, Richard Socher",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ],
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2017-discourse-based_objectives_for_fast_unsupervised_sentence_representation_learning",
      "type": "core",
      "title": "Discourse-based objectives for fast unsupervised sentence representation learning",
      "year": 2017,
      "authors": "Yacine Jernite, Samuel R. Bowman, David Sontag",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ],
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2017-attention_is_all_you_need",
      "type": "core",
      "title": "Attention Is All You Need",
      "year": "2017",
      "authors": null,
      "name": "Transformer",
      "parents": {
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Later on, attention models are getting more and more attention in both NLP,[36,4], and recommender systems,[23,16]",
          "Recent research finds that residual connections can help training very deep neural networks even if they are not convolutional neural networks[36]",
          "Our model is motivated by the Transformer model in[36]and[16]",
          "Recent advances in deep learning, especially the discovery of various attention mechanisms[2,33]and newer architectures[36,4]in addition to classical RNN and CNN architecture in natural language processing, have allowed us to make better use of the temporal ordering of items that each user has engaged with",
          "SASRec: a self-attentive sequential recommendation method[16]motivated by Transformer in NLP[36]"
        ],
        "2018-self-attentive_sequential_recommendation": [
          "Recently, a new sequential modelTransfomerachieved state-of-the-art performance and efficiency for machine translation tasks[3]",
          "RNNs are generally suitable for modeling sequences, though recent studies show CNNs and self-attention can be stronger in some sequential settings[3,44]",
          "The scaled dot-product attention[3]is defined as:",
          "Recently, a purely attention-based sequence-to-sequence method, Transfomer[3], achieved state-of-the-art performance and efficiency on machine translation tasks which had previously been dominated by RNN/CNN-based approaches[32,33]",
          "Recently, a self-attention method was proposed which uses the same objects as queries, keys, and values[3]",
          "We empirically find our method is over ten times faster than RNN and CNN-based methods with GPUs (the result is similar to that in[3]for machine translation tasks), and the maximum lengthn𝑛ncan easily scale to a few hundred which is generally sufficient for existing benchmark datasets",
          "(8)Multi-head: The authors of Transformer[3]found that it is useful to use ‘multi-head’ attention, which applies attention inhℎhsubspaces (each ad/h𝑑ℎd/h-dimensional space)",
          "We also tried the fixed position embedding as used in[3], but found that this led to worse performance in our case",
          "); and 3) models with more parameters often require more training time"
        ],
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ],
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ],
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "Following recent work on large language models, our network is based on the transformer architectureVaswani et al (2017)",
          "In the rest of this paper, we present an overview of the modifications we made to the transformer architectureVaswani et al (2017), as well as our training method",
          "More recently, transformer networks, based on self-attention, have led to important improvements, especially for capturing long range dependencies(Vaswani et al,2017; Radford et al,2018; Dai et al,2019)"
        ],
        "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model": [
          "With the emergence of Transformers(Vaswani,2017)and the scaling laws(Henighan et al,2020), the researchers try to scale up the parameters of the generative language model"
        ],
        "2025-a_tutorial_on_llm_reasoning:_relevant_methods_behind_chatgpt_o1": [
          ", word) in the sequence given the previous tokens[29]",
          "Typically, this is achieved using neural networks like transformers[29], which are trained to minimise the negative log-likelihood of the training data"
        ]
      }
    },
    {
      "id": "2017-a_broad-coverage_challenge_corpus_for_sentence_understanding_through_inference",
      "type": "core",
      "title": "A broad-coverage challenge corpus for sentence understanding through inference",
      "year": 2017,
      "authors": "Adina Williams, Nikita Nangia, Samuel R. Bowman",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ],
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2016-squad:_100000+_questions_for_machine_comprehension_of_text",
      "type": "core",
      "title": "SQuAD: 100,000+ questions for machine comprehension of text",
      "year": 2016,
      "authors": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ],
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2016-learning_distributed_representations_of_sentences_from_unlabelled_data",
      "type": "core",
      "title": "Learning distributed representations of sentences from unlabelled data",
      "year": 2016,
      "authors": "Felix Hill, Kyunghyun Cho, Anna Korhonen",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ],
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2016-bidirectional_attention_flow_for_machine_comprehension",
      "type": "core",
      "title": "Bidirectional attention flow for machine comprehension",
      "year": 2016,
      "authors": "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2016-a_decomposable_attention_model_for_natural_language_inference",
      "type": "core",
      "title": "A decomposable attention model for natural language inference",
      "year": 2016,
      "authors": "Ankur P. Parikh, Oscar Täckström, Dipanjan Das, Jakob Uszkoreit",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2015-skip-thought_vectors",
      "type": "core",
      "title": "Skip-thought vectors",
      "year": 2015,
      "authors": "Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, Sanja Fidler",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ],
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2015-semi-supervised_sequence_learning",
      "type": "core",
      "title": "Semi-supervised sequence learning",
      "year": 2015,
      "authors": "Andrew M. Dai, Quoc V. Le",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ],
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2015-aligning_books_and_movies:_towards_story-like_visual_explanations_by_watching_movies_and_reading_books",
      "type": "core",
      "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "year": 2015,
      "authors": "Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ],
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2015-a_large_annotated_corpus_for_learning_natural_language_inference",
      "type": "core",
      "title": "A large annotated corpus for learning natural language inference",
      "year": 2015,
      "authors": "Samuel R. Bowman, Gabor Angeli, Christopher Potts, Christopher D. Manning",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2014-how_transferable_are_features_in_deep_neural_networks?",
      "type": "core",
      "title": "How transferable are features in deep neural networks?",
      "year": 2014,
      "authors": "Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2014-distributed_representations_of_sentences_and_documents",
      "type": "core",
      "title": "Distributed representations of sentences and documents",
      "year": 2014,
      "authors": "Quoc V. Le, Tomas Mikolov",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ],
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          "(2015); Logeswaran and Lee (2018)or paragraph embeddingsLe and Mikolov (2014)"
        ]
      }
    },
    {
      "id": "2013-one_billion_word_benchmark_for_measuring_progress_in_statistical_language_modeling",
      "type": "core",
      "title": "One billion word benchmark for measuring progress in statistical language modeling",
      "year": 2013,
      "authors": "Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, Tony Robinson",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ],
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "While this work relied on a simple smoothing technique, calledStupid Backoff,Heafield et al (2013)later showed how to scale Kneser-Ney smoothing to Web-scale data"
        ]
      }
    },
    {
      "id": "2013-distributed_representations_of_words_and_phrases_and_their_compositionality",
      "type": "core",
      "title": "Distributed representations of words and phrases and their compositionality",
      "year": 2013,
      "authors": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean",
      "name": null,
      "parents": {
        "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations": [
          null
        ],
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2003-introduction_to_the_conll-2003_shared_task:_language-independent_named_entity_recognition",
      "type": "core",
      "title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition",
      "year": 2003,
      "authors": "Erik F. Tjong Kim Sang, Fien De Meulder",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          "In this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) taskTjong Kim Sang and De Meulder (2003)",
          "(2013), and named entity recognitionTjong Kim Sang and De Meulder (2003)",
          null
        ]
      }
    },
    {
      "id": "1817-a_framework_for_learning_predictive_structures_from_multiple_tasks_and_unlabeled_data",
      "type": "core",
      "title": "A framework for learning predictive structures from multiple tasks and unlabeled data",
      "year": "1817",
      "authors": "Rie Kubota Ando and Tong Zhang. 2005",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "1801-maskgan:_better_text_generation_via_filling_in_the_",
      "type": "core",
      "title": "Maskgan: Better text generation via filling in the_",
      "year": "1801",
      "authors": "William Fedus, Ian Goodfellow, and Andrew M Dai. 2018",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "1638-contextual_string_embeddings_for_sequence_labeling",
      "type": "core",
      "title": "Contextual string embeddings for sequence labeling",
      "year": "1638",
      "authors": "Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "1631-recursive_deep_models_for_semantic_compositionality_over_a_sentiment_treebank",
      "type": "core",
      "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
      "year": "1631",
      "authors": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "1606-bridging_nonlinearities_and_stochastic_regularizers_with_gaussian_error_linear_units",
      "type": "core",
      "title": "Bridging nonlinearities and stochastic regularizers with gaussian error linear units",
      "year": "1606",
      "authors": "Dan Hendrycks and Kevin Gimpel. 2016",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          "We use ageluactivationHendrycks and Gimpel (2016)rather than the standardrelu, following OpenAI GPT"
        ]
      }
    },
    {
      "id": "1532-glove:_global_vectors_for_word_representation",
      "type": "core",
      "title": "Glove: Global vectors for word representation",
      "year": "1532",
      "authors": "Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "1096-extracting_and_composing_robust_features_with_denoising_autoencoders",
      "type": "core",
      "title": "Extracting and composing robust features with denoising autoencoders",
      "year": "1096",
      "authors": "Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008",
      "name": null,
      "parents": {
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    }
  ],
  [
    {
      "id": "2017-structured_attention_networks",
      "type": "core",
      "title": "Structured attention networks",
      "year": "2017",
      "authors": "Yoon Kim, Carl Denton, Luong Hoang, Alexander M. Rush",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences[2,19]"
        ]
      }
    },
    {
      "id": "2017-outrageously_large_neural_networks:_the_sparsely-gated_mixture-of-experts_layer",
      "type": "core",
      "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
      "year": "2017",
      "authors": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "Recent work has achieved significant improvements in computational efficiency through factorization tricks[21]and conditional computation[32], while also improving model performance in case of the latter",
          "MoE[32]"
        ]
      }
    },
    {
      "id": "2017-neural_machine_translation_in_linear_time",
      "type": "core",
      "title": "Neural machine translation in linear time",
      "year": "2017",
      "authors": "Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, Koray Kavukcuoglu",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet[18]and ConvS2S[9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions",
          "In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as[17,18]and[9]",
          "Doing so requires a stack ofO​(n/k)O(n/k)convolutional layers in the case of contiguous kernels, orO​(l​o​gk​(n))O(log_{k}(n))in the case of dilated convolutions[18], increasing the length of the longest paths between any two positions in the network",
          "ByteNet[18]"
        ]
      }
    },
    {
      "id": "2017-massive_exploration_of_neural_machine_translation_architectures",
      "type": "core",
      "title": "Massive exploration of neural machine translation architectures",
      "year": "2017",
      "authors": "Denny Britz, Anna Goldie, Minh-Thang Luong, Quoc Le",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "While for small values ofdkd_{k}the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values ofdkd_{k}[3]",
          "Sentences were encoded using byte-pair encoding[3], which has a shared source-target vocabulary of about 37000 tokens"
        ]
      }
    },
    {
      "id": "2017-factorization_tricks_for_lstm_networks",
      "type": "core",
      "title": "Factorization tricks for LSTM networks",
      "year": "2017",
      "authors": "Oleksii Kuchaiev, Boris Ginsburg",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "Recent work has achieved significant improvements in computational efficiency through factorization tricks[21]and conditional computation[32], while also improving model performance in case of the latter"
        ]
      }
    },
    {
      "id": "2017-convolutional_sequence_to_sequence_learning",
      "type": "core",
      "title": "Convolutional sequence to sequence learning",
      "year": "2017",
      "authors": "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet[18]and ConvS2S[9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions",
          "There are many choices of positional encodings, learned and fixed[9]",
          "This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as[38,2,9]",
          "ConvS2S Ensemble[9]",
          "We also experimented with using learned positional embeddings[9]instead, and found that the two versions produced nearly identical results (see Table3row (E))",
          "In row (E) we replace our sinusoidal positional encoding with learned positional embeddings[9], and observe nearly identical results to the base model",
          "In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as[17,18]and[9]",
          "ConvS2S[9]"
        ]
      }
    },
    {
      "id": "2017-a_structured_self-attentive_sentence_embedding",
      "type": "core",
      "title": "A structured self-attentive sentence embedding",
      "year": "2017",
      "authors": "Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, Yoshua Bengio",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations[4,27,28,22]"
        ]
      }
    },
    {
      "id": "2017-a_deep_reinforced_model_for_abstractive_summarization",
      "type": "core",
      "title": "A deep reinforced model for abstractive summarization",
      "year": "2017",
      "authors": "Romain Paulus, Caiming Xiong, Richard Socher",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations[4,27,28,22]"
        ]
      }
    },
    {
      "id": "2016-xception:_deep_learning_with_depthwise_separable_convolutions",
      "type": "core",
      "title": "Xception: Deep learning with depthwise separable convolutions",
      "year": "2016",
      "authors": "François Chollet",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "Separable convolutions[6], however, decrease the complexity considerably, toO​(k⋅n⋅d+n⋅d2)O(k\\cdot n\\cdot d+n\\cdot d^{2})"
        ]
      }
    },
    {
      "id": "2016-using_the_output_embedding_to_improve_language_models",
      "type": "core",
      "title": "Using the output embedding to improve language models",
      "year": "2016",
      "authors": "Ofir Press, Lior Wolf",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to[30]"
        ]
      }
    },
    {
      "id": "2016-recurrent_neural_network_grammars",
      "type": "core",
      "title": "Recurrent neural network grammars",
      "year": "2016",
      "authors": "Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, Noah A. Smith",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "(2016)[8]",
          "Our results in Table4show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar[8]"
        ]
      }
    },
    {
      "id": "2016-neural_gpus_learn_algorithms",
      "type": "core",
      "title": "Neural GPUs learn algorithms",
      "year": "2016",
      "authors": "Łukasz Kaiser, Ilya Sutskever",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as[17,18]and[9]"
        ]
      }
    },
    {
      "id": "2016-long_short-term_memory-networks_for_machine_reading",
      "type": "core",
      "title": "Long short-term memory-networks for machine reading",
      "year": "2016",
      "authors": "Jianpeng Cheng, Li Dong, Mirella Lapata",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations[4,27,28,22]"
        ]
      }
    },
    {
      "id": "2016-layer_normalization",
      "type": "core",
      "title": "Layer normalization",
      "year": "2016",
      "authors": "Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "We employ a residual connection[11]around each of the two sub-layers, followed by layer normalization[1]"
        ],
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Layer normalization[1]normalizes neurons within a layer"
        ],
        "2018-self-attentive_sequential_recommendation": [
          ", zero-mean and unit-variance), which is beneficial for stabilizing and accelerating neural network training[36]"
        ]
      }
    },
    {
      "id": "2016-google’s_neural_machine_translation_system:_bridging_the_gap_between_human_and_machine_translation",
      "type": "core",
      "title": "Google’s neural machine translation system: Bridging the gap between human and machine translation",
      "year": "2016",
      "authors": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "6[38]",
          "This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as[38,2,9]",
          "In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence lengthnnis smaller than the representation dimensionalitydd, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece[38]and byte-pair[31]representations",
          "Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures[38,24,15]",
          "GNMT + RL[38]",
          "GNMT + RL Ensemble[38]",
          "For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary[38]"
        ],
        "2018-self-attentive_sequential_recommendation": [
          "Recently, a purely attention-based sequence-to-sequence method, Transfomer[3], achieved state-of-the-art performance and efficiency on machine translation tasks which had previously been dominated by RNN/CNN-based approaches[32,33]"
        ],
        "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding": [
          null
        ]
      }
    },
    {
      "id": "2016-exploring_the_limits_of_language_modeling",
      "type": "core",
      "title": "Exploring the limits of language modeling",
      "year": "2016",
      "authors": "Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, Yonghui Wu",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures[38,24,15]"
        ],
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "In the context of neural language models,Jozefowicz et al (2016)obtained state-of-the-art results on the Billion Word benchmark by scaling LSTMs to 1 billion parameters"
        ]
      }
    },
    {
      "id": "2016-deep_residual_learning_for_image_recognition",
      "type": "core",
      "title": "Deep Residual Learning for Image Recognition",
      "year": "2016",
      "authors": null,
      "name": "Residual",
      "parents": {
        "2016-wide_&_deep_learning_for_recommender_systems": [
          "In computer vision, deep residual learning[2]has been used to reduce the difficulty of training deeper models and improve accuracy with shortcut connections which skip one or more layers"
        ],
        "2017-attention_is_all_you_need": [
          "We employ a residual connection[11]around each of the two sub-layers, followed by layer normalization[1]"
        ],
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Residual connections are firstly proposed in ResNet for image classification problems[7]"
        ],
        "2018-self-attentive_sequential_recommendation": [
          "However, simply adding more layers did not easily correspond to better performance until residual networks were proposed[35]"
        ]
      }
    },
    {
      "id": "2016-deep_recurrent_models_with_fast-forward_connections_for_neural_machine_translation",
      "type": "core",
      "title": "Deep recurrent models with fast-forward connections for neural machine translation",
      "year": "2016",
      "authors": "Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, Wei Xu",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "Deep-Att + PosUnk[39]",
          "Deep-Att + PosUnk Ensemble[39]"
        ],
        "2018-self-attentive_sequential_recommendation": [
          "Recently, a purely attention-based sequence-to-sequence method, Transfomer[3], achieved state-of-the-art performance and efficiency on machine translation tasks which had previously been dominated by RNN/CNN-based approaches[32,33]"
        ]
      }
    },
    {
      "id": "2016-can_active_memory_replace_attention?",
      "type": "core",
      "title": "Can active memory replace attention?",
      "year": "2016",
      "authors": "Łukasz Kaiser, Samy Bengio",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet[18]and ConvS2S[9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions"
        ]
      }
    },
    {
      "id": "2016-a_decomposable_attention_model",
      "type": "core",
      "title": "A decomposable attention model",
      "year": "2016",
      "authors": "Ankur P. Parikh, Oscar Täckström, Dipanjan Das, Jakob Uszkoreit",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "In all but a few cases[27], however, such attention mechanisms are used in conjunction with a recurrent network",
          "Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations[4,27,28,22]"
        ]
      }
    },
    {
      "id": "2015-rethinking_the_inception_architecture_for_computer_vision",
      "type": "core",
      "title": "Rethinking the inception architecture for computer vision",
      "year": "2015",
      "authors": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "1[36]"
        ]
      }
    },
    {
      "id": "2015-neural_machine_translation_of_rare_words_with_subword_units",
      "type": "core",
      "title": "Neural machine translation of rare words with subword units",
      "year": "2015",
      "authors": "Rico Sennrich, Barry Haddow, Alexandra Birch",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence lengthnnis smaller than the representation dimensionalitydd, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece[38]and byte-pair[31]representations"
        ],
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "We tokenize the data with the byte-pair encoding (BPE) algorithm(Sennrich et al,2015), using the implementation from SentencePiece(Kudo and Richardson,2018)"
        ]
      }
    },
    {
      "id": "2015-multi-task_sequence_to_sequence_learning",
      "type": "core",
      "title": "Multi-task sequence to sequence learning",
      "year": "2015",
      "authors": "Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, Lukasz Kaiser",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "(2015)[23]"
        ]
      }
    },
    {
      "id": "2015-grammar_as_a_foreign_language",
      "type": "core",
      "title": "Grammar as a foreign language",
      "year": "2015",
      "authors": "Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, Geoffrey Hinton",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences[37]",
          "In contrast to RNN sequence-to-sequence models[37], the Transformer outperforms the BerkeleyParser[29]even when training only on the WSJ training set of 40K sentences",
          "(2014)[37]",
          "Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes[37]"
        ]
      }
    },
    {
      "id": "2015-end-to-end_memory_networks",
      "type": "core",
      "title": "End-to-end memory networks",
      "year": "2015",
      "authors": "Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks[34]"
        ]
      }
    },
    {
      "id": "2015-effective_approaches_to_attention-based_neural_machine_translation",
      "type": "core",
      "title": "Effective approaches to attention-based neural machine translation",
      "year": "2015",
      "authors": "Minh-Thang Luong, Hieu Pham, Christopher D. Manning",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures[38,24,15]"
        ]
      }
    },
    {
      "id": "2015-adam:_a_method_for_stochastic_optimization",
      "type": "core",
      "title": "Adam: A method for stochastic optimization",
      "year": "2015",
      "authors": "Diederik P. Kingma, Jimmy Ba",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "We used the Adam optimizer[20]withβ1=0"
        ],
        "2018-self-attentive_sequential_recommendation": [
          "For fair comparison, we implement BPR, FMC, FPMC, and TransRec usingTemsorFlowwith the Adam[41]optimizer",
          "The network is optimized by theAdamoptimizer[41], which is a variant of Stochastic Gradient Descent (SGD) with adaptive moment estimation",
          "The optimizer is theAdamoptimizer[41], the learning rate is set to0"
        ]
      }
    },
    {
      "id": "2014-sequence_to_sequence_learning_with_neural_networks",
      "type": "core",
      "title": "Sequence to sequence learning with neural networks",
      "year": "2014",
      "authors": "Ilya Sutskever, Oriol Vinyals, Quoc V. Le",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "Recurrent neural networks, long short-term memory[13]and gated recurrent[7]neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation[35,2,5]",
          "Most competitive neural sequence transduction models have an encoder-decoder structure[5,2,35]"
        ],
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Recent advances in deep learning, especially the discovery of various attention mechanisms[2,33]and newer architectures[36,4]in addition to classical RNN and CNN architecture in natural language processing, have allowed us to make better use of the temporal ordering of items that each user has engaged with"
        ]
      }
    },
    {
      "id": "2014-neural_machine_translation_by_jointly_learning_to_align_and_translate",
      "type": "core",
      "title": "Neural machine translation by jointly learning to align and translate",
      "year": "2014",
      "authors": "Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as[38,2,9]",
          "Recurrent neural networks, long short-term memory[13]and gated recurrent[7]neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation[35,2,5]",
          "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences[2,19]",
          "The two most commonly used attention functions are additive attention[2], and dot-product (multiplicative) attention",
          "Most competitive neural sequence transduction models have an encoder-decoder structure[5,2,35]"
        ],
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "Recent advances in deep learning, especially the discovery of various attention mechanisms[2,33]and newer architectures[36,4]in addition to classical RNN and CNN architecture in natural language processing, have allowed us to make better use of the temporal ordering of items that each user has engaged with"
        ]
      }
    },
    {
      "id": "2014-learning_phrase_representations_using_rnn_encoder-decoder_for_statistical_machine_translation",
      "type": "core",
      "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "year": "2014",
      "authors": "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "Recurrent neural networks, long short-term memory[13]and gated recurrent[7]neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation[35,2,5]",
          "Most competitive neural sequence transduction models have an encoder-decoder structure[5,2,35]"
        ]
      }
    },
    {
      "id": "2014-empirical_evaluation_of_gated_recurrent_neural_networks_on_sequence_modeling",
      "type": "core",
      "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "year": "2014",
      "authors": "Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "Recurrent neural networks, long short-term memory[13]and gated recurrent[7]neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation[35,2,5]"
        ],
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "It utilizes the GRU structures[3]initially proposed for speech modelling"
        ]
      }
    },
    {
      "id": "2014-dropout:_a_simple_way_to_prevent_neural_networks_from_overfitting",
      "type": "core",
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "year": "2014",
      "authors": "Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "We apply dropout[33]to the output of each sub-layer, before it is added to the sub-layer input and normalized"
        ],
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "For deep neural networks, it has been shown thatℓpsubscriptℓ𝑝\\ell_{p}regularizations are often too weak, while dropout[12,32]is more effective in practice",
          "Dropout[32]is applied to the embedding layerE𝐸E, self-attention layer and pointwise feed-forward layer by stochastically dropping some percentage of hidden units to prevent co-adaption of neurons"
        ],
        "2018-self-attentive_sequential_recommendation": [
          "To alleviate overfitting problems in deep neural networks, ‘Dropout’ regularization techniques have been shown to be effective in various neural network architectures[38]"
        ]
      }
    },
    {
      "id": "2013-generating_sequences_with_recurrent_neural_networks",
      "type": "core",
      "title": "Generating sequences with recurrent neural networks",
      "year": "2013",
      "authors": "Alex Graves",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next"
        ],
        "2023-llama:_open_and_efficient_foundation_language_models": [
          "In the past two decades, neural networks have been successfully applied to the language modelling task, starting from feed forward models(Bengio et al,2000), recurrent neural networks(Elman,1990; Mikolov et al,2010)and LSTMs(Hochreiter and Schmidhuber,1997; Graves,2013)"
        ]
      }
    },
    {
      "id": "2013-fast_and_accurate_shift-reduce_constituent_parsing",
      "type": "core",
      "title": "Fast and accurate shift-reduce constituent parsing",
      "year": "2013",
      "authors": "Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "(2013)[40]"
        ]
      }
    },
    {
      "id": "2009-self-training_pcfg_grammars_with_latent_annotations_across_languages",
      "type": "core",
      "title": "Self-training PCFG grammars with latent annotations across languages",
      "year": "2009",
      "authors": "Zhongqiang Huang and Mary Harper",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "Huang & Harper (2009)[14]"
        ]
      }
    },
    {
      "id": "2006-learning_accurate_compact_and_interpretable_tree_annotation",
      "type": "core",
      "title": "Learning accurate, compact, and interpretable tree annotation",
      "year": "2006",
      "authors": "Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "(2006)[29]",
          "In contrast to RNN sequence-to-sequence models[37], the Transformer outperforms the BerkeleyParser[29]even when training only on the WSJ training set of 40K sentences"
        ]
      }
    },
    {
      "id": "2006-effective_self-training_for_parsing",
      "type": "core",
      "title": "Effective self-training for parsing",
      "year": "2006",
      "authors": "David McClosky, Eugene Charniak, and Mark Johnson",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "(2006)[26]"
        ]
      }
    },
    {
      "id": "2001-gradient_flow_in_recurrent_nets:_the_difficulty_of_learning_long-term_dependencies_2001",
      "type": "core",
      "title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001",
      "year": "2001",
      "authors": "Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "This makes it more difficult to learn dependencies between distant positions[12]",
          "The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies[12]"
        ]
      }
    },
    {
      "id": "1993-building_a_large_annotated_corpus_of_english:_the_penn_treebank",
      "type": "core",
      "title": "Building a large annotated corpus of english: The penn treebank",
      "year": "1993",
      "authors": "Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini",
      "name": null,
      "parents": {
        "2017-attention_is_all_you_need": [
          "We trained a 4-layer transformer withdm​o​d​e​l=1024d_{model}=1024on the Wall Street Journal (WSJ) portion of the Penn Treebank[25], about 40K training sentences"
        ]
      }
    }
  ],
  [
    {
      "id": "2015-very_deep_convolutional_networks_for_large-scale_image_recognition",
      "type": "core",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "year": "2015",
      "authors": "Karen Simonyan, Andrew Zisserman",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "VGG-16[41]",
          "Recent evidence[41,44]reveals that network depth is of crucial importance, and the leading results[41,44,13,16]on the challenging ImageNet dataset[36]all exploit “very deep”[41]models, with a depth of sixteen[41]to thirty[16]",
          "Following[40,41], we assume that the image-level classifiers are first adopted for predicting the class labels of an image, and the localization algorithm only accounts for predicting bounding boxes based on the predicted classes",
          "VGG[41](v5)",
          "VGG’s[41]",
          "It is worth noticing that our model hasfewerfilters andlowercomplexity than VGG nets[41](Fig",
          "1[41]",
          "For best results, we adopt the fully-convolutional form as in[41,13], and average the scores at multiple scales (images are resized such that the shorter side is in{224,256,384,480,640}224256384480640\\{224,256,384,480,640\\})",
          "Here we are interested in the improvements of replacing VGG-16[41]with ResNet-101",
          "Our 152-layer residual net is the deepest network ever presented on ImageNet, while still having lower complexity than VGG nets[41]",
          "On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8×\\timesdeeper than VGG nets[41]but still having lower complexity",
          "Our implementation for ImageNet follows the practice in[21,41]",
          "3, middle) are mainly inspired by the philosophy of VGG nets[41](Fig",
          "Following[41], we first perform “oracle” testing using the ground truth class as the classification prediction",
          "VGG[41](ILSVRC’14)"
        ]
      }
    },
    {
      "id": "2015-training_very_deep_networks",
      "type": "core",
      "title": "Training very deep networks",
      "year": "2015",
      "authors": "Rupesh Kumar Srivastava, Klaus Greff, Jürgen Schmidhuber",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Highway[42,43]",
          "Concurrent with our work, “highway networks”[42,43]present shortcut connections with gating functions[15]"
        ]
      }
    },
    {
      "id": "2015-object_detection_via_a_multi-region_&_semantic_segmentation-aware_cnn_model",
      "type": "core",
      "title": "Object detection via a multi-region & semantic segmentation-aware cnn model",
      "year": "2015",
      "authors": "Spyros Gidaris, Nikos Komodakis",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "6% mAP on PASCAL VOC 2007 (Table11) and 83",
          "Our box refinement partially follows the iterative localization in[6]"
        ]
      }
    },
    {
      "id": "2015-object_detection_networks_on_convolutional_feature_maps",
      "type": "core",
      "title": "Object detection networks on convolutional feature maps",
      "year": "2015",
      "authors": "Shaoqing Ren, Kaiming He, Ross Girshick, Xiangyu Zhang, Jian Sun",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Multi-scale training/testing has been developed in[12,7]by selecting a scale from a feature pyramid, and in[33]by using maxout layers",
          "We adopt the idea of “Networks on Conv feature maps” (NoC)[33]to address this issue"
        ]
      }
    },
    {
      "id": "2015-highway_networks",
      "type": "core",
      "title": "Highway networks",
      "year": "2015",
      "authors": "Rupesh Kumar Srivastava, Klaus Greff, Jürgen Schmidhuber",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Highway[42,43]",
          "Concurrent with our work, “highway networks”[42,43]present shortcut connections with gating functions[15]",
          "Unexpectedly, such degradation isnot caused by overfitting, and adding more layers to a suitably deep model leads tohigher training error, as reported in[11,42]and thoroughly verified by our experiments",
          "4, left) and on MNIST (see[42]), suggesting that such an optimization difficulty is a fundamental problem",
          "It hasfewerparameters than other deep and thin networks such as FitNet[35]and Highway[42](Table6), yet is among the state-of-the-art results (6"
        ]
      }
    },
    {
      "id": "2015-going_deeper_with_convolutions",
      "type": "core",
      "title": "Going deeper with convolutions",
      "year": "2015",
      "authors": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Recent evidence[41,44]reveals that network depth is of crucial importance, and the leading results[41,44,13,16]on the challenging ImageNet dataset[36]all exploit “very deep”[41]models, with a depth of sixteen[41]to thirty[16]",
          "In[44,24], a few intermediate layers are directly connected to auxiliary classifiers for addressing vanishing/exploding gradients",
          "GoogLeNet[44]",
          "GoogLeNet[44](ILSVRC’14)"
        ]
      }
    },
    {
      "id": "2015-fully_convolutional_networks_for_semantic_segmentation",
      "type": "core",
      "title": "Fully convolutional networks for semantic segmentation",
      "year": "2015",
      "authors": "Evan Shelhamer, Jonathan Long, Trevor Darrell",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Many other nontrivial visual recognition tasks[8,12,7,32,27]have also greatly benefited from very deep models"
        ]
      }
    },
    {
      "id": "2015-fitnets:_hints_for_thin_deep_nets",
      "type": "core",
      "title": "Fitnets: Hints for thin deep nets",
      "year": "2015",
      "authors": "Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, Yoshua Bengio",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Strong regularization such as maxout[10]or dropout[14]is applied to obtain the best results ([10,25,24,35]) on this dataset",
          "FitNet[35]",
          "It hasfewerparameters than other deep and thin networks such as FitNet[35]and Highway[42](Table6), yet is among the state-of-the-art results (6"
        ]
      }
    },
    {
      "id": "2015-faster_r-cnn:_towards_real-time_object_detection_with_region_proposal_networks",
      "type": "core",
      "title": "Faster R-CNN: Towards real-time object detection with region proposal networks",
      "year": "2015",
      "authors": "Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Many other nontrivial visual recognition tasks[8,12,7,32,27]have also greatly benefited from very deep models",
          "In the above, all results are obtained by single-scale training/testing as in[32], where the image’s shorter side iss=600𝑠600s=600pixels",
          "Following[7,32], for the PASCAL VOC 2007testset, we use the 5ktrainvalimages in VOC 2007 and 16ktrainvalimages in VOC 2012 for training (“07+12”)",
          "The above results are only based on theproposal network(RPN) in Faster R-CNN[32]",
          "We adoptFaster R-CNN[32]as the detection method",
          "Our localization algorithm is based on the RPN framework of[32]with a few modifications",
          "To avoid negative samples being dominate, 8 anchors are randomly sampled for each image, where the sampled positive and negative anchors have a ratio of 1:1[32]",
          "Unlike VGG-16 used in[32], our ResNet has no hidden fc layers",
          "In this section we introduce our detection method based on the baseline Faster R-CNN[32]system"
        ]
      }
    },
    {
      "id": "2015-fast_r-cnn",
      "type": "core",
      "title": "Fast R-CNN",
      "year": "2015",
      "authors": "Ross Girshick",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Many other nontrivial visual recognition tasks[8,12,7,32,27]have also greatly benefited from very deep models",
          "These layers are shared by a region proposal network (RPN, generating 300 proposals)[32]and a Fast R-CNN detection network[7]",
          "Following[7,32], for the PASCAL VOC 2007testset, we use the 5ktrainvalimages in VOC 2007 and 16ktrainvalimages in VOC 2012 for training (“07+12”)",
          "Multi-scale training/testing has been developed in[12,7]by selecting a scale from a feature pyramid, and in[33]by using maxout layers",
          "One may use thedetection network(Fast R-CNN[7]) in Faster R-CNN to improve the results"
        ]
      }
    },
    {
      "id": "2015-delving_deep_into_rectifiers:_surpassing_human-level_performance_on_imagenet_classification",
      "type": "core",
      "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "year": "2015",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Recent evidence[41,44]reveals that network depth is of crucial importance, and the leading results[41,44,13,16]on the challenging ImageNet dataset[36]all exploit “very deep”[41]models, with a depth of sixteen[41]to thirty[16]",
          "For best results, we adopt the fully-convolutional form as in[41,13], and average the scores at multiple scales (images are resized such that the shorter side is in{224,256,384,480,640}224256384480640\\{224,256,384,480,640\\})",
          "PReLU-net[13]",
          "This problem, however, has been largely addressed by normalized initialization[23,9,37,13]and intermediate normalization layers[16], which enable networks with tens of layers to start converging for stochastic gradient descent (SGD) with backpropagation[22]",
          "9, and adopt the weight initialization in[13]and BN[16]but with no dropout",
          "We initialize the weights as in[13]and train all plain/residual nets from scratch"
        ]
      }
    },
    {
      "id": "2015-convolutional_neural_networks_at_constrained_time_cost",
      "type": "core",
      "title": "Convolutional neural networks at constrained time cost",
      "year": "2015",
      "authors": "Kaiming He, Jian Sun",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Unexpectedly, such degradation isnot caused by overfitting, and adding more layers to a suitably deep model leads tohigher training error, as reported in[11,42]and thoroughly verified by our experiments"
        ]
      }
    },
    {
      "id": "2015-batch_normalization:_accelerating_deep_network_training_by_reducing_internal_covariate_shift",
      "type": "core",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "year": "2015",
      "authors": "Sergey Ioffe, Christian Szegedy",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Recent evidence[41,44]reveals that network depth is of crucial importance, and the leading results[41,44,13,16]on the challenging ImageNet dataset[36]all exploit “very deep”[41]models, with a depth of sixteen[41]to thirty[16]",
          "We adopt batch normalization (BN)[16]right after each convolution and before activation, following[16]",
          "9, and adopt the weight initialization in[13]and BN[16]but with no dropout",
          "This problem, however, has been largely addressed by normalized initialization[23,9,37,13]and intermediate normalization layers[16], which enable networks with tens of layers to start converging for stochastic gradient descent (SGD) with backpropagation[22]",
          "These plain networks are trained with BN[16], which ensures forward propagated signals to have non-zero variances",
          "BN-inception[16]"
        ],
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "One alternative is the batch normalization[15]but we find it does not work as well as the layer normalization in practice even for a reasonable large batch size of 128"
        ],
        "2018-self-attentive_sequential_recommendation": [
          "Unlike batch normalization[37], the statistics used in layer normalization are independent of other samples in the same batch"
        ]
      }
    },
    {
      "id": "2014-visualizing_and_understanding_convolutional_neural_networks",
      "type": "core",
      "title": "Visualizing and understanding convolutional neural networks",
      "year": "2014",
      "authors": "M. D. Zeiler and R. Fergus",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Deep convolutional neural networks[22,21]have led to a series of breakthroughs for image classification[21,50,40]"
        ]
      }
    },
    {
      "id": "2014-spatial_pyramid_pooling_in_deep_convolutional_networks_for_visual_recognition",
      "type": "core",
      "title": "Spatial pyramid pooling in deep convolutional networks for visual recognition",
      "year": "2014",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Many other nontrivial visual recognition tasks[8,12,7,32,27]have also greatly benefited from very deep models",
          "Multi-scale training/testing has been developed in[12,7]by selecting a scale from a feature pyramid, and in[33]by using maxout layers",
          "Given the full-image conv feature map, we pool a feature by global Spatial Pyramid Pooling[12](with a “single-level” pyramid) which can be implemented as “RoI” pooling using the entire image’s bounding box as the RoI"
        ]
      }
    },
    {
      "id": "2014-rich_feature_hierarchies_for_accurate_object_detection_and_semantic_segmentation",
      "type": "core",
      "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "year": "2014",
      "authors": "Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "The image region is cropped from a proposal, warped to 224×\\times224 pixels, and fed into the classification network as in R-CNN[8]",
          "Many other nontrivial visual recognition tasks[8,12,7,32,27]have also greatly benefited from very deep models",
          "We split the validation set into two parts (val1/val2) following[8]",
          "Motivated by this, in our current experiment we use the original R-CNN[8]that is RoI-centric, in place of Fast R-CNN",
          "3[8], followed by box voting[6]"
        ]
      }
    },
    {
      "id": "2014-overfeat:_integrated_recognition_localization_and_detection_using_convolutional_networks",
      "type": "core",
      "title": "Overfeat: Integrated recognition, localization and detection using convolutional networks",
      "year": "2014",
      "authors": "Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, Yann LeCun",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Deep convolutional neural networks[22,21]have led to a series of breakthroughs for image classification[21,50,40]",
          "OverFeat[40](ILSVRC’13)",
          "Following[40,41], we assume that the image-level classifiers are first adopted for predicting the class labels of an image, and the localization algorithm only accounts for predicting bounding boxes based on the predicted classes"
        ]
      }
    },
    {
      "id": "2014-microsoft_coco:_common_objects_in_context",
      "type": "core",
      "title": "Microsoft COCO: Common objects in context",
      "year": "2014",
      "authors": "Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Dollár",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Table8and8show the object detection baseline results on PASCAL VOC 2007 and 2012[5]and COCO[26]",
          "The MS COCO dataset[26]involves 80 object categories"
        ]
      }
    },
    {
      "id": "2014-imagenet_large_scale_visual_recognition_challenge",
      "type": "core",
      "title": "Imagenet large scale visual recognition challenge",
      "year": "2014",
      "authors": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, Li Fei-Fei",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Recent evidence[41,44]reveals that network depth is of crucial importance, and the leading results[41,44,13,16]on the challenging ImageNet dataset[36]all exploit “very deep”[41]models, with a depth of sixteen[41]to thirty[16]",
          "On the ImageNet classification dataset[36], we obtain excellent results by extremely deep residual nets",
          "We present comprehensive experiments on ImageNet[36]to show the degradation problem and evaluate our method",
          "The ImageNet Localization (LOC) task[36]requires to classify and localize the objects",
          "We evaluate our method on the ImageNet 2012 classification dataset[36]that consists of 1000 classes"
        ]
      }
    },
    {
      "id": "2014-deeply-supervised_nets",
      "type": "core",
      "title": "Deeply-supervised nets",
      "year": "2014",
      "authors": "Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, Zhuowen Tu",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "DSN[24]",
          "In[44,24], a few intermediate layers are directly connected to auxiliary classifiers for addressing vanishing/exploding gradients",
          "We follow the simple data augmentation in[24]for training: 4 pixels are padded on each side, and a 32×\\times32 crop is randomly sampled from the padded image or its horizontal flip",
          "Strong regularization such as maxout[10]or dropout[14]is applied to obtain the best results ([10,25,24,35]) on this dataset"
        ]
      }
    },
    {
      "id": "2014-caffe:_convolutional_architecture_for_fast_feature_embedding",
      "type": "core",
      "title": "Caffe: Convolutional architecture for fast feature embedding",
      "year": "2014",
      "authors": "Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, Trevor Darrell",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          ", Caffe[19]) without modifying the solvers"
        ]
      }
    },
    {
      "id": "2013-pushing_stochastic_gradient_towards_second-order_methods–backpropagation_learning_with_transformations_in_nonlinearities",
      "type": "core",
      "title": "Pushing stochastic gradient towards second-order methods–backpropagation learning with transformations in nonlinearities",
      "year": "2013",
      "authors": "Tommi Vatanen, Tapani Raiko, Harri Valpola, Yann LeCun",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "The papers of[39,38,31,47]propose methods for centering layer responses, gradients, and propagated errors, implemented by shortcut connections"
        ]
      }
    },
    {
      "id": "2013-network_in_network",
      "type": "core",
      "title": "Network in network",
      "year": "2013",
      "authors": "Min Lin, Qiang Chen, Shuicheng Yan",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Strong regularization such as maxout[10]or dropout[14]is applied to obtain the best results ([10,25,24,35]) on this dataset",
          "NIN[25]"
        ]
      }
    },
    {
      "id": "2013-maxout_networks",
      "type": "core",
      "title": "Maxout networks",
      "year": "2013",
      "authors": "Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, Yoshua Bengio",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Maxout[10]",
          "The testing result of this 1202-layer network is worse than that of our 110-layer network, although both have similar training error"
        ]
      }
    },
    {
      "id": "2013-exact_solutions_to_the_nonlinear_dynamics_of_learning_in_deep_linear_neural_networks",
      "type": "core",
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "year": "2013",
      "authors": "Andrew M. Saxe, James L. McClelland, Surya Ganguli",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "This problem, however, has been largely addressed by normalized initialization[23,9,37,13]and intermediate normalization layers[16], which enable networks with tens of layers to start converging for stochastic gradient descent (SGD) with backpropagation[22]"
        ]
      }
    },
    {
      "id": "2012-improving_neural_networks_by_preventing_co-adaptation_of_feature_detectors",
      "type": "core",
      "title": "Improving neural networks by preventing co-adaptation of feature detectors",
      "year": "2012",
      "authors": "Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, Ruslan R. Salakhutdinov",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "We do not use dropout[14], following the practice in[16]",
          "Strong regularization such as maxout[10]or dropout[14]is applied to obtain the best results ([10,25,24,35]) on this dataset"
        ],
        "2019-temporal_collaborative_ranking_via_personalized_transformer": [
          "For deep neural networks, it has been shown thatℓpsubscriptℓ𝑝\\ell_{p}regularizations are often too weak, while dropout[12,32]is more effective in practice"
        ]
      }
    },
    {
      "id": "2012-imagenet_classification_with_deep_convolutional_neural_networks",
      "type": "core",
      "title": "Imagenet classification with deep convolutional neural networks",
      "year": "2012",
      "authors": "A. Krizhevsky, I. Sutskever, and G. Hinton",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Our implementation for ImageNet follows the practice in[21,41]",
          "Deep convolutional neural networks[22,21]have led to a series of breakthroughs for image classification[21,50,40]",
          "In testing, for comparison studies we adopt the standard 10-crop testing[21]"
        ]
      }
    },
    {
      "id": "2012-deep_learning_made_easier_by_linear_transformations_in_perceptrons",
      "type": "core",
      "title": "Deep learning made easier by linear transformations in perceptrons",
      "year": "2012",
      "authors": "T. Raiko, H. Valpola, and Y. LeCun",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "The papers of[39,38,31,47]propose methods for centering layer responses, gradients, and propagated errors, implemented by shortcut connections"
        ]
      }
    },
    {
      "id": "2012-aggregating_local_image_descriptors_into_compact_codes",
      "type": "core",
      "title": "Aggregating local image descriptors into compact codes",
      "year": "2012",
      "authors": "H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and C. Schmid",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "In image recognition, VLAD[18]is a representation that encodes by the residual vectors with respect to a dictionary, and Fisher Vector[30]can be formulated as a probabilistic version[18]of VLAD"
        ]
      }
    },
    {
      "id": "2011-the_devil_is_in_the_details:_an_evaluation_of_recent_feature_encoding_methods",
      "type": "core",
      "title": "The devil is in the details: an evaluation of recent feature encoding methods",
      "year": "2011",
      "authors": "K. Chatfield, V. Lempitsky, A. Vedaldi, and A. Zisserman",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Both of them are powerful shallow representations for image retrieval and classification[4,48]"
        ]
      }
    },
    {
      "id": "2011-product_quantization_for_nearest_neighbor_search",
      "type": "core",
      "title": "Product quantization for nearest neighbor search",
      "year": "2011",
      "authors": "H. Jegou, M. Douze, and C. Schmid",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "For vector quantization, encoding residual vectors[17]is shown to be more effective than encoding original vectors"
        ]
      }
    },
    {
      "id": "2010-understanding_the_difficulty_of_training_deep_feedforward_neural_networks",
      "type": "core",
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "year": "2010",
      "authors": "X. Glorot and Y. Bengio",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Driven by the significance of depth, a question arises:Is learning better networks as easy as stacking more layers?An obstacle to answering this question was the notorious problem of vanishing/exploding gradients[1,9], which hamper convergence from the beginning"
        ]
      }
    },
    {
      "id": "2010-the_pascal_visual_object_classes_(voc)_challenge",
      "type": "core",
      "title": "The Pascal Visual Object Classes (VOC) Challenge",
      "year": "2010",
      "authors": "M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Table8and8show the object detection baseline results on PASCAL VOC 2007 and 2012[5]and COCO[26]"
        ]
      }
    },
    {
      "id": "2010-rectified_linear_units_improve_restricted_boltzmann_machines",
      "type": "core",
      "title": "Rectified linear units improve restricted boltzmann machines",
      "year": "2010",
      "authors": "V. Nair and G. E. Hinton",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "2that has two layers,ℱ=W2​σ​(W1​𝐱)ℱsubscript𝑊2𝜎subscript𝑊1𝐱\\mathcal{F}=W_{2}\\sigma(W_{1}\\mathbf{x})in whichσ𝜎\\sigmadenotes ReLU[29]and the biases are omitted for simplifying notations"
        ]
      }
    },
    {
      "id": "2009-learning_multiple_layers_of_features_from_tiny_images",
      "type": "core",
      "title": "Learning multiple layers of features from tiny images",
      "year": "2009",
      "authors": "A. Krizhevsky",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Similar phenomena are also shown on the CIFAR-10 set[20], suggesting that the optimization difficulties and the effects of our method are not just akin to a particular dataset",
          "We conducted more studies on the CIFAR-10 dataset[20], which consists of 50k training images and 10k testing images in 10 classes"
        ]
      }
    },
    {
      "id": "2008-vlfeat:_an_open_and_portable_library_of_computer_vision_algorithms_2008",
      "type": "core",
      "title": "VLFeat: An open and portable library of computer vision algorithms, 2008",
      "year": "2008",
      "authors": "A. Vedaldi and B. Fulkerson",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Both of them are powerful shallow representations for image retrieval and classification[4,48]"
        ]
      }
    },
    {
      "id": "2007-fisher_kernels_on_visual_vocabularies_for_image_categorization",
      "type": "core",
      "title": "Fisher kernels on visual vocabularies for image categorization",
      "year": "2007",
      "authors": "F. Perronnin and C. Dance",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "In image recognition, VLAD[18]is a representation that encodes by the residual vectors with respect to a dictionary, and Fisher Vector[30]can be formulated as a probabilistic version[18]of VLAD"
        ]
      }
    },
    {
      "id": "2006-locally_adapted_hierarchical_basis_preconditioning",
      "type": "core",
      "title": "Locally adapted hierarchical basis preconditioning",
      "year": "2006",
      "authors": "R. Szeliski",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "An alternative to Multigrid is hierarchical basis preconditioning[45,46], which relies on variables that represent residual vectors between two scales"
        ]
      }
    },
    {
      "id": "2000-a_multigrid_tutorial",
      "type": "core",
      "title": "A Multigrid Tutorial",
      "year": "2000",
      "authors": "W. L. Briggs, S. F. McCormick, et al",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "In low-level vision and computer graphics, for solving Partial Differential Equations (PDEs), the widely used Multigrid method[3]reformulates the system as subproblems at multiple scales, where each subproblem is responsible for the residual solution between a coarser and a finer scale"
        ]
      }
    },
    {
      "id": "1999-modern_applied_statistics_with_s-plus",
      "type": "core",
      "title": "Modern applied statistics with s-plus",
      "year": "1999",
      "authors": "W. Venables and B. Ripley",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Shortcut connections[2,34,49]are those skipping one or more layers",
          "Practices and theories that lead to shortcut connections[2,34,49]have been studied for a long time"
        ]
      }
    },
    {
      "id": "1998-efficient_backprop",
      "type": "core",
      "title": "Efficient backprop",
      "year": "1998",
      "authors": "Y. LeCun, L. Bottou, G. B. Orr, and K.-R. Müller",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "This problem, however, has been largely addressed by normalized initialization[23,9,37,13]and intermediate normalization layers[16], which enable networks with tens of layers to start converging for stochastic gradient descent (SGD) with backpropagation[22]"
        ]
      }
    },
    {
      "id": "1998-centering_neural_network_gradient_factors",
      "type": "core",
      "title": "Centering neural network gradient factors",
      "year": "1998",
      "authors": "N. N. Schraudolph",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "The papers of[39,38,31,47]propose methods for centering layer responses, gradients, and propagated errors, implemented by shortcut connections"
        ]
      }
    },
    {
      "id": "1998-accelerated_gradient_descent_by_factor-centering_decomposition",
      "type": "core",
      "title": "Accelerated gradient descent by factor-centering decomposition",
      "year": "1998",
      "authors": "N. N. Schraudolph",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "The papers of[39,38,31,47]propose methods for centering layer responses, gradients, and propagated errors, implemented by shortcut connections"
        ]
      }
    },
    {
      "id": "1997-long_short-term_memory",
      "type": "core",
      "title": "Long short-term memory",
      "year": "1997",
      "authors": "Ivo Danihelka, Greg Wayne, Benigno Uria, Nal Kalchbrenner, Alex Graves",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Concurrent with our work, “highway networks”[42,43]present shortcut connections with gating functions[15]"
        ],
        "2017-attention_is_all_you_need": [
          "Recurrent neural networks, long short-term memory[13]and gated recurrent[7]neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation[35,2,5]"
        ]
      }
    },
    {
      "id": "1996-pattern_recognition_and_neural_networks",
      "type": "core",
      "title": "Pattern recognition and neural networks",
      "year": "1996",
      "authors": "B. D. Ripley",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Shortcut connections[2,34,49]are those skipping one or more layers",
          "Practices and theories that lead to shortcut connections[2,34,49]have been studied for a long time"
        ]
      }
    },
    {
      "id": "1995-neural_networks_for_pattern_recognition",
      "type": "core",
      "title": "Neural networks for pattern recognition",
      "year": "1995",
      "authors": "Kyongsik Yun, Alexander Huyen, Thomas Lu",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "2)",
          "Practices and theories that lead to shortcut connections[2,34,49]have been studied for a long time"
        ]
      }
    },
    {
      "id": "1994-learning_long-term_dependencies_with_gradient_descent_is_difficult",
      "type": "core",
      "title": "Learning long-term dependencies with gradient descent is difficult",
      "year": "1994",
      "authors": "Y. Bengio, P. Simard, and P. Frasconi",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "Driven by the significance of depth, a question arises:Is learning better networks as easy as stacking more layers?An obstacle to answering this question was the notorious problem of vanishing/exploding gradients[1,9], which hamper convergence from the beginning"
        ]
      }
    },
    {
      "id": "1990-fast_surface_interpolation_using_hierarchical_basis_functions",
      "type": "core",
      "title": "Fast surface interpolation using hierarchical basis functions",
      "year": "1990",
      "authors": "R. Szeliski",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "An alternative to Multigrid is hierarchical basis preconditioning[45,46], which relies on variables that represent residual vectors between two scales"
        ]
      }
    },
    {
      "id": "1989-backpropagation_applied_to_handwritten_zip_code_recognition",
      "type": "core",
      "title": "Backpropagation applied to handwritten zip code recognition",
      "year": "1989",
      "authors": "Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel",
      "name": null,
      "parents": {
        "2016-deep_residual_learning_for_image_recognition": [
          "This problem, however, has been largely addressed by normalized initialization[23,9,37,13]and intermediate normalization layers[16], which enable networks with tens of layers to start converging for stochastic gradient descent (SGD) with backpropagation[22]",
          "Deep convolutional neural networks[22,21]have led to a series of breakthroughs for image classification[21,50,40]"
        ]
      }
    }
  ]
]