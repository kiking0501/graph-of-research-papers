{
  "name": "GPT-3/Few-Shot LLM",
  "year": 2020,
  "url": "https://ar5iv.labs.arxiv.org/html/2005.14165v4",
  "title": "Language Models are Few-Shot Learners",
  "sections": {
    "S1": "1 Introduction",
    "S2": "2 Approach",
    "S3": "3 Results",
    "S4": "4 Measuring and Preventing Memorization Of Benchmarks",
    "S5": "5 Limitations",
    "S6": "6 Broader Impacts",
    "S7": "7 Related Work",
    "S8": "8 Conclusion",
    "Sx1": "Acknowledgements",
    "Sx2": "Contributions"
  },
  "citations": [
    {
      "section_id": "S1",
      "cite_enum": "82",
      "cite_id": "82",
      "sentence": "First, single-layer representations were learned using word vectors[82,102]and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations[24,81,100](though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models[134]have been directly fine-tuned, entirely removing the need for task-specific architectures[112,20,43]"
    },
    {
      "section_id": "S1",
      "cite_enum": "102",
      "cite_id": "102",
      "sentence": "First, single-layer representations were learned using word vectors[82,102]and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations[24,81,100](though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models[134]have been directly fine-tuned, entirely removing the need for task-specific architectures[112,20,43]"
    },
    {
      "section_id": "S1",
      "cite_enum": "24",
      "cite_id": "24",
      "sentence": "First, single-layer representations were learned using word vectors[82,102]and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations[24,81,100](though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models[134]have been directly fine-tuned, entirely removing the need for task-specific architectures[112,20,43]"
    },
    {
      "section_id": "S1",
      "cite_enum": "81",
      "cite_id": "81",
      "sentence": "First, single-layer representations were learned using word vectors[82,102]and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations[24,81,100](though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models[134]have been directly fine-tuned, entirely removing the need for task-specific architectures[112,20,43]"
    },
    {
      "section_id": "S1",
      "cite_enum": "100",
      "cite_id": "100",
      "sentence": "First, single-layer representations were learned using word vectors[82,102]and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations[24,81,100](though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models[134]have been directly fine-tuned, entirely removing the need for task-specific architectures[112,20,43]"
    },
    {
      "section_id": "S1",
      "cite_enum": "134",
      "cite_id": "134",
      "sentence": "First, single-layer representations were learned using word vectors[82,102]and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations[24,81,100](though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models[134]have been directly fine-tuned, entirely removing the need for task-specific architectures[112,20,43]"
    },
    {
      "section_id": "S1",
      "cite_enum": "112",
      "cite_id": "112",
      "sentence": "First, single-layer representations were learned using word vectors[82,102]and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations[24,81,100](though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models[134]have been directly fine-tuned, entirely removing the need for task-specific architectures[112,20,43]"
    },
    {
      "section_id": "S1",
      "cite_enum": "20",
      "cite_id": "20",
      "sentence": "First, single-layer representations were learned using word vectors[82,102]and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations[24,81,100](though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models[134]have been directly fine-tuned, entirely removing the need for task-specific architectures[112,20,43]"
    },
    {
      "section_id": "S1",
      "cite_enum": "43",
      "cite_id": "43",
      "sentence": "First, single-layer representations were learned using word vectors[82,102]and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations[24,81,100](though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models[134]have been directly fine-tuned, entirely removing the need for task-specific architectures[112,20,43]"
    },
    {
      "section_id": "S1",
      "cite_enum": "116",
      "cite_id": "116",
      "sentence": "This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms[116,74,139,62]"
    },
    {
      "section_id": "S1",
      "cite_enum": "74",
      "cite_id": "74",
      "sentence": "This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms[116,74,139,62]"
    },
    {
      "section_id": "S1",
      "cite_enum": "139",
      "cite_id": "139",
      "sentence": "This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms[116,74,139,62]"
    },
    {
      "section_id": "S1",
      "cite_enum": "62",
      "cite_id": "62",
      "sentence": "This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms[116,74,139,62]"
    },
    {
      "section_id": "S1",
      "cite_enum": "41",
      "cite_id": "41",
      "sentence": "For instance[41]observe that larger models do not necessarily generalize better out-of-distribution"
    },
    {
      "section_id": "S1",
      "cite_enum": "138",
      "cite_id": "138",
      "sentence": "There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly specific to the training distribution and does not generalize well outside it[138,88]"
    },
    {
      "section_id": "S1",
      "cite_enum": "88",
      "cite_id": "88",
      "sentence": "There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly specific to the training distribution and does not generalize well outside it[138,88]"
    },
    {
      "section_id": "S1",
      "cite_enum": "36",
      "cite_id": "36",
      "sentence": "Thus, the performance of fine-tuned models on specific benchmarks, even when it is nominally at human-level, may exaggerate actual performance on the underlying task[36,91]"
    },
    {
      "section_id": "S1",
      "cite_enum": "91",
      "cite_id": "91",
      "sentence": "Thus, the performance of fine-tuned models on specific benchmarks, even when it is nominally at human-level, may exaggerate actual performance on the underlying task[36,91]"
    },
    {
      "section_id": "S1",
      "cite_enum": "117",
      "cite_id": "117",
      "sentence": "Recent work[117]attempts to do this via what we call “in-context learning”, using the text input of a pretrained language model as a form of task specification: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next"
    },
    {
      "section_id": "S1",
      "cite_enum": "117",
      "cite_id": "117",
      "sentence": "While it has shown some initial promise, this approach still achieves results far inferior to fine-tuning – for example[117]achieves only 4% on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind the state of the art"
    },
    {
      "section_id": "S1",
      "cite_enum": "112",
      "cite_id": "112",
      "sentence": "In recent years the capacity of transformer language models has increased substantially, from 100 million parameters[112], to 300 million parameters[20], to 1"
    },
    {
      "section_id": "S1",
      "cite_enum": "20",
      "cite_id": "20",
      "sentence": "In recent years the capacity of transformer language models has increased substantially, from 100 million parameters[112], to 300 million parameters[20], to 1"
    },
    {
      "section_id": "S1",
      "cite_enum": "117",
      "cite_id": "117",
      "sentence": "5 billion parameters[117], to 8 billion parameters[125], 11 billion parameters[116], and finally 17 billion parameters[132]"
    },
    {
      "section_id": "S1",
      "cite_enum": "125",
      "cite_id": "125",
      "sentence": "5 billion parameters[117], to 8 billion parameters[125], 11 billion parameters[116], and finally 17 billion parameters[132]"
    },
    {
      "section_id": "S1",
      "cite_enum": "116",
      "cite_id": "116",
      "sentence": "5 billion parameters[117], to 8 billion parameters[125], 11 billion parameters[116], and finally 17 billion parameters[132]"
    },
    {
      "section_id": "S1",
      "cite_enum": "132",
      "cite_id": "132",
      "sentence": "5 billion parameters[117], to 8 billion parameters[125], 11 billion parameters[116], and finally 17 billion parameters[132]"
    },
    {
      "section_id": "S1",
      "cite_enum": "57",
      "cite_id": "57",
      "sentence": "Each increase has brought improvements in text synthesis and/or downstream NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale[57]"
    },
    {
      "section_id": "S2",
      "cite_enum": "117",
      "cite_id": "117",
      "sentence": "Our basic pre-training approach, including model, data, and training, is similar to the process described in[117], with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training"
    },
    {
      "section_id": "S2",
      "cite_enum": "117",
      "cite_id": "117",
      "sentence": "Our basic pre-training approach, including model, data, and training, is similar to the process described in[117], with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training"
    },
    {
      "section_id": "S2",
      "cite_enum": "88",
      "cite_id": "88",
      "sentence": "The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution[88], and the potential to exploit spurious features of the training data[36,91], potentially resulting in an unfair comparison with human performance"
    },
    {
      "section_id": "S2",
      "cite_enum": "36",
      "cite_id": "36",
      "sentence": "The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution[88], and the potential to exploit spurious features of the training data[36,91], potentially resulting in an unfair comparison with human performance"
    },
    {
      "section_id": "S2",
      "cite_enum": "91",
      "cite_id": "91",
      "sentence": "The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution[88], and the potential to exploit spurious features of the training data[36,91], potentially resulting in an unfair comparison with human performance"
    },
    {
      "section_id": "S2",
      "cite_enum": "117",
      "cite_id": "117",
      "sentence": "Few-Shot (FS)is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning[117], but no weight updates are allowed"
    },
    {
      "section_id": "S2",
      "cite_enum": "45",
      "cite_id": "45",
      "sentence": "As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in ML[45,133]– both involve learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task"
    },
    {
      "section_id": "S2",
      "cite_enum": "133",
      "cite_id": "133",
      "sentence": "As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in ML[45,133]– both involve learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task"
    },
    {
      "section_id": "S2",
      "cite_enum": "117",
      "cite_id": "117",
      "sentence": "We use the same model and architecture as GPT-2[117], including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer[15]"
    },
    {
      "section_id": "S2",
      "cite_enum": "15",
      "cite_id": "15",
      "sentence": "We use the same model and architecture as GPT-2[117], including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer[15]"
    },
    {
      "section_id": "S2",
      "cite_enum": "57",
      "cite_id": "57",
      "sentence": "Previous work[57]suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks"
    },
    {
      "section_id": "S2",
      "cite_enum": "57",
      "cite_id": "57",
      "sentence": "Previous work[57]suggests that validation loss is not strongly sensitive to these parameters within a reasonably broad range"
    },
    {
      "section_id": "S2",
      "cite_enum": "57",
      "cite_id": "57",
      "sentence": "Based on the analysis in Scaling Laws For Neural Language Models[57]we train much larger models on many fewer tokens than is typical As a consequence, although GPT-3 3B is almost 10x larger than RoBERTa-Large (355M params), both models took roughly 50 petaflop/s-days of compute during pre-training"
    },
    {
      "section_id": "S2",
      "cite_enum": "116",
      "cite_id": "116",
      "sentence": "org/the-data/[116]constituting nearly a trillion words"
    },
    {
      "section_id": "S2",
      "cite_enum": "117",
      "cite_id": "117",
      "sentence": "For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset[117], collected by scraping links over a longer period of time, and first described in[57], two internet-based books corpora (Books1 and Books2) and English-language Wikipedia"
    },
    {
      "section_id": "S2",
      "cite_enum": "57",
      "cite_id": "57",
      "sentence": "For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset[117], collected by scraping links over a longer period of time, and first described in[57], two internet-based books corpora (Books1 and Books2) and English-language Wikipedia"
    },
    {
      "section_id": "S2",
      "cite_enum": "57",
      "cite_id": "57",
      "sentence": "As found in[57,85], larger models can typically use a larger batch size, but require a smaller learning rate"
    },
    {
      "section_id": "S2",
      "cite_enum": "85",
      "cite_id": "85",
      "sentence": "As found in[57,85], larger models can typically use a larger batch size, but require a smaller learning rate"
    },
    {
      "section_id": "S2",
      "cite_enum": "85",
      "cite_id": "85",
      "sentence": "As found in[57,85], larger models can typically use a larger batch size, but require a smaller learning rate"
    },
    {
      "section_id": "S2",
      "cite_enum": "116",
      "cite_id": "116",
      "sentence": "“True” or “False” rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by[116](see AppendixG) for details"
    },
    {
      "section_id": "S2",
      "cite_enum": "116",
      "cite_id": "116",
      "sentence": "On tasks with free-form completion, we use beam search with the same parameters as[116]: a beam width of 4 and a length penalty ofα=0"
    },
    {
      "section_id": "S3",
      "cite_enum": "57",
      "cite_id": "57",
      "sentence": "The power-law behavior observed in[57]continues for an additional two orders of magnitude with only small deviations from the predicted curve"
    },
    {
      "section_id": "S3",
      "cite_enum": "57",
      "cite_id": "57",
      "sentence": "As observed in[57], language modeling performance follows a power-law when making efficient use of training compute"
    },
    {
      "section_id": "S3",
      "cite_enum": "86",
      "cite_id": "86",
      "sentence": "We calculate zero-shot perplexity on the Penn Tree Bank (PTB)[86]dataset measured in[117]"
    },
    {
      "section_id": "S3",
      "cite_enum": "117",
      "cite_id": "117",
      "sentence": "We calculate zero-shot perplexity on the Penn Tree Bank (PTB)[86]dataset measured in[117]"
    },
    {
      "section_id": "S3",
      "cite_enum": "117",
      "cite_id": "117",
      "sentence": "a[117]"
    },
    {
      "section_id": "S3",
      "cite_enum": "132",
      "cite_id": "132",
      "sentence": "a[132]b[117]c[64]d[63]"
    },
    {
      "section_id": "S3",
      "cite_enum": "117",
      "cite_id": "117",
      "sentence": "a[132]b[117]c[64]d[63]"
    },
    {
      "section_id": "S3",
      "cite_enum": "64",
      "cite_id": "64",
      "sentence": "a[132]b[117]c[64]d[63]"
    },
    {
      "section_id": "S3",
      "cite_enum": "63",
      "cite_id": "63",
      "sentence": "63b91"
    },
    {
      "section_id": "S3",
      "cite_enum": "132",
      "cite_id": "132",
      "sentence": "7B outperforms the SOTA 17B parameter Turing-NLG[132]in this setting, and GPT-3 175B advances the state of the art by 18%"
    },
    {
      "section_id": "S3",
      "cite_enum": "99",
      "cite_id": "99",
      "sentence": "The LAMBADA dataset[99]tests the modeling of long-range dependencies in text – the model is asked to predict the last word of sentences which require reading a paragraph of context"
    },
    {
      "section_id": "S3",
      "cite_enum": "9",
      "cite_id": "9",
      "sentence": "The LAMBADA dataset[99]tests the modeling of long-range dependencies in text – the model is asked to predict the last word of sentences which require reading a paragraph of context"
    },
    {
      "section_id": "S3",
      "cite_enum": "125",
      "cite_id": "125",
      "sentence": "5% improvement achieved by a doubling of model size between two recent state of the art results ([125]and[132]) and argue that “continuing to expand hardware and data sizes by orders of magnitude is not the path forward”"
    },
    {
      "section_id": "S3",
      "cite_enum": "132",
      "cite_id": "132",
      "sentence": "5% improvement achieved by a doubling of model size between two recent state of the art results ([125]and[132]) and argue that “continuing to expand hardware and data sizes by orders of magnitude is not the path forward”"
    },
    {
      "section_id": "S3",
      "cite_enum": "117",
      "cite_id": "117",
      "sentence": "This problem has been partially addressed in the past with stop-word filters[117](which ban “continuation” words)"
    },
    {
      "section_id": "S3",
      "cite_enum": "140",
      "cite_id": "140",
      "sentence": "The HellaSwag dataset[140]involves picking the best ending to a story or set of instructions"
    },
    {
      "section_id": "S3",
      "cite_enum": "141",
      "cite_id": "141",
      "sentence": "5B parameter language model[141]but still a fair amount lower than the overall SOTA of 85"
    },
    {
      "section_id": "S3",
      "cite_enum": "83",
      "cite_id": "83",
      "sentence": "We next evaluate GPT-3 on the StoryCloze 2016 dataset[83], which involves selecting the correct ending sentence for five-sentence long stories"
    },
    {
      "section_id": "S3",
      "cite_enum": "64",
      "cite_id": "64",
      "sentence": "1% lower than the fine-tuned SOTA using a BERT based model[64]but improves over previous zero-shot results by roughly 10%"
    },
    {
      "section_id": "S3",
      "cite_enum": "75",
      "cite_id": "75",
      "sentence": "RAG (Fine-tuned, Open-Domain)[75]"
    },
    {
      "section_id": "S3",
      "cite_enum": "115",
      "cite_id": "115",
      "sentence": "T5-11B+SSM (Fine-tuned, Closed-Book)[115]"
    },
    {
      "section_id": "S3",
      "cite_enum": "75",
      "cite_id": "75",
      "sentence": "One-shot and few-shot performance make significant gains over zero-shot behavior, matching and exceeding the performance of the SOTA fine-tuned open-domain model, RAG[75]"
    },
    {
      "section_id": "S3",
      "cite_enum": "115",
      "cite_id": "115",
      "sentence": "[115]recently demonstrated that a large language model can perform surprisingly well directly answering the questions without conditioning on auxilliary information"
    },
    {
      "section_id": "S3",
      "cite_enum": "115",
      "cite_id": "115",
      "sentence": "[115]recently demonstrated that a large language model can perform surprisingly well directly answering the questions without conditioning on auxilliary information"
    },
    {
      "section_id": "S3",
      "cite_enum": "58",
      "cite_id": "58",
      "sentence": "We evaluate GPT-3 on the 3 datasets in[115]: Natural Questions[58], WebQuestions[5], and TriviaQA[49], using the same splits"
    },
    {
      "section_id": "S3",
      "cite_enum": "5",
      "cite_id": "5",
      "sentence": "[115]recently demonstrated that a large language model can perform surprisingly well directly answering the questions without conditioning on auxilliary information"
    },
    {
      "section_id": "S3",
      "cite_enum": "49",
      "cite_id": "49",
      "sentence": "We evaluate GPT-3 on the 3 datasets in[115]: Natural Questions[58], WebQuestions[5], and TriviaQA[49], using the same splits"
    },
    {
      "section_id": "S3",
      "cite_enum": "75",
      "cite_id": "75",
      "sentence": "3B parameter dense vector index of 21M documents[75]"
    },
    {
      "section_id": "S3",
      "cite_enum": "123",
      "cite_id": "123",
      "sentence": "Existing unsupervised machine translation approaches often combine pretraining on a pair of monolingual datasets with back-translation[123]to bridge the two languages in a controlled way"
    },
    {
      "section_id": "S3",
      "cite_enum": "61",
      "cite_id": "61",
      "sentence": "XLM[61]"
    },
    {
      "section_id": "S3",
      "cite_enum": "127",
      "cite_id": "127",
      "sentence": "MASS[127]"
    },
    {
      "section_id": "S3",
      "cite_enum": "66",
      "cite_id": "66",
      "sentence": "mBART[66]"
    },
    {
      "section_id": "S3",
      "cite_enum": "101",
      "cite_id": "101",
      "sentence": "SacreBLEUf[101]results reported in AppendixH"
    },
    {
      "section_id": "S3",
      "cite_enum": "29",
      "cite_id": "29",
      "sentence": "1mBART[66]--29"
    },
    {
      "section_id": "S3",
      "cite_enum": "23",
      "cite_id": "23",
      "sentence": "235"
    },
    {
      "section_id": "S3",
      "cite_enum": "136",
      "cite_id": "136",
      "sentence": "a[29]b[23]c[136]d[95]e[66]f[SacreBLEU signature: BLEU+case"
    },
    {
      "section_id": "S3",
      "cite_enum": "95",
      "cite_id": "95",
      "sentence": "a[29]b[23]c[136]d[95]e[66]f[SacreBLEU signature: BLEU+case"
    },
    {
      "section_id": "S3",
      "cite_enum": "66",
      "cite_id": "66",
      "sentence": "1mBART[66]--29"
    },
    {
      "section_id": "S3",
      "cite_enum": "70",
      "cite_id": "70",
      "sentence": "5 BLEU of the overall SOTA which is achieved by a combination of unsupervised pretraining, supervised finetuning on 608K labeled examples, and backtranslation[70]"
    },
    {
      "section_id": "S3",
      "cite_enum": "118",
      "cite_id": "118",
      "sentence": "a[118]b[79]"
    },
    {
      "section_id": "S3",
      "cite_enum": "79",
      "cite_id": "79",
      "sentence": "a[118]b[79]"
    },
    {
      "section_id": "S3",
      "cite_enum": "65",
      "cite_id": "65",
      "sentence": "The Winograd Schemas Challenge[65]is a classical task in NLP that involves determining which word a pronoun refers to, when the pronoun is grammatically ambiguous but semantically unambiguous to a human"
    },
    {
      "section_id": "S3",
      "cite_enum": "118",
      "cite_id": "118",
      "sentence": "Recently fine-tuned language models have achieved near-human performance on the original Winograd dataset, but more difficult versions such as the adversarially-mined Winogrande dataset[118]still significantly lag human performance"
    },
    {
      "section_id": "S3",
      "cite_enum": "117",
      "cite_id": "117",
      "sentence": "On Winograd we test GPT-3 on the original set of 273 Winograd schemas, using the same “partial evaluation” method described in[117]"
    },
    {
      "section_id": "S3",
      "cite_enum": "118",
      "cite_id": "118",
      "sentence": "6% achieved with a fine-tuned high capacity model (T5), and human performance on the task as reported by[118]is 94"
    },
    {
      "section_id": "S3",
      "cite_enum": "55",
      "cite_id": "55",
      "sentence": "0[55]"
    },
    {
      "section_id": "S3",
      "cite_enum": "55",
      "cite_id": "55",
      "sentence": "5[55]"
    },
    {
      "section_id": "S3",
      "cite_enum": "55",
      "cite_id": "55",
      "sentence": "2[55]"
    },
    {
      "section_id": "S3",
      "cite_enum": "11",
      "cite_id": "11",
      "sentence": "The first, PhysicalQA (PIQA)[11], asks common sense questions about how the physical world works and is intended as a probe of grounded understanding of the world"
    },
    {
      "section_id": "S3",
      "cite_enum": "14",
      "cite_id": "14",
      "sentence": "ARC[14]is a dataset of multiple-choice questions collected from 3rd to 9th grade science exams"
    },
    {
      "section_id": "S3",
      "cite_enum": "55",
      "cite_id": "55",
      "sentence": "This is approaching the performance of a fine-tuned RoBERTa baseline (55"
    },
    {
      "section_id": "S3",
      "cite_enum": "55",
      "cite_id": "55",
      "sentence": "This is approaching the performance of a fine-tuned RoBERTa baseline (55"
    },
    {
      "section_id": "S3",
      "cite_enum": "84",
      "cite_id": "84",
      "sentence": "On OpenBookQA[84], GPT-3 improves significantly from zero to few shot settings but is still over 20 points short of the overall SOTA"
    },
    {
      "section_id": "S3",
      "cite_enum": "53",
      "cite_id": "53",
      "sentence": "a[53]b[50]c[2]d[103]e[125]"
    },
    {
      "section_id": "S3",
      "cite_enum": "50",
      "cite_id": "50",
      "sentence": "a[53]b[50]c[2]d[103]e[125]"
    },
    {
      "section_id": "S3",
      "cite_enum": "2",
      "cite_id": "2",
      "sentence": "SettingCoQADROPQuACSQuADv2RACE-hRACE-mFine-tuned SOTA90"
    },
    {
      "section_id": "S3",
      "cite_enum": "103",
      "cite_id": "103",
      "sentence": "a[53]b[50]c[2]d[103]e[125]"
    },
    {
      "section_id": "S3",
      "cite_enum": "125",
      "cite_id": "125",
      "sentence": "a[53]b[50]c[2]d[103]e[125]"
    },
    {
      "section_id": "S3",
      "cite_enum": "106",
      "cite_id": "106",
      "sentence": "GPT-3 performs best (within 3 points of the human baseline) on CoQA[106]a free-form conversational dataset and performs worst (13 F1 below an ELMo baseline) on QuAC[16]a dataset which requires modeling structured dialog acts and answer span selections of teacher-student interactions"
    },
    {
      "section_id": "S3",
      "cite_enum": "16",
      "cite_id": "16",
      "sentence": "GPT-3 performs best (within 3 points of the human baseline) on CoQA[106]a free-form conversational dataset and performs worst (13 F1 below an ELMo baseline) on QuAC[16]a dataset which requires modeling structured dialog acts and answer span selections of teacher-student interactions"
    },
    {
      "section_id": "S3",
      "cite_enum": "27",
      "cite_id": "27",
      "sentence": "On DROP[27], a dataset testing discrete reasoning and numeracy in the context of reading comprehension, GPT-3 in a few-shot setting outperforms the fine-tuned BERT baseline from the original paper but is still well below both human performance and state-of-the-art approaches which augment neural networks with symbolic systems[110]"
    },
    {
      "section_id": "S3",
      "cite_enum": "110",
      "cite_id": "110",
      "sentence": "On DROP[27], a dataset testing discrete reasoning and numeracy in the context of reading comprehension, GPT-3 in a few-shot setting outperforms the fine-tuned BERT baseline from the original paper but is still well below both human performance and state-of-the-art approaches which augment neural networks with symbolic systems[110]"
    },
    {
      "section_id": "S3",
      "cite_enum": "108",
      "cite_id": "108",
      "sentence": "0[108], GPT-3 demonstrates its few-shot learning capabilities, improving by almost 10 F1 (to 69"
    },
    {
      "section_id": "S3",
      "cite_enum": "78",
      "cite_id": "78",
      "sentence": "On RACE[78], a multiple choice dataset of middle school and high school english examinations, GPT-3 performs relatively weakly and is only competitive with the earliest work utilizing contextual representations and is still 45% behind SOTA"
    },
    {
      "section_id": "S3",
      "cite_enum": "135",
      "cite_id": "135",
      "sentence": "In order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark[135][135][17][25][105][54][142][21][8][34][6][96][98]"
    },
    {
      "section_id": "S3",
      "cite_enum": "135",
      "cite_id": "135",
      "sentence": "In order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark[135][135][17][25][105][54][142][21][8][34][6][96][98]"
    },
    {
      "section_id": "S3",
      "cite_enum": "17",
      "cite_id": "17",
      "sentence": "In order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark[135][135][17][25][105][54][142][21][8][34][6][96][98]"
    },
    {
      "section_id": "S3",
      "cite_enum": "25",
      "cite_id": "25",
      "sentence": "In order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark[135][135][17][25][105][54][142][21][8][34][6][96][98]"
    },
    {
      "section_id": "S3",
      "cite_enum": "105",
      "cite_id": "105",
      "sentence": "In order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark[135][135][17][25][105][54][142][21][8][34][6][96][98]"
    },
    {
      "section_id": "S3",
      "cite_enum": "54",
      "cite_id": "54",
      "sentence": "In order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark[135][135][17][25][105][54][142][21][8][34][6][96][98]"
    },
    {
      "section_id": "S3",
      "cite_enum": "142",
      "cite_id": "142",
      "sentence": "In order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark[135][135][17][25][105][54][142][21][8][34][6][96][98]"
    },
    {
      "section_id": "S3",
      "cite_enum": "21",
      "cite_id": "21",
      "sentence": "In order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark[135][135][17][25][105][54][142][21][8][34][6][96][98]"
    },
    {
      "section_id": "S3",
      "cite_enum": "8",
      "cite_id": "8",
      "sentence": "In order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark[135][135][17][25][105][54][142][21][8][34][6][96][98]"
    },
    {
      "section_id": "S3",
      "cite_enum": "34",
      "cite_id": "34",
      "sentence": "In order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark[135][135][17][25][105][54][142][21][8][34][6][96][98]"
    },
    {
      "section_id": "S3",
      "cite_enum": "6",
      "cite_id": "6",
      "sentence": "In order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark[135][135][17][25][105][54][142][21][8][34][6][96][98]"
    },
    {
      "section_id": "S3",
      "cite_enum": "96",
      "cite_id": "96",
      "sentence": "In order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark[135][135][17][25][105][54][142][21][8][34][6][96][98]"
    },
    {
      "section_id": "S3",
      "cite_enum": "98",
      "cite_id": "98",
      "sentence": "In order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark[135][135][17][25][105][54][142][21][8][34][6][96][98]"
    },
    {
      "section_id": "S3",
      "cite_enum": "31",
      "cite_id": "31",
      "sentence": "Natural Language Inference (NLI)[31]concerns the ability to understand the relationship between two sentences"
    },
    {
      "section_id": "S3",
      "cite_enum": "94",
      "cite_id": "94",
      "sentence": "We also evaluate on the recently introduced Adversarial Natural Language Inference (ANLI) dataset[94]"
    },
    {
      "section_id": "S3",
      "cite_enum": "92",
      "cite_id": "92",
      "sentence": "For each task we generate 10,000 examples, which we chose to be the top 10,000 most frequent words as measured by[92]of length more than 4 characters and less than 15 characters"
    },
    {
      "section_id": "S3",
      "cite_enum": "131",
      "cite_id": "131",
      "sentence": "To test GPT-3 on another task that is somewhat unusual relative to the typical distribution of text, we collected a set of 374 “SAT analogy” problems[131]"
    },
    {
      "section_id": "S3",
      "cite_enum": "129",
      "cite_id": "129",
      "sentence": "7% in the zero-shot setting, whereas the average score among college applicants was 57%[129](random guessing yields 20%)"
    },
    {
      "section_id": "S3",
      "cite_enum": "117",
      "cite_id": "117",
      "sentence": "Previous work on generative language models qualitatively tested their ability to generate synthetic “news articles” by conditional sampling from the model given a human-written prompt consisting of a plausible first sentence for a news story[117]"
    },
    {
      "section_id": "S3",
      "cite_enum": "117",
      "cite_id": "117",
      "sentence": "Previous work on generative language models qualitatively tested their ability to generate synthetic “news articles” by conditional sampling from the model given a human-written prompt consisting of a plausible first sentence for a news story[117]"
    },
    {
      "section_id": "S3",
      "cite_enum": "56",
      "cite_id": "56",
      "sentence": "Similar work has been carried out by Kreps et al[56]and Zellers et al[141]"
    },
    {
      "section_id": "S3",
      "cite_enum": "141",
      "cite_id": "141",
      "sentence": "Similar work has been carried out by Kreps et al[56]and Zellers et al[141]"
    },
    {
      "section_id": "S3",
      "cite_enum": "48",
      "cite_id": "48",
      "sentence": "Related work on language model detection by Ippolito et al[48]indicates that automatic discriminators likeGrover[141]and GLTR[37]may have greater success at detecting model generated text than human evaluators"
    },
    {
      "section_id": "S3",
      "cite_enum": "141",
      "cite_id": "141",
      "sentence": "Related work on language model detection by Ippolito et al[48]indicates that automatic discriminators likeGrover[141]and GLTR[37]may have greater success at detecting model generated text than human evaluators"
    },
    {
      "section_id": "S3",
      "cite_enum": "37",
      "cite_id": "37",
      "sentence": "Related work on language model detection by Ippolito et al[48]indicates that automatic discriminators likeGrover[141]and GLTR[37]may have greater success at detecting model generated text than human evaluators"
    },
    {
      "section_id": "S3",
      "cite_enum": "48",
      "cite_id": "48",
      "sentence": "Ippolito et al[48]also note that human accuracy at detecting model generated text increases as humans observe more tokens"
    },
    {
      "section_id": "S3",
      "cite_enum": "13",
      "cite_id": "13",
      "sentence": "A task studied in developmental linguistics[13]is the ability to learn and utilize new words, for example using a word in a sentence after seeing it defined only once, or conversely inferring a word’s meaning from only one usage"
    },
    {
      "section_id": "S4",
      "cite_enum": "130",
      "cite_id": "130",
      "sentence": "This concern is not just hypothetical One of the first papers to train a language model on Common Crawl data[130]detected and removed a training document which overlapped with one of their evaluation datasets"
    },
    {
      "section_id": "S4",
      "cite_enum": "117",
      "cite_id": "117",
      "sentence": "Other work such as GPT-2[117]also conducted post-hoc overlap analysis"
    },
    {
      "section_id": "S5",
      "cite_enum": "11",
      "cite_id": "11",
      "sentence": "Within the domain of discrete language tasks, we have noticed informally that GPT-3 seems to have special difficulty with “common sense physics”, despite doing well on some datasets (such as PIQA[11]) that test this domain"
    },
    {
      "section_id": "S5",
      "cite_enum": "116",
      "cite_id": "116",
      "sentence": "This is a noticeable difference from much of the recent literature, which has documented improved fine-tuning performance when using these approaches over standard language models[116]"
    },
    {
      "section_id": "S5",
      "cite_enum": "115",
      "cite_id": "115",
      "sentence": "[115]demonstrate benefits of customizing prediction to entities of interest"
    },
    {
      "section_id": "S5",
      "cite_enum": "9",
      "cite_id": "9",
      "sentence": "Finally, large pretrained language models are not grounded in other domains of experience, such as video or real-world physical interaction, and thus lack a large amount of context about the world[9]"
    },
    {
      "section_id": "S5",
      "cite_enum": "143",
      "cite_id": "143",
      "sentence": "Promising future directions in this vein might include learning the objective function from humans[143], fine-tuning with reinforcement learning, or adding additional modalities such as images to provide grounding and a better model of the world[18]"
    },
    {
      "section_id": "S5",
      "cite_enum": "18",
      "cite_id": "18",
      "sentence": "Promising future directions in this vein might include learning the objective function from humans[143], fine-tuning with reinforcement learning, or adding additional modalities such as images to provide grounding and a better model of the world[18]"
    },
    {
      "section_id": "S5",
      "cite_enum": "71",
      "cite_id": "71",
      "sentence": "While GPT-3 takes a step towards test-time sample efficiency closer to that of humans (one-shot or zero-shot), it still sees much more text during pre-training than a human sees in the their lifetime[71]"
    },
    {
      "section_id": "S5",
      "cite_enum": "44",
      "cite_id": "44",
      "sentence": "One possible future direction to address this is distillation[44]of large models down to a manageable size for specific tasks"
    },
    {
      "section_id": "S5",
      "cite_enum": "69",
      "cite_id": "69",
      "sentence": "Distillation is well-explored in general[69]but has not been tried at the scale of hundred of billions parameters; new challenges and opportunities may be associated with applying it to models of this size"
    },
    {
      "section_id": "S6",
      "cite_enum": "113",
      "cite_id": "113",
      "sentence": "To help with this, we can think in terms of traditional security risk assessment frameworks, which outline key steps such as identifying threats and potential impacts, assessing likelihood, and determining risk as a combination of likelihood and impact[113]"
    },
    {
      "section_id": "S6",
      "cite_enum": "119",
      "cite_id": "119",
      "sentence": "state-sponsored) groups with long-term agendas[119]"
    },
    {
      "section_id": "S6",
      "cite_enum": "19",
      "cite_id": "19",
      "sentence": "This is concerning, since model bias could harm people in the relevant groups in different ways by entrenching existing stereotypes and producing demeaning portrayals amongst other potential harms[19]"
    },
    {
      "section_id": "footnote8",
      "cite_enum": "46",
      "cite_id": "46",
      "sentence": "See, for example,[46,90,120]"
    },
    {
      "section_id": "footnote8",
      "cite_enum": "90",
      "cite_id": "90",
      "sentence": "See, for example,[46,90,120]"
    },
    {
      "section_id": "footnote8",
      "cite_enum": "120",
      "cite_id": "120",
      "sentence": "See, for example,[46,90,120]"
    },
    {
      "section_id": "S6",
      "cite_enum": "111",
      "cite_id": "111",
      "sentence": "We also carried out pronoun resolution on the Winogender dataset[111]using two methods which further corroborated the model’s tendency to associate most occupations with males"
    },
    {
      "section_id": "S6",
      "cite_enum": "60",
      "cite_id": "60",
      "sentence": "We looked at the adjectives and adverbs in the top 100 most favored words using an off-the-shelf POS tagger[60]"
    },
    {
      "section_id": "S6",
      "cite_enum": "46",
      "cite_id": "46",
      "sentence": "Given prior research demonstrating that language models produce text of differing sentiment when varying features such as occupation[46], we explored how race impacted sentiment"
    },
    {
      "section_id": "S6",
      "cite_enum": "7",
      "cite_id": "7",
      "sentence": "We measured sentiment using Senti WordNet[7]for the words which co-occurred disproportionately with each race"
    },
    {
      "section_id": "S6",
      "cite_enum": "89",
      "cite_id": "89",
      "sentence": "Our work is inspired by the literature on characterizing model attributes to develop informative labels such as Model Cards for Model Reporting from[89]"
    },
    {
      "section_id": "S6",
      "cite_enum": "104",
      "cite_id": "104",
      "sentence": "The literature on this is also extensive[104,46], so we offer only a few brief comments on future directions specific to large language models"
    },
    {
      "section_id": "S6",
      "cite_enum": "46",
      "cite_id": "46",
      "sentence": "The literature on this is also extensive[104,46], so we offer only a few brief comments on future directions specific to large language models"
    },
    {
      "section_id": "S6",
      "cite_enum": "4",
      "cite_id": "4",
      "sentence": "The literature on this is also extensive[104,46], so we offer only a few brief comments on future directions specific to large language models"
    },
    {
      "section_id": "S6",
      "cite_enum": "32",
      "cite_id": "32",
      "sentence": "Thus, mitigation work should not be approached purely with a metric driven objective to ‘remove’ bias as this has been shown to have blind spots[32,93]but in a holistic manner"
    },
    {
      "section_id": "S6",
      "cite_enum": "93",
      "cite_id": "93",
      "sentence": "Thus, mitigation work should not be approached purely with a metric driven objective to ‘remove’ bias as this has been shown to have blind spots[32,93]but in a holistic manner"
    },
    {
      "section_id": "S6",
      "cite_enum": "122",
      "cite_id": "122",
      "sentence": "This means we should be cognizant of the cost and efficiency of such models, as advocated by[122]"
    },
    {
      "section_id": "S6",
      "cite_enum": "69",
      "cite_id": "69",
      "sentence": "Additionally, techniques like model distillation[69]can further bring down the cost of such models, letting us adopt a paradigm of training single, large-scale models, then creating more efficient versions of them for use in appropriate contexts"
    },
    {
      "section_id": "S6",
      "cite_enum": "39",
      "cite_id": "39",
      "sentence": "Algorithmic progress may also naturally further increase the efficiency of such models over time, similar to trends observed in image recognition and neural machine translation[39]"
    },
    {
      "section_id": "S7",
      "cite_enum": "51",
      "cite_id": "51",
      "sentence": "An early work scaled LSTM based language models to over a billion parameters[51]"
    },
    {
      "section_id": "S7",
      "cite_enum": "134",
      "cite_id": "134",
      "sentence": "Work in this vein has successively increased model size: 213 million parameters[134]in the original paper, 300 million parameters[20], 1"
    },
    {
      "section_id": "S7",
      "cite_enum": "20",
      "cite_id": "20",
      "sentence": "Work in this vein has successively increased model size: 213 million parameters[134]in the original paper, 300 million parameters[20], 1"
    },
    {
      "section_id": "S7",
      "cite_enum": "117",
      "cite_id": "117",
      "sentence": "5 billion parameters[117], 8 billion parameters[125], 11 billion parameters[116], and most recently 17 billion parameters[132]"
    },
    {
      "section_id": "S7",
      "cite_enum": "125",
      "cite_id": "125",
      "sentence": "5 billion parameters[117], 8 billion parameters[125], 11 billion parameters[116], and most recently 17 billion parameters[132]"
    },
    {
      "section_id": "S7",
      "cite_enum": "116",
      "cite_id": "116",
      "sentence": "5 billion parameters[117], 8 billion parameters[125], 11 billion parameters[116], and most recently 17 billion parameters[132]"
    },
    {
      "section_id": "S7",
      "cite_enum": "132",
      "cite_id": "132",
      "sentence": "5 billion parameters[117], 8 billion parameters[125], 11 billion parameters[116], and most recently 17 billion parameters[132]"
    },
    {
      "section_id": "S7",
      "cite_enum": "10",
      "cite_id": "10",
      "sentence": "These approaches rely on the conditional computation framework[10]and specifically, the mixture-of-experts method[124]has been used to produce 100 billion parameter models and more recently 50 billion parameter translation models[3], though only a small fraction of the parameters are actually used on each forward pass"
    },
    {
      "section_id": "S7",
      "cite_enum": "124",
      "cite_id": "124",
      "sentence": "These approaches rely on the conditional computation framework[10]and specifically, the mixture-of-experts method[124]has been used to produce 100 billion parameter models and more recently 50 billion parameter translation models[3], though only a small fraction of the parameters are actually used on each forward pass"
    },
    {
      "section_id": "S7",
      "cite_enum": "3",
      "cite_id": "3",
      "sentence": "Work in this vein has successively increased model size: 213 million parameters[134]in the original paper, 300 million parameters[20], 1"
    },
    {
      "section_id": "S7",
      "cite_enum": "35",
      "cite_id": "35",
      "sentence": "A third approach increases computation without increasing parameters; examples of this approach include adaptive computation time[35]and the universal transformer[22]"
    },
    {
      "section_id": "S7",
      "cite_enum": "22",
      "cite_id": "22",
      "sentence": "A third approach increases computation without increasing parameters; examples of this approach include adaptive computation time[35]and the universal transformer[22]"
    },
    {
      "section_id": "S7",
      "cite_enum": "57",
      "cite_id": "57",
      "sentence": "[57,114,77,42], find a smooth power-law trend in loss as autoregressive language models are scaled up"
    },
    {
      "section_id": "S7",
      "cite_enum": "114",
      "cite_id": "114",
      "sentence": "[57,114,77,42], find a smooth power-law trend in loss as autoregressive language models are scaled up"
    },
    {
      "section_id": "S7",
      "cite_enum": "77",
      "cite_id": "77",
      "sentence": "[57,114,77,42], find a smooth power-law trend in loss as autoregressive language models are scaled up"
    },
    {
      "section_id": "S7",
      "cite_enum": "42",
      "cite_id": "42",
      "sentence": "[57,114,77,42], find a smooth power-law trend in loss as autoregressive language models are scaled up"
    },
    {
      "section_id": "S7",
      "cite_enum": "62",
      "cite_id": "62",
      "sentence": "This approach includes ALBERT[62]as well as general[44]and task-specific[121,52,59]approaches to distillation of language models"
    },
    {
      "section_id": "S7",
      "cite_enum": "44",
      "cite_id": "44",
      "sentence": "This approach includes ALBERT[62]as well as general[44]and task-specific[121,52,59]approaches to distillation of language models"
    },
    {
      "section_id": "S7",
      "cite_enum": "121",
      "cite_id": "121",
      "sentence": "This approach includes ALBERT[62]as well as general[44]and task-specific[121,52,59]approaches to distillation of language models"
    },
    {
      "section_id": "S7",
      "cite_enum": "52",
      "cite_id": "52",
      "sentence": "This approach includes ALBERT[62]as well as general[44]and task-specific[121,52,59]approaches to distillation of language models"
    },
    {
      "section_id": "S7",
      "cite_enum": "59",
      "cite_id": "59",
      "sentence": "This approach includes ALBERT[62]as well as general[44]and task-specific[121,52,59]approaches to distillation of language models"
    },
    {
      "section_id": "S7",
      "cite_enum": "58",
      "cite_id": "58",
      "sentence": "As fine-tuned language models have neared human performance on many standard benchmark tasks, considerable effort has been devoted to constructing more difficult or open-ended tasks, including question answering[58,47,14,84], reading comprehension[16,106], and adversarially constructed datasets designed to be difficult for existing language models[118,94]"
    },
    {
      "section_id": "S7",
      "cite_enum": "47",
      "cite_id": "47",
      "sentence": "As fine-tuned language models have neared human performance on many standard benchmark tasks, considerable effort has been devoted to constructing more difficult or open-ended tasks, including question answering[58,47,14,84], reading comprehension[16,106], and adversarially constructed datasets designed to be difficult for existing language models[118,94]"
    },
    {
      "section_id": "S7",
      "cite_enum": "14",
      "cite_id": "14",
      "sentence": "As fine-tuned language models have neared human performance on many standard benchmark tasks, considerable effort has been devoted to constructing more difficult or open-ended tasks, including question answering[58,47,14,84], reading comprehension[16,106], and adversarially constructed datasets designed to be difficult for existing language models[118,94]"
    },
    {
      "section_id": "S7",
      "cite_enum": "84",
      "cite_id": "84",
      "sentence": "As fine-tuned language models have neared human performance on many standard benchmark tasks, considerable effort has been devoted to constructing more difficult or open-ended tasks, including question answering[58,47,14,84], reading comprehension[16,106], and adversarially constructed datasets designed to be difficult for existing language models[118,94]"
    },
    {
      "section_id": "S7",
      "cite_enum": "16",
      "cite_id": "16",
      "sentence": "As fine-tuned language models have neared human performance on many standard benchmark tasks, considerable effort has been devoted to constructing more difficult or open-ended tasks, including question answering[58,47,14,84], reading comprehension[16,106], and adversarially constructed datasets designed to be difficult for existing language models[118,94]"
    },
    {
      "section_id": "S7",
      "cite_enum": "106",
      "cite_id": "106",
      "sentence": "As fine-tuned language models have neared human performance on many standard benchmark tasks, considerable effort has been devoted to constructing more difficult or open-ended tasks, including question answering[58,47,14,84], reading comprehension[16,106], and adversarially constructed datasets designed to be difficult for existing language models[118,94]"
    },
    {
      "section_id": "S7",
      "cite_enum": "118",
      "cite_id": "118",
      "sentence": "As fine-tuned language models have neared human performance on many standard benchmark tasks, considerable effort has been devoted to constructing more difficult or open-ended tasks, including question answering[58,47,14,84], reading comprehension[16,106], and adversarially constructed datasets designed to be difficult for existing language models[118,94]"
    },
    {
      "section_id": "S7",
      "cite_enum": "94",
      "cite_id": "94",
      "sentence": "As fine-tuned language models have neared human performance on many standard benchmark tasks, considerable effort has been devoted to constructing more difficult or open-ended tasks, including question answering[58,47,14,84], reading comprehension[16,106], and adversarially constructed datasets designed to be difficult for existing language models[118,94]"
    },
    {
      "section_id": "S7",
      "cite_enum": "116",
      "cite_id": "116",
      "sentence": "Recent efforts include[116,115], which fine-tuned an 11 billion parameter language model, and[33], which focused on attending over a large corpus of data at test time"
    },
    {
      "section_id": "S7",
      "cite_enum": "115",
      "cite_id": "115",
      "sentence": "Recent efforts include[116,115], which fine-tuned an 11 billion parameter language model, and[33], which focused on attending over a large corpus of data at test time"
    },
    {
      "section_id": "S7",
      "cite_enum": "33",
      "cite_id": "33",
      "sentence": "Recent efforts include[116,115], which fine-tuned an 11 billion parameter language model, and[33], which focused on attending over a large corpus of data at test time"
    },
    {
      "section_id": "S7",
      "cite_enum": "33",
      "cite_id": "33",
      "sentence": "Recent efforts include[116,115], which fine-tuned an 11 billion parameter language model, and[33], which focused on attending over a large corpus of data at test time"
    },
    {
      "section_id": "S7",
      "cite_enum": "75",
      "cite_id": "75",
      "sentence": "Our work differs in focusing on in-context learning but could be combined in the future with those of[33,75]"
    },
    {
      "section_id": "S7",
      "cite_enum": "117",
      "cite_id": "117",
      "sentence": "Metalearning in language models has been utilized in[117], though with much more limited results and no systematic study"
    },
    {
      "section_id": "S7",
      "cite_enum": "133",
      "cite_id": "133",
      "sentence": "More broadly, language model metalearning has an inner-loop-outer-loop structure, making it structurally similar to metalearning as applied to ML in general Here there is an extensive literature, including matching networks[133], RL2[26], learning to optimize[109,1,73]and MAML[30]"
    },
    {
      "section_id": "S7",
      "cite_enum": "26",
      "cite_id": "26",
      "sentence": "More broadly, language model metalearning has an inner-loop-outer-loop structure, making it structurally similar to metalearning as applied to ML in general Here there is an extensive literature, including matching networks[133], RL2[26], learning to optimize[109,1,73]and MAML[30]"
    },
    {
      "section_id": "S7",
      "cite_enum": "109",
      "cite_id": "109",
      "sentence": "More broadly, language model metalearning has an inner-loop-outer-loop structure, making it structurally similar to metalearning as applied to ML in general Here there is an extensive literature, including matching networks[133], RL2[26], learning to optimize[109,1,73]and MAML[30]"
    },
    {
      "section_id": "S7",
      "cite_enum": "1",
      "cite_id": "1",
      "sentence": "Metalearning in language models has been utilized in[117], though with much more limited results and no systematic study"
    },
    {
      "section_id": "S7",
      "cite_enum": "73",
      "cite_id": "73",
      "sentence": "More broadly, language model metalearning has an inner-loop-outer-loop structure, making it structurally similar to metalearning as applied to ML in general Here there is an extensive literature, including matching networks[133], RL2[26], learning to optimize[109,1,73]and MAML[30]"
    },
    {
      "section_id": "S7",
      "cite_enum": "30",
      "cite_id": "30",
      "sentence": "More broadly, language model metalearning has an inner-loop-outer-loop structure, making it structurally similar to metalearning as applied to ML in general Here there is an extensive literature, including matching networks[133], RL2[26], learning to optimize[109,1,73]and MAML[30]"
    },
    {
      "section_id": "S7",
      "cite_enum": "45",
      "cite_id": "45",
      "sentence": "Our approach of stuffing the model’s context with previous examples is most structurally similar to RL2 and also resembles[45], in that an inner loop of adaptation takes place through computation in the model’s activations across timesteps, without updating the weights, while an outer loop (in this case just language model pre-training) updates the weights, and implicitly learns the ability to adapt to or at least recognize tasks defined at inference-time"
    },
    {
      "section_id": "S7",
      "cite_enum": "107",
      "cite_id": "107",
      "sentence": "Few-shot auto-regressive density estimation was explored in[107]and[38]studied low-resource NMT as a few-shot learning problem"
    },
    {
      "section_id": "S7",
      "cite_enum": "38",
      "cite_id": "38",
      "sentence": "Few-shot auto-regressive density estimation was explored in[107]and[38]studied low-resource NMT as a few-shot learning problem"
    },
    {
      "section_id": "S7",
      "cite_enum": "126",
      "cite_id": "126",
      "sentence": "While the mechanism of our few-shot approach is different, prior work has also explored ways of using pre-trained language models in combination with gradient descent to perform few-shot learning[126]"
    },
    {
      "section_id": "S7",
      "cite_enum": "137",
      "cite_id": "137",
      "sentence": "Another sub-field with similar goals is semi-supervised learning where approaches such as UDA[137]also explore methods of fine-tuning when very little labeled data is available"
    },
    {
      "section_id": "S7",
      "cite_enum": "87",
      "cite_id": "87",
      "sentence": "Giving multi-task models instructions in natural language was first formalized in a supervised setting with[87]and utilized for some tasks (such as summarizing) in a language model with[117]"
    },
    {
      "section_id": "S7",
      "cite_enum": "117",
      "cite_id": "117",
      "sentence": "Giving multi-task models instructions in natural language was first formalized in a supervised setting with[87]and utilized for some tasks (such as summarizing) in a language model with[117]"
    },
    {
      "section_id": "S7",
      "cite_enum": "116",
      "cite_id": "116",
      "sentence": "The notion of presenting tasks in natural language was also explored in the text-to-text transformer[116], although there it was applied for multi-task fine-tuning rather than for in-context learning without weight updates"
    },
    {
      "section_id": "S7",
      "cite_enum": "12",
      "cite_id": "12",
      "sentence": "Another approach to increasing generality and transfer-learning capability in language models is multi-task learning[12], which fine-tunes on a mixture of downstream tasks together, rather than separately updating the weights for each one"
    },
    {
      "section_id": "S7",
      "cite_enum": "67",
      "cite_id": "67",
      "sentence": "Multi-task learning has shown some promising initial results[67,76]and multi-stage fine-tuning has recently become a standardized part of SOTA results on some datasets[97]and pushed the boundaries on certain tasks[55], but is still limited by the need to manually curate collections of datasets and set up training curricula"
    },
    {
      "section_id": "S7",
      "cite_enum": "76",
      "cite_id": "76",
      "sentence": "Multi-task learning has shown some promising initial results[67,76]and multi-stage fine-tuning has recently become a standardized part of SOTA results on some datasets[97]and pushed the boundaries on certain tasks[55], but is still limited by the need to manually curate collections of datasets and set up training curricula"
    },
    {
      "section_id": "S7",
      "cite_enum": "97",
      "cite_id": "97",
      "sentence": "Multi-task learning has shown some promising initial results[67,76]and multi-stage fine-tuning has recently become a standardized part of SOTA results on some datasets[97]and pushed the boundaries on certain tasks[55], but is still limited by the need to manually curate collections of datasets and set up training curricula"
    },
    {
      "section_id": "S7",
      "cite_enum": "55",
      "cite_id": "55",
      "sentence": "Multi-task learning has shown some promising initial results[67,76]and multi-stage fine-tuning has recently become a standardized part of SOTA results on some datasets[97]and pushed the boundaries on certain tasks[55], but is still limited by the need to manually curate collections of datasets and set up training curricula"
    },
    {
      "section_id": "S7",
      "cite_enum": "128",
      "cite_id": "128",
      "sentence": "One direction for future work might be attempting to generate a broader set of explicit tasks for multi-task learning, for example through procedural generation[128], human interaction[144], or active learning[80]"
    },
    {
      "section_id": "S7",
      "cite_enum": "144",
      "cite_id": "144",
      "sentence": "One direction for future work might be attempting to generate a broader set of explicit tasks for multi-task learning, for example through procedural generation[128], human interaction[144], or active learning[80]"
    },
    {
      "section_id": "S7",
      "cite_enum": "80",
      "cite_id": "80",
      "sentence": "One direction for future work might be attempting to generate a broader set of explicit tasks for multi-task learning, for example through procedural generation[128], human interaction[144], or active learning[80]"
    },
    {
      "section_id": "S7",
      "cite_enum": "20",
      "cite_id": "20",
      "sentence": "Algorithmic innovation in language models over the last two years has been enormous, including denoising-based bidirectionality[20], prefixLM[24]and encoder-decoder architectures[72,116], random permutations during training[139], architectures that improve the efficiency of sampling[28], improvements in data and training procedures[74], and efficiency increases in the embedding parameters[62]"
    },
    {
      "section_id": "S7",
      "cite_enum": "24",
      "cite_id": "24",
      "sentence": "Algorithmic innovation in language models over the last two years has been enormous, including denoising-based bidirectionality[20], prefixLM[24]and encoder-decoder architectures[72,116], random permutations during training[139], architectures that improve the efficiency of sampling[28], improvements in data and training procedures[74], and efficiency increases in the embedding parameters[62]"
    },
    {
      "section_id": "S7",
      "cite_enum": "72",
      "cite_id": "72",
      "sentence": "Algorithmic innovation in language models over the last two years has been enormous, including denoising-based bidirectionality[20], prefixLM[24]and encoder-decoder architectures[72,116], random permutations during training[139], architectures that improve the efficiency of sampling[28], improvements in data and training procedures[74], and efficiency increases in the embedding parameters[62]"
    },
    {
      "section_id": "S7",
      "cite_enum": "116",
      "cite_id": "116",
      "sentence": "Algorithmic innovation in language models over the last two years has been enormous, including denoising-based bidirectionality[20], prefixLM[24]and encoder-decoder architectures[72,116], random permutations during training[139], architectures that improve the efficiency of sampling[28], improvements in data and training procedures[74], and efficiency increases in the embedding parameters[62]"
    },
    {
      "section_id": "S7",
      "cite_enum": "139",
      "cite_id": "139",
      "sentence": "Algorithmic innovation in language models over the last two years has been enormous, including denoising-based bidirectionality[20], prefixLM[24]and encoder-decoder architectures[72,116], random permutations during training[139], architectures that improve the efficiency of sampling[28], improvements in data and training procedures[74], and efficiency increases in the embedding parameters[62]"
    },
    {
      "section_id": "S7",
      "cite_enum": "28",
      "cite_id": "28",
      "sentence": "Algorithmic innovation in language models over the last two years has been enormous, including denoising-based bidirectionality[20], prefixLM[24]and encoder-decoder architectures[72,116], random permutations during training[139], architectures that improve the efficiency of sampling[28], improvements in data and training procedures[74], and efficiency increases in the embedding parameters[62]"
    },
    {
      "section_id": "S7",
      "cite_enum": "74",
      "cite_id": "74",
      "sentence": "Algorithmic innovation in language models over the last two years has been enormous, including denoising-based bidirectionality[20], prefixLM[24]and encoder-decoder architectures[72,116], random permutations during training[139], architectures that improve the efficiency of sampling[28], improvements in data and training procedures[74], and efficiency increases in the embedding parameters[62]"
    },
    {
      "section_id": "S7",
      "cite_enum": "62",
      "cite_id": "62",
      "sentence": "Algorithmic innovation in language models over the last two years has been enormous, including denoising-based bidirectionality[20], prefixLM[24]and encoder-decoder architectures[72,116], random permutations during training[139], architectures that improve the efficiency of sampling[28], improvements in data and training procedures[74], and efficiency increases in the embedding parameters[62]"
    },
    {
      "section_id": "A2",
      "cite_enum": "68",
      "cite_id": "68",
      "sentence": "1 to provide a small amount of regularization[68]"
    },
    {
      "section_id": "A6",
      "cite_enum": "40",
      "cite_id": "40",
      "sentence": "We first experimented with a few prompts, then generated four samples with no additional editing or selection (sampling at temperature111using nucleus sampling[40]withP=0"
    }
  ],
  "preview": "<div class=\"ltx_para\" id=\"S1.p1\">\n<p class=\"ltx_p\" id=\"S1.p1.1\">Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly flexible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word vectors  and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations  (though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models  have been directly fine-tuned, entirely removing the need for task-specific architectures .</p>\n</div><div class=\"ltx_para\" id=\"S1.p2\">\n<p class=\"ltx_p\" id=\"S1.p2.1\">This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms . However, a major limitation to this approach is that while the architecture is task-agnostic, there is still a need for task-specific datasets and task-specific fine-tuning: to achieve strong performance on a desired task typically requires fine-tuning on a dataset of thousands to hundreds of thousands of examples specific to that task. Removing this limitation would be desirable, for several reasons.</p>\n</div><div class=\"ltx_para\" id=\"S1.p3\">\n<p class=\"ltx_p\" id=\"S1.p3.1\">First, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the applicability of language models. There exists a very wide range of possible useful language tasks, encompassing anything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story. For many of these tasks it is difficult to collect a large supervised training dataset, especially when the process must be repeated for every new task.</p>\n</div><div class=\"ltx_para\" id=\"S1.p4\">\n<p class=\"ltx_p\" id=\"S1.p4.1\">Second, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness of the model and the narrowness of the training distribution. This can create problems for the pre-training plus fine-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then fine-tuned on very narrow task distributions. For instance  observe that larger models do not necessarily generalize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly specific to the training distribution and does not generalize well outside it . Thus, the performance of fine-tuned models on specific benchmarks, even when it is nominally at human-level, may exaggerate actual performance on the underlying task .</p>\n</div><div class=\"ltx_para\" id=\"S1.p5\">\n<p class=\"ltx_p\" id=\"S1.p5.1\">Third, humans do not require large supervised datasets to learn most language tasks – a brief directive in natural language (e.g. “please tell me if this sentence describes something happy or something sad”) or at most a tiny number of demonstrations (e.g. “here are two examples of people acting brave; please give a third example of bravery”) is often sufficient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages – it allows humans to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy dialogue. To be broadly useful, we would someday like our NLP systems to have this same fluidity and generality.</p>\n</div><div class=\"ltx_para\" id=\"S1.p6\">\n<p class=\"ltx_p\" id=\"S1.p6.1\">One potential route towards addressing these issues is meta-learning<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>In the context of language models this has sometimes been called “zero-shot transfer”, but this term is potentially ambiguous: the method is “zero-shot” in the sense that no gradient updates are performed, but it often involves providing inference-time demonstrations to the model, so is not truly learning from zero examples. To avoid this confusion, we use the term “meta-learning” to capture the inner-loop / outer-loop structure of the general method, and the term “in context-learning” to refer to the inner loop of meta-learning. We further specialize the description to “zero-shot”, “one-shot”, or “few-shot” depending on how many demonstrations are provided at inference time. These terms are intended to remain agnostic on the question of whether the model learns new tasks from scratch at inference time or simply recognizes patterns seen during training – this is an important issue which we discuss later in the paper, but “meta-learning” is intended to encompass both possibilities, and simply describes the inner-outer loop structure.</span></span></span> – which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure <a class=\"ltx_ref\" href=\"#S1.F1\" title=\"Figure 1.1 ‣ 1 Introduction ‣ Language Models are Few-Shot Learners\"><span class=\"ltx_text ltx_ref_tag\">1.1</span></a>). Recent work  attempts to do this via what we call “in-context learning”, using the text input of a pretrained language model as a form of task specification: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.</p>\n</div><div class=\"ltx_para\" id=\"S1.p7\">\n<p class=\"ltx_p\" id=\"S1.p7.1\">While it has shown some initial promise, this approach still achieves results far inferior to fine-tuning – for example  achieves only 4% on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind the state of the art. Meta-learning clearly requires substantial improvement in order to be viable as a practical method of solving language tasks.</p>\n</div><div class=\"ltx_para\" id=\"S1.p8\">\n<p class=\"ltx_p\" id=\"S1.p8.1\">Another recent trend in language modeling may offer a way forward. In recent years the capacity of transformer language models has increased substantially, from 100 million parameters , to 300 million parameters , to 1.5 billion parameters , to 8 billion parameters , 11 billion parameters , and finally 17 billion parameters . Each increase has brought improvements in text synthesis and/or downstream NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale . Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale.</p>\n</div><div class=\"ltx_para\" id=\"S1.p9\">\n<p class=\"ltx_p\" id=\"S1.p9.1\">In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call GPT-3, and measuring its in-context learning abilities. Specifically, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set. For each task, we evaluate GPT-3 under 3 conditions: (a) “few-shot learning”, or in-context learning where we allow as many demonstrations as will fit into the model’s context window (typically 10 to 100), (b) “one-shot learning”, where we allow only one demonstration, and (c) “zero-shot” learning, where no demonstrations are allowed and only an instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditional fine-tuning setting, but we leave this to future work.</p>\n</div><div class=\"ltx_para\" id=\"S1.p10\">\n<p class=\"ltx_p\" id=\"S1.p10.1\">Figure <a class=\"ltx_ref\" href=\"#S1.F2\" title=\"Figure 1.2 ‣ 1 Introduction ‣ Language Models are Few-Shot Learners\"><span class=\"ltx_text ltx_ref_tag\">1.2</span></a> illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word. Model performance improves with the addition of a natural language task description, and with the number of examples in the model’s context, <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p10.1.m1.1\"><semantics id=\"S1.p10.1.m1.1a\"><mi id=\"S1.p10.1.m1.1.1\" xref=\"S1.p10.1.m1.1.1.cmml\">K</mi><annotation-xml encoding=\"MathML-Content\" id=\"S1.p10.1.m1.1b\"><ci id=\"S1.p10.1.m1.1.1.cmml\" xref=\"S1.p10.1.m1.1.1\">𝐾</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.p10.1.m1.1c\">K</annotation></semantics></math>. Few-shot learning also improves dramatically with model size. Though the results in this case are particularly striking, the general trends with both model size and number of examples in-context hold for most tasks we study. We emphasize that these “learning” curves involve no gradient updates or fine-tuning, just increasing numbers of demonstrations given as conditioning.</p>\n</div><div class=\"ltx_para\" id=\"S1.p11\">\n<p class=\"ltx_p\" id=\"S1.p11.1\">Broadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held by fine-tuned models). For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in the one-shot setting, 85.0 F1 in the few-shot setting. Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in the zero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting, the last of which is state-of-the-art relative to fine-tuned models operating in the same closed-book setting.</p>\n</div><div class=\"ltx_para\" id=\"S1.p12\">\n<p class=\"ltx_p\" id=\"S1.p12.1\">GPT-3 also displays one-shot and few-shot proficiency at tasks designed to test rapid adaption or on-the-fly reasoning, which include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them defined only once. We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human evaluators have difficulty distinguishing from human-generated articles.</p>\n</div><div class=\"ltx_para\" id=\"S1.p13\">\n<p class=\"ltx_p\" id=\"S1.p13.1\">At the same time, we also find some tasks on which few-shot performance struggles, even at the scale of GPT-3. This includes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE or QuAC. By presenting a broad characterization of GPT-3’s strengths and weaknesses, including these limitations, we hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed.</p>\n</div><div class=\"ltx_para\" id=\"S1.p14\">\n<p class=\"ltx_p\" id=\"S1.p14.1\">A heuristic sense of the overall results can be seen in Figure <a class=\"ltx_ref\" href=\"#S1.F3\" title=\"Figure 1.3 ‣ 1 Introduction ‣ Language Models are Few-Shot Learners\"><span class=\"ltx_text ltx_ref_tag\">1.3</span></a>, which aggregates the various tasks (though it should not be seen as a rigorous or meaningful benchmark in itself).</p>\n</div><div class=\"ltx_para\" id=\"S1.p15\">\n<p class=\"ltx_p\" id=\"S1.p15.1\">We also undertake a systematic study of “data contamination” – a growing problem when training high capacity models on datasets such as Common Crawl, which can potentially include content from test datasets simply because such content often exists on the web. In this paper we develop systematic tools to measure data contamination and quantify its distorting effects. Although we find that data contamination has a minimal effect on GPT-3’s performance on most datasets, we do identify a few datasets where it could be inflating results, and we either do not report results on these datasets or we note them with an asterisk, depending on the severity.</p>\n</div><div class=\"ltx_para\" id=\"S1.p16\">\n<p class=\"ltx_p\" id=\"S1.p16.1\">In addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion parameters) in order to compare their performance to GPT-3 in the zero, one and few-shot settings. Broadly, for most tasks we find relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap between zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models are more proficient meta-learners.</p>\n</div><div class=\"ltx_para\" id=\"S1.p17\">\n<p class=\"ltx_p\" id=\"S1.p17.1\">Finally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and broader societal impacts, and attempt a preliminary analysis of GPT-3’s characteristics in this regard.</p>\n</div><div class=\"ltx_para\" id=\"S1.p18\">\n<p class=\"ltx_p\" id=\"S1.p18.1\">The remainder of this paper is organized as follows. In Section <a class=\"ltx_ref\" href=\"#S2\" title=\"2 Approach ‣ Language Models are Few-Shot Learners\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we describe our approach and methods for training GPT-3 and evaluating it. Section <a class=\"ltx_ref\" href=\"#S3\" title=\"3 Results ‣ Language Models are Few-Shot Learners\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents results on the full range of tasks in the zero-, one- and few-shot settings. Section <a class=\"ltx_ref\" href=\"#S4.F1\" title=\"Figure 4.1 ‣ 4 Measuring and Preventing Memorization Of Benchmarks ‣ Language Models are Few-Shot Learners\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a> addresses questions of data contamination (train-test overlap). Section <a class=\"ltx_ref\" href=\"#S5\" title=\"5 Limitations ‣ Language Models are Few-Shot Learners\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> discusses limitations of GPT-3. Section <a class=\"ltx_ref\" href=\"#S6\" title=\"6 Broader Impacts ‣ Language Models are Few-Shot Learners\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> discusses broader impacts. Section <a class=\"ltx_ref\" href=\"#S7\" title=\"7 Related Work ‣ Language Models are Few-Shot Learners\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> reviews related work and Section <a class=\"ltx_ref\" href=\"#S8\" title=\"8 Conclusion ‣ Language Models are Few-Shot Learners\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> concludes.</p>\n</div>",
  "references": {
    "1": {
      "enum": "1",
      "authors": "Yutian Chen, Matthew W. Hoffman, Sergio Gomez Colmenarejo, Misha Denil, Timothy P. Lillicrap, Matt Botvinick, Nando de Freitas",
      "title": "Learning to learn by gradient descent by gradient descent",
      "publication": "In Advances in neural information processing systems, pages 3981–3989, 2016",
      "year": 2016,
      "summary": "We learn recurrent neural network optimizers trained on simple synthetic functions by gradient descent. We show that these learned optimizers exhibit a remarkable degree of transfer in that they can be used to efficiently optimize a broad range of derivative-free black-box functions, including Gaussian process bandits, simple control objectives, global optimization benchmarks and hyper-parameter tuning tasks. Up to the training horizon, the learned optimizers learn to trade-off exploration and exploitation, and compare favourably with heavily engineered Bayesian optimization packages for hyper-parameter tuning.",
      "standard_url": "http://arxiv.org/abs/1611.03824v6",
      "id": "2016-learning_to_learn_by_gradient_descent_by_gradient_descent"
    },
    "2": {
      "enum": "2",
      "authors": "WeChat AI",
      "title": "Tr-mt (ensemble), December 2019",
      "publication": null,
      "year": "2019",
      "summary": null,
      "standard_url": null,
      "id": "2019-tr-mt_(ensemble)_december_2019"
    },
    "3": {
      "enum": "3",
      "authors": "Roee Aharoni, Melvin Johnson, Orhan Firat",
      "title": "Massively multilingual neural machine translation",
      "publication": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019",
      "year": 2019,
      "summary": "Multilingual neural machine translation (NMT) enables training a single model that supports translation from multiple source languages into multiple target languages. In this paper, we push the limits of multilingual NMT in terms of number of languages being used. We perform extensive experiments in training massively multilingual NMT models, translating up to 102 languages to and from English within a single model. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages. Our experiments on a large-scale dataset with 102 languages to and from English and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.",
      "standard_url": "http://arxiv.org/abs/1903.00089v3",
      "id": "2019-massively_multilingual_neural_machine_translation"
    },
    "4": {
      "enum": "4",
      "authors": "Himel Ghosh, Ahmed Mosharafa, Georg Groh",
      "title": "bias",
      "publication": "in nlp.   arXiv preprint arXiv:2005.14050",
      "year": 2025,
      "summary": "Media bias detection is a critical task in ensuring fair and balanced information dissemination, yet it remains challenging due to the subjectivity of bias and the scarcity of high-quality annotated data. In this work, we perform sentence-level bias classification by fine-tuning a RoBERTa-based model on the expert-annotated BABE dataset. Using McNemar's test and the 5x2 cross-validation paired t-test, we show statistically significant improvements in performance when comparing our model to a domain-adaptively pre-trained DA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model avoids common pitfalls like oversensitivity to politically charged terms and instead attends more meaningfully to contextually relevant tokens. For a comprehensive examination of media bias, we present a pipeline that combines our model with an already-existing bias-type classifier. Our method exhibits good generalization and interpretability, despite being constrained by sentence-level analysis and dataset size because of a lack of larger and more advanced bias corpora. We talk about context-aware modeling, bias neutralization, and advanced bias type classification as potential future directions. Our findings contribute to building more robust, explainable, and socially responsible NLP systems for media bias detection.",
      "standard_url": "http://arxiv.org/abs/2505.13010v2",
      "id": "2025-bias"
    },
    "5": {
      "enum": "5",
      "authors": "Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang",
      "title": "Semantic parsing on freebase from question-answer pairs",
      "publication": "In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533–1544, 2013",
      "year": "2013",
      "summary": null,
      "standard_url": null,
      "id": "2013-semantic_parsing_on_freebase_from_question-answer_pairs"
    },
    "6": {
      "enum": "6",
      "authors": "Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini",
      "title": "The fifth PASCAL recognizing textual entailment challenge",
      "publication": "2009",
      "year": "2009",
      "summary": null,
      "standard_url": null,
      "id": "2009-the_fifth_pascal_recognizing_textual_entailment_challenge"
    },
    "7": {
      "enum": "7",
      "authors": "Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani",
      "title": "Sentiwordnet 3.0: an enhanced lexical resource for sentiment analysis and opinion mining",
      "publication": "In Lrec, volume 10, pages 2200–2204, 2010",
      "year": "2010",
      "summary": null,
      "standard_url": null,
      "id": "2010-sentiwordnet_3.0:_an_enhanced_lexical_resource_for_sentiment_analysis_and_opinion_mining"
    },
    "8": {
      "enum": "8",
      "authors": "Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor",
      "title": "The second PASCAL recognising textual entailment challenge",
      "publication": "2006",
      "year": "2006",
      "summary": null,
      "standard_url": null,
      "id": "2006-the_second_pascal_recognising_textual_entailment_challenge"
    },
    "9": {
      "enum": "9",
      "authors": "Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, Joseph Turian",
      "title": "Experience grounds language",
      "publication": "arXiv preprint arXiv:2004.10151, 2020",
      "year": 2020,
      "summary": "Language understanding research is held back by a failure to relate language to the physical world it describes and to the social interactions it facilitates. Despite the incredible effectiveness of language processing models to tackle tasks after being trained on text alone, successful linguistic communication relies on a shared experience of the world. It is this shared experience that makes utterances meaningful.\n  Natural language processing is a diverse field, and progress throughout its development has come from new representational theories, modeling techniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large, text-only corpora requires the parallel tradition of research on the broader physical and social context of language to address the deeper questions of communication.",
      "standard_url": "http://arxiv.org/abs/2004.10151v3",
      "id": "2020-experience_grounds_language"
    },
    "10": {
      "enum": "10",
      "authors": "Yoshua Bengio, Nicholas Léonard, Aaron Courville",
      "title": "Estimating or propagating gradients through stochastic neurons for conditional computation",
      "publication": "Arxiv, 2013",
      "year": 2013,
      "summary": "Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we \"back-propagate\" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of {\\em conditional computation}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.",
      "standard_url": "http://arxiv.org/abs/1308.3432v1",
      "id": "2013-estimating_or_propagating_gradients_through_stochastic_neurons_for_conditional_computation"
    },
    "11": {
      "enum": "11",
      "authors": "Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, Yejin Choi",
      "title": "Piqa: Reasoning about physical commonsense in natural language",
      "publication": "arXiv preprint arXiv:1911.11641, 2019",
      "year": 2019,
      "summary": "To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of physical commonsense pose a challenge to today's natural language understanding systems. While recent pretrained models (such as BERT) have made progress on question answering over more abstract domains - such as news articles and encyclopedia entries, where text is plentiful - in more physical domains, text is inherently limited due to reporting bias. Can AI systems learn to reliably answer physical common-sense questions without experiencing the physical world? In this paper, we introduce the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering or PIQA. Though humans find the dataset easy (95% accuracy), large pretrained models struggle (77%). We provide analysis about the dimensions of knowledge that existing models lack, which offers significant opportunities for future research.",
      "standard_url": "http://arxiv.org/abs/1911.11641v1",
      "id": "2019-piqa:_reasoning_about_physical_commonsense_in_natural_language"
    },
    "12": {
      "enum": "12",
      "authors": "Jialei Wang, Mladen Kolar, Nathan Srebro",
      "title": "Multitask learning",
      "publication": "Machine learning, 28(1), 1997",
      "year": 2015,
      "summary": "We consider the problem of distributed multi-task learning, where each machine learns a separate, but related, task. Specifically, each machine learns a linear predictor in high-dimensional space,where all tasks share the same small support. We present a communication-efficient estimator based on the debiased lasso and show that it is comparable with the optimal centralized method.",
      "standard_url": "http://arxiv.org/abs/1510.00633v1",
      "id": "2015-multitask_learning"
    },
    "13": {
      "enum": "13",
      "authors": "Susan Carey and Elsa Bartlett",
      "title": "Acquiring a single new word",
      "publication": "Proceedings of the Stanford Child Language Conference, 1978",
      "year": "1978",
      "summary": null,
      "standard_url": null,
      "id": "1978-acquiring_a_single_new_word"
    },
    "14": {
      "enum": "14",
      "authors": "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord",
      "title": "Think you have solved question answering? try arc, the ai2 reasoning challenge",
      "publication": "ArXiv, abs/1803.05457, 2018",
      "year": 2018,
      "summary": "We present a new question set, text corpus, and baselines assembled to encourage AI research in advanced question answering. Together, these constitute the AI2 Reasoning Challenge (ARC), which requires far more powerful knowledge and reasoning than previous challenges such as SQuAD or SNLI. The ARC question set is partitioned into a Challenge Set and an Easy Set, where the Challenge Set contains only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurence algorithm. The dataset contains only natural, grade-school science questions (authored for human tests), and is the largest public-domain set of this kind (7,787 questions). We test several baselines on the Challenge Set, including leading neural models from the SQuAD and SNLI tasks, and find that none are able to significantly outperform a random baseline, reflecting the difficult nature of this task. We are also releasing the ARC Corpus, a corpus of 14M science sentences relevant to the task, and implementations of the three neural baseline models tested. Can your model perform better? We pose ARC as a challenge to the community.",
      "standard_url": "http://arxiv.org/abs/1803.05457v1",
      "id": "2018-think_you_have_solved_question_answering?_try_arc_the_ai2_reasoning_challenge"
    },
    "15": {
      "enum": "15",
      "authors": "Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever",
      "title": "Generating long sequences with sparse transformers, 2019",
      "publication": null,
      "year": "2019",
      "summary": null,
      "standard_url": null,
      "id": "2019-generating_long_sequences_with_sparse_transformers_2019"
    },
    "16": {
      "enum": "16",
      "authors": "Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, Luke Zettlemoyer",
      "title": "Quac : Question answering in context",
      "publication": "Arxiv, 2018",
      "year": 2018,
      "summary": "We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at http://quac.ai.",
      "standard_url": "http://arxiv.org/abs/1808.07036v3",
      "id": "2018-quac_:_question_answering_in_context"
    },
    "17": {
      "enum": "17",
      "authors": "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova",
      "title": "BoolQ: Exploring the surprising difficulty of natural yes/no questions",
      "publication": "arXiv preprint arXiv:1905.10044, 2019",
      "year": 2019,
      "summary": "In this paper we study yes/no questions that are naturally occurring --- meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, BoolQ, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid information, and require difficult entailment-like inference to solve. We also explore the effectiveness of a range of transfer learning baselines. We find that transferring from entailment data is more effective than transferring from paraphrase or extractive QA data, and that it, surprisingly, continues to be very beneficial even when starting from massive pre-trained language models such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on our train set. It achieves 80.4% accuracy compared to 90% accuracy of human annotators (and 62% majority-baseline), leaving a significant gap for future work.",
      "standard_url": "http://arxiv.org/abs/1905.10044v1",
      "id": "2019-boolq:_exploring_the_surprising_difficulty_of_natural_yes/no_questions"
    },
    "18": {
      "enum": "18",
      "authors": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu",
      "title": "Uniter: Learning universal image-text representations",
      "publication": "arXiv preprint arXiv:1909.11740, 2019",
      "year": "2019",
      "summary": null,
      "standard_url": null,
      "id": "2019-uniter:_learning_universal_image-text_representations"
    },
    "19": {
      "enum": "19",
      "authors": "Kate Crawford",
      "title": "The trouble with bias",
      "publication": "NIPS 2017 Keynote, 2017",
      "year": "2017",
      "summary": null,
      "standard_url": null,
      "id": "2017-the_trouble_with_bias"
    },
    "20": {
      "enum": "20",
      "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "publication": "arXiv preprint arXiv:1810.04805, 2018",
      "year": 2018,
      "summary": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n  BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
      "standard_url": "http://arxiv.org/abs/1810.04805v2",
      "id": "2018-bert:_pre-training_of_deep_bidirectional_transformers_for_language_understanding"
    },
    "21": {
      "enum": "21",
      "authors": "Ido Dagan, Oren Glickman, and Bernardo Magnini",
      "title": "The PASCAL recognising textual entailment challenge",
      "publication": "In Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising textual entailment, pages 177–190. Springer, 2006",
      "year": "2006",
      "summary": null,
      "standard_url": null,
      "id": "2006-the_pascal_recognising_textual_entailment_challenge"
    },
    "22": {
      "enum": "22",
      "authors": "Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, Łukasz Kaiser",
      "title": "Universal transformers",
      "publication": "Arxiv, 2018",
      "year": 2018,
      "summary": "Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions, UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.",
      "standard_url": "http://arxiv.org/abs/1807.03819v3",
      "id": "2018-universal_transformers"
    },
    "23": {
      "enum": "23",
      "authors": "Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heafield",
      "title": "Edinburgh’s phrase-based machine translation systems for wmt-14",
      "publication": "In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 97–104, 2014",
      "year": "2014",
      "summary": null,
      "standard_url": null,
      "id": "2014-edinburgh’s_phrase-based_machine_translation_systems_for_wmt-14"
    },
    "24": {
      "enum": "24",
      "authors": "Andrew M. Dai, Quoc V. Le",
      "title": "Semi-supervised sequence learning",
      "publication": "In Advances in neural information processing systems, 2015",
      "year": 2015,
      "summary": "We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a \"pretraining\" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.",
      "standard_url": "http://arxiv.org/abs/1511.01432v1",
      "id": "2015-semi-supervised_sequence_learning"
    },
    "25": {
      "enum": "25",
      "authors": "Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser",
      "title": "The CommitmentBank: Investigating projection in naturally occurring discourse",
      "publication": "2019",
      "year": "2019",
      "summary": null,
      "standard_url": null,
      "id": "2019-the_commitmentbank:_investigating_projection_in_naturally_occurring_discourse"
    },
    "26": {
      "enum": "26",
      "authors": "Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel",
      "title": "Rl2: Fast reinforcement learning via slow reinforcement learning",
      "publication": "ArXiv, abs/1611.02779, 2016",
      "year": "2016",
      "summary": null,
      "standard_url": null,
      "id": "2016-rl2:_fast_reinforcement_learning_via_slow_reinforcement_learning"
    },
    "27": {
      "enum": "27",
      "authors": "Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner",
      "title": "Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
      "publication": "arXiv preprint arXiv:1903.00161, 2019",
      "year": 2019,
      "summary": "Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new English reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 96k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literature on this dataset and show that the best systems only achieve 32.7% F1 on our generalized accuracy metric, while expert human performance is 96.0%. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0% F1.",
      "standard_url": "http://arxiv.org/abs/1903.00161v2",
      "id": "2019-drop:_a_reading_comprehension_benchmark_requiring_discrete_reasoning_over_paragraphs"
    },
    "28": {
      "enum": "28",
      "authors": "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov",
      "title": "Transformer-xl: Attentive language models beyond a fixed-length context",
      "publication": "Arxiv, 2019",
      "year": 2019,
      "summary": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
      "standard_url": "http://arxiv.org/abs/1901.02860v3",
      "id": "2019-transformer-xl:_attentive_language_models_beyond_a_fixed-length_context"
    },
    "29": {
      "enum": "29",
      "authors": "Sergey Edunov, Myle Ott, Michael Auli, David Grangier",
      "title": "Understanding back-translation at scale",
      "publication": "arXiv preprint arXiv:1808.09381, 2018",
      "year": 2018,
      "summary": "An effective method to improve neural machine translation with monolingual data is to augment the parallel training corpus with back-translations of target language sentences. This work broadens the understanding of back-translation and investigates a number of methods to generate synthetic source sentences. We find that in all but resource poor settings back-translations obtained via sampling or noised beam outputs are most effective. Our analysis shows that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. We also compare how synthetic data compares to genuine bitext and study various domain effects. Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT'14 English-German test set.",
      "standard_url": "http://arxiv.org/abs/1808.09381v2",
      "id": "2018-understanding_back-translation_at_scale"
    },
    "30": {
      "enum": "30",
      "authors": "Chelsea Finn, Pieter Abbeel, Sergey Levine",
      "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
      "publication": "ArXiv, abs/1703.03400, 2017",
      "year": 2017,
      "summary": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.",
      "standard_url": "http://arxiv.org/abs/1703.03400v3",
      "id": "2017-model-agnostic_meta-learning_for_fast_adaptation_of_deep_networks"
    },
    "31": {
      "enum": "31",
      "authors": "Yaroslav Fyodorov",
      "title": "A natural logic inference system, 2000",
      "publication": null,
      "year": "2000",
      "summary": null,
      "standard_url": null,
      "id": "2000-a_natural_logic_inference_system_2000"
    },
    "32": {
      "enum": "32",
      "authors": "Hila Gonen, Yoav Goldberg",
      "title": "Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them",
      "publication": "arXiv preprint arXiv:1903.03862, 2019",
      "year": 2019,
      "summary": "Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between \"gender-neutralized\" words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.",
      "standard_url": "http://arxiv.org/abs/1903.03862v2",
      "id": "2019-lipstick_on_a_pig:_debiasing_methods_cover_up_systematic_gender_biases_in_word_embeddings_but_do_not_remove_them"
    },
    "33": {
      "enum": "33",
      "authors": "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang",
      "title": "Realm: Retrieval-augmented language model pre-training",
      "publication": "arXiv preprint arXiv:2002.08909, 2020",
      "year": 2020,
      "summary": "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts.\n  To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents.\n  We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.",
      "standard_url": "http://arxiv.org/abs/2002.08909v1",
      "id": "2020-realm:_retrieval-augmented_language_model_pre-training"
    },
    "34": {
      "enum": "34",
      "authors": "Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan",
      "title": "The third PASCAL recognizing textual entailment challenge",
      "publication": "In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pages 1–9. Association for Computational Linguistics, 2007",
      "year": "2007",
      "summary": null,
      "standard_url": null,
      "id": "2007-the_third_pascal_recognizing_textual_entailment_challenge"
    },
    "35": {
      "enum": "35",
      "authors": "Alex Graves",
      "title": "Adaptive computation time for recurrent neural networks",
      "publication": "Arxiv, 2016",
      "year": 2016,
      "summary": "This paper introduces Adaptive Computation Time (ACT), an algorithm that allows recurrent neural networks to learn how many computational steps to take between receiving an input and emitting an output. ACT requires minimal changes to the network architecture, is deterministic and differentiable, and does not add any noise to the parameter gradients. Experimental results are provided for four synthetic problems: determining the parity of binary vectors, applying binary logic operations, adding integers, and sorting real numbers. Overall, performance is dramatically improved by the use of ACT, which successfully adapts the number of computational steps to the requirements of the problem. We also present character-level language modelling results on the Hutter prize Wikipedia dataset. In this case ACT does not yield large gains in performance; however it does provide intriguing insight into the structure of the data, with more computation allocated to harder-to-predict transitions, such as spaces between words and ends of sentences. This suggests that ACT or other adaptive computation methods could provide a generic method for inferring segment boundaries in sequence data.",
      "standard_url": "http://arxiv.org/abs/1603.08983v6",
      "id": "2016-adaptive_computation_time_for_recurrent_neural_networks"
    },
    "36": {
      "enum": "36",
      "authors": "Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R. Bowman, Noah A. Smith",
      "title": "Annotation artifacts in natural language inference data",
      "publication": "arXiv preprint arXiv:1803.02324, 2018",
      "year": 2018,
      "summary": "Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67% of SNLI (Bowman et. al, 2015) and 53% of MultiNLI (Williams et. al, 2017). Our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem.",
      "standard_url": "http://arxiv.org/abs/1803.02324v2",
      "id": "2018-annotation_artifacts_in_natural_language_inference_data"
    },
    "37": {
      "enum": "37",
      "authors": "Sebastian Gehrmann, Hendrik Strobelt, Alexander M. Rush",
      "title": "Gltr: Statistical detection and visualization of generated text",
      "publication": "arXiv preprint arXiv: 1906.04043, 2019",
      "year": 2019,
      "summary": "The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by and explained to non-experts. We develop GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across common sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54% to 72% without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs",
      "standard_url": "http://arxiv.org/abs/1906.04043v1",
      "id": "2019-gltr:_statistical_detection_and_visualization_of_generated_text"
    },
    "38": {
      "enum": "38",
      "authors": "Jiatao Gu, Yong Wang, Yun Chen, Kyunghyun Cho, Victor O. K. Li",
      "title": "Meta-learning for low-resource neural machine translation",
      "publication": "arXiv preprint arXiv:1808.08437, 2018",
      "year": 2018,
      "summary": "In this paper, we propose to extend the recently introduced model-agnostic meta-learning algorithm (MAML) for low-resource neural machine translation (NMT). We frame low-resource translation as a meta-learning problem, and we learn to adapt to low-resource languages based on multilingual high-resource language tasks. We use the universal lexical representation~\\citep{gu2018universal} to overcome the input-output mismatch across different languages. We evaluate the proposed meta-learning strategy using eighteen European languages (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt, Nl, Pl, Pt, Sk, Sl, Sv and Ru) as source tasks and five diverse languages (Ro, Lv, Fi, Tr and Ko) as target tasks. We show that the proposed approach significantly outperforms the multilingual, transfer learning based approach~\\citep{zoph2016transfer} and enables us to train a competitive NMT system with only a fraction of training examples. For instance, the proposed approach can achieve as high as 22.04 BLEU on Romanian-English WMT'16 by seeing only 16,000 translated words (~600 parallel sentences).",
      "standard_url": "http://arxiv.org/abs/1808.08437v1",
      "id": "2018-meta-learning_for_low-resource_neural_machine_translation"
    },
    "39": {
      "enum": "39",
      "authors": "Daniel Hernandez and Tom Brown",
      "title": "Ai and efficiency, May 2020",
      "publication": null,
      "year": "2020",
      "summary": null,
      "standard_url": null,
      "id": "2020-ai_and_efficiency_may_2020"
    },
    "40": {
      "enum": "40",
      "authors": "Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi",
      "title": "The curious case of neural text degeneration",
      "publication": "CoRR, abs/1904.09751, 2019",
      "year": 2019,
      "summary": "Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive.\n  In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.",
      "standard_url": "http://arxiv.org/abs/1904.09751v2",
      "id": "2019-the_curious_case_of_neural_text_degeneration"
    },
    "41": {
      "enum": "41",
      "authors": "Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, Dawn Song",
      "title": "Pretrained transformers improve out of distribution robustness",
      "publication": "arXiv preprint arXiv:2004.06100, 2020",
      "year": 2020,
      "summary": "Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers' performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.",
      "standard_url": "http://arxiv.org/abs/2004.06100v2",
      "id": "2020-pretrained_transformers_improve_out_of_distribution_robustness"
    },
    "42": {
      "enum": "42",
      "authors": "Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, Yanqi Zhou",
      "title": "Deep learning scaling is predictable, empirically",
      "publication": "arXiv preprint arXiv:1712.00409, 2017",
      "year": 2017,
      "summary": "Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art.\n  This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the \"steepness\" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.",
      "standard_url": "http://arxiv.org/abs/1712.00409v1",
      "id": "2017-deep_learning_scaling_is_predictable_empirically"
    },
    "43": {
      "enum": "43",
      "authors": "Jeremy Howard, Sebastian Ruder",
      "title": "Universal language model fine-tuning for text classification",
      "publication": "arXiv preprint arXiv:1801.06146, 2018",
      "year": 2018,
      "summary": "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.",
      "standard_url": "http://arxiv.org/abs/1801.06146v5",
      "id": "2018-universal_language_model_fine-tuning_for_text_classification"
    },
    "44": {
      "enum": "44",
      "authors": "Geoffrey Hinton, Oriol Vinyals, Jeff Dean",
      "title": "Distilling the knowledge in a neural network",
      "publication": "arXiv preprint arXiv:1503.02531, 2015",
      "year": 2015,
      "summary": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",
      "standard_url": "http://arxiv.org/abs/1503.02531v1",
      "id": "2015-distilling_the_knowledge_in_a_neural_network"
    },
    "45": {
      "enum": "45",
      "authors": "Sepp Hochreiter, A Steven Younger, and Peter R Conwell",
      "title": "Learning to Learn Using Gradient Descent",
      "publication": "In International Conference on Artificial Neural Networks, pages 87–94. Springer, 2001",
      "year": "2001",
      "summary": null,
      "standard_url": null,
      "id": "2001-learning_to_learn_using_gradient_descent"
    },
    "46": {
      "enum": "46",
      "authors": "Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae, Vishal Maini, Dani Yogatama, Pushmeet Kohli",
      "title": "Reducing sentiment bias in language models via counterfactual evaluation",
      "publication": "arXiv preprint arXiv:1911.03064, 2019",
      "year": 2019,
      "summary": "Advances in language modeling architectures and the availability of large text corpora have driven progress in automatic text generation. While this results in models capable of generating coherent texts, it also prompts models to internalize social biases present in the training corpus. This paper aims to quantify and reduce a particular type of bias exhibited by language models: bias in the sentiment of generated text. Given a conditioning context (e.g., a writing prompt) and a language model, we analyze if (and how) the sentiment of the generated text is affected by changes in values of sensitive attributes (e.g., country names, occupations, genders) in the conditioning context using a form of counterfactual evaluation. We quantify sentiment bias by adopting individual and group fairness metrics from the fair machine learning literature, and demonstrate that large-scale models trained on two different corpora (news articles, and Wikipedia) exhibit considerable levels of bias. We then propose embedding and sentiment prediction-derived regularization on the language model's latent representations. The regularizations improve fairness metrics while retaining comparable levels of perplexity and semantic similarity.",
      "standard_url": "http://arxiv.org/abs/1911.03064v3",
      "id": "2019-reducing_sentiment_bias_in_language_models_via_counterfactual_evaluation"
    },
    "47": {
      "enum": "47",
      "authors": "Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daumé III",
      "title": "A neural network for factoid question answering over paragraphs",
      "publication": "In Empirical Methods in Natural Language Processing, 2014",
      "year": "2014",
      "summary": null,
      "standard_url": null,
      "id": "2014-a_neural_network_for_factoid_question_answering_over_paragraphs"
    },
    "48": {
      "enum": "48",
      "authors": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
      "title": "Automatic detection of generated text is easiest when humans are fooled",
      "publication": "arXiv preprint arXiv:1911.00650, 2019",
      "year": 2019,
      "summary": "Recent advancements in neural language modelling make it possible to rapidly generate vast amounts of human-sounding text. The capabilities of humans and automatic discriminators to detect machine-generated text have been a large source of research interest, but humans and machines rely on different cues to make their decisions. Here, we perform careful benchmarking and analysis of three popular sampling-based decoding strategies---top-$k$, nucleus sampling, and untruncated random sampling---and show that improvements in decoding methods have primarily optimized for fooling humans. This comes at the expense of introducing statistical abnormalities that make detection easy for automatic systems. We also show that though both human and automatic detector performance improve with longer excerpt length, even multi-sentence excerpts can fool expert human raters over 30% of the time. Our findings reveal the importance of using both human and automatic detectors to assess the humanness of text generation systems.",
      "standard_url": "http://arxiv.org/abs/1911.00650v2",
      "id": "2019-automatic_detection_of_generated_text_is_easiest_when_humans_are_fooled"
    },
    "49": {
      "enum": "49",
      "authors": "Mandar Joshi, Eunsol Choi, Daniel S. Weld, Luke Zettlemoyer",
      "title": "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension",
      "publication": "arXiv preprint arXiv:1705.03551, 2017",
      "year": 2017,
      "summary": "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at -- http://nlp.cs.washington.edu/triviaqa/",
      "standard_url": "http://arxiv.org/abs/1705.03551v2",
      "id": "2017-triviaqa:_a_large_scale_distantly_supervised_challenge_dataset_for_reading_comprehension"
    },
    "50": {
      "enum": "50",
      "authors": "Zheng Junyuan and Gamma Lab NYC",
      "title": "Numeric transformer - albert, March 2020",
      "publication": null,
      "year": "2020",
      "summary": null,
      "standard_url": null,
      "id": "2020-numeric_transformer_-_albert_march_2020"
    },
    "51": {
      "enum": "51",
      "authors": "Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, Yonghui Wu",
      "title": "Exploring the limits of language modeling",
      "publication": "arXiv preprint arXiv:1602.02410, 2016",
      "year": 2016,
      "summary": "In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.",
      "standard_url": "http://arxiv.org/abs/1602.02410v2",
      "id": "2016-exploring_the_limits_of_language_modeling"
    },
    "52": {
      "enum": "52",
      "authors": "Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu",
      "title": "TinyBERT: Distilling BERT for natural language understanding",
      "publication": "arXiv preprint arXiv:1909.10351, 2019",
      "year": 2019,
      "summary": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large teacher BERT can be effectively transferred to a small student Tiny-BERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pretraining and task-specific learning stages. This framework ensures that TinyBERT can capture he general-domain as well as the task-specific knowledge in BERT.\n  TinyBERT with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERTBASE on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT with 4 layers is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only about 28% parameters and about 31% inference time of them. Moreover, TinyBERT with 6 layers performs on-par with its teacher BERTBASE.",
      "standard_url": "http://arxiv.org/abs/1909.10351v5",
      "id": "2019-tinybert:_distilling_bert_for_natural_language_understanding"
    },
    "53": {
      "enum": "53",
      "authors": "Ying Ju, Fubang Zhao, Shijie Chen, Bowen Zheng, Xuefeng Yang, Yunfeng Liu",
      "title": "Technical report on conversational question answering",
      "publication": "arXiv preprint arXiv:1909.10772, 2019",
      "year": 2019,
      "summary": "Conversational Question Answering is a challenging task since it requires understanding of conversational history. In this project, we propose a new system RoBERTa + AT +KD, which involves rationale tagging multi-task, adversarial training, knowledge distillation and a linguistic post-process strategy. Our single model achieves 90.4(F1) on the CoQA test set without data augmentation, outperforming the current state-of-the-art single model by 2.6% F1.",
      "standard_url": "http://arxiv.org/abs/1909.10772v1",
      "id": "2019-technical_report_on_conversational_question_answering"
    },
    "54": {
      "enum": "54",
      "authors": "Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth",
      "title": "Looking beyond the surface: A challenge set for reading comprehension over multiple sentences",
      "publication": "In Proceedings of North American Chapter of the Association for Computational Linguistics (NAACL), 2018",
      "year": "2018",
      "summary": null,
      "standard_url": null,
      "id": "2018-looking_beyond_the_surface:_a_challenge_set_for_reading_comprehension_over_multiple_sentences"
    },
    "55": {
      "enum": "55",
      "authors": "Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, Hannaneh Hajishirzi",
      "title": "Unifiedqa: Crossing format boundaries with a single qa system",
      "publication": "arXiv preprint arXiv:2005.00700, 2020",
      "year": 2020,
      "summary": "Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained QA model, UnifiedQA, that performs surprisingly well across 17 QA datasets spanning 4 diverse formats. UnifiedQA performs on par with 9 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UnifiedQA performs surprisingly well, showing strong generalization from its out-of-format training data. Finally, simply fine-tuning this pre-trained QA model into specialized models results in a new state of the art on 6 datasets, establishing UnifiedQA as a strong starting point for building QA systems.",
      "standard_url": "http://arxiv.org/abs/2005.00700v3",
      "id": "2020-unifiedqa:_crossing_format_boundaries_with_a_single_qa_system"
    },
    "56": {
      "enum": "56",
      "authors": "Sarah E. Kreps, Miles McCain, and Miles Brundage",
      "title": "All the news that’s fit to fabricate: Ai-generated text as a tool of media misinformation, 2020",
      "publication": null,
      "year": "2020",
      "summary": null,
      "standard_url": null,
      "id": "2020-all_the_news_that’s_fit_to_fabricate:_ai-generated_text_as_a_tool_of_media_misinformation_2020"
    },
    "57": {
      "enum": "57",
      "authors": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei",
      "title": "Scaling laws for neural language models, 2020",
      "publication": null,
      "year": "2020",
      "summary": null,
      "standard_url": null,
      "id": "2020-scaling_laws_for_neural_language_models_2020"
    },
    "58": {
      "enum": "58",
      "authors": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov",
      "title": "Natural questions: a benchmark for question answering research",
      "publication": "Transactions of the Association of Computational Linguistics, 2019",
      "year": "2019",
      "summary": null,
      "standard_url": null,
      "id": "2019-natural_questions:_a_benchmark_for_question_answering_research"
    },
    "59": {
      "enum": "59",
      "authors": "Yoon Kim, Alexander M. Rush",
      "title": "Sequence-level knowledge distillation",
      "publication": "Arxiv, 2016",
      "year": 2016,
      "summary": "Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches. However to reach competitive performance, NMT models need to be exceedingly large. In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT. We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with little loss in performance. It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight pruning on top of knowledge distillation results in a student model that has 13 times fewer parameters than the original teacher model, with a decrease of 0.4 BLEU.",
      "standard_url": "http://arxiv.org/abs/1606.07947v4",
      "id": "2016-sequence-level_knowledge_distillation"
    },
    "60": {
      "enum": "60",
      "authors": "Edward Loper and Steven Bird",
      "title": "Nltk: The natural language toolkit, 2002",
      "publication": null,
      "year": "2002",
      "summary": null,
      "standard_url": null,
      "id": "2002-nltk:_the_natural_language_toolkit_2002"
    },
    "61": {
      "enum": "61",
      "authors": "Guillaume Lample, Alexis Conneau",
      "title": "Cross-lingual language model pretraining",
      "publication": "arXiv preprint arXiv:1901.07291, 2019",
      "year": 2019,
      "summary": "Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.",
      "standard_url": "http://arxiv.org/abs/1901.07291v1",
      "id": "2019-cross-lingual_language_model_pretraining"
    },
    "62": {
      "enum": "62",
      "authors": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut",
      "title": "ALBERT: A lite BERT for self-supervised learning of language representations",
      "publication": "arXiv preprint arXiv:1909.11942, 2019",
      "year": 2019,
      "summary": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.",
      "standard_url": "http://arxiv.org/abs/1909.11942v6",
      "id": "2019-albert:_a_lite_bert_for_self-supervised_learning_of_language_representations"
    },
    "63": {
      "enum": "63",
      "authors": "Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, Jianfeng Gao",
      "title": "Adversarial training for large neural language models",
      "publication": "arXiv preprint arXiv:2004.08994, 2020",
      "year": 2020,
      "summary": "Generalization and robustness are both key desiderata for designing machine learning methods. Adversarial training can enhance robustness, but past work often finds it hurts generalization. In natural language processing (NLP), pre-training large neural language models such as BERT have demonstrated impressive gain in generalization for a variety of tasks, with further improvement from adversarial fine-tuning. However, these models are still vulnerable to adversarial attacks. In this paper, we show that adversarial pre-training can improve both generalization and robustness. We propose a general algorithm ALUM (Adversarial training for large neural LangUage Models), which regularizes the training objective by applying perturbations in the embedding space that maximizes the adversarial loss. We present the first comprehensive study of adversarial training in all stages, including pre-training from scratch, continual pre-training on a well-trained model, and task-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide range of NLP tasks, in both regular and adversarial scenarios. Even for models that have been well trained on extremely large text corpora, such as RoBERTa, ALUM can still produce significant gains from continual pre-training, whereas conventional non-adversarial methods can not. ALUM can be further combined with task-specific fine-tuning to attain additional gains. The ALUM code is publicly available at https://github.com/namisan/mt-dnn.",
      "standard_url": "http://arxiv.org/abs/2004.08994v2",
      "id": "2020-adversarial_training_for_large_neural_language_models"
    },
    "64": {
      "enum": "64",
      "authors": "Zhongyang Li, Xiao Ding, Ting Liu",
      "title": "Story ending prediction by transferable bert",
      "publication": "arXiv preprint arXiv:1905.07504, 2019",
      "year": 2019,
      "summary": "Recent advances, such as GPT and BERT, have shown success in incorporating a pre-trained transformer language model and fine-tuning operation to improve downstream NLP systems. However, this framework still has some fundamental problems in effectively incorporating supervised knowledge from other related tasks. In this study, we investigate a transferable BERT (TransBERT) training framework, which can transfer not only general language knowledge from large-scale unlabeled data but also specific kinds of knowledge from various semantically related supervised tasks, for a target task. Particularly, we propose utilizing three kinds of transfer tasks, including natural language inference, sentiment classification, and next action prediction, to further train BERT based on a pre-trained model. This enables the model to get a better initialization for the target task. We take story ending prediction as the target task to conduct experiments. The final result, an accuracy of 91.8%, dramatically outperforms previous state-of-the-art baseline methods. Several comparative experiments give some helpful suggestions on how to select transfer tasks. Error analysis shows what are the strength and weakness of BERT-based models for story ending prediction.",
      "standard_url": "http://arxiv.org/abs/1905.07504v2",
      "id": "2019-story_ending_prediction_by_transferable_bert"
    },
    "65": {
      "enum": "65",
      "authors": "Vid Kocijan, Ernest Davis, Thomas Lukasiewicz, Gary Marcus, Leora Morgenstern",
      "title": "The Winograd schema challenge",
      "publication": "In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning, 2012",
      "year": 2022,
      "summary": "The Winograd Schema Challenge - a set of twin sentences involving pronoun reference disambiguation that seem to require the use of commonsense knowledge - was proposed by Hector Levesque in 2011. By 2019, a number of AI systems, based on large pre-trained transformer-based language models and fine-tuned on these kinds of problems, achieved better than 90% accuracy. In this paper, we review the history of the Winograd Schema Challenge and discuss the lasting contributions of the flurry of research that has taken place on the WSC in the last decade. We discuss the significance of various datasets developed for WSC, and the research community's deeper understanding of the role of surrogate tasks in assessing the intelligence of an AI system.",
      "standard_url": "http://arxiv.org/abs/2201.02387v3",
      "id": "2022-the_winograd_schema_challenge"
    },
    "66": {
      "enum": "66",
      "authors": "Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer",
      "title": "Multilingual denoising pre-training for neural machine translation",
      "publication": "arXiv preprint arXiv:2001.08210, 2020",
      "year": 2020,
      "summary": "This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART -- a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective. mBART is one of the first methods for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, while previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task-specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show it also enables new types of transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training.",
      "standard_url": "http://arxiv.org/abs/2001.08210v2",
      "id": "2020-multilingual_denoising_pre-training_for_neural_machine_translation"
    },
    "67": {
      "enum": "67",
      "authors": "Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang",
      "title": "Representation learning using multi-task deep neural networks for semantic classification and information retrieval",
      "publication": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2015",
      "year": "2015",
      "summary": null,
      "standard_url": null,
      "id": "2015-representation_learning_using_multi-task_deep_neural_networks_for_semantic_classification_and_information_retrieval"
    },
    "68": {
      "enum": "68",
      "authors": "Ilya Loshchilov, Frank Hutter",
      "title": "Decoupled weight decay regularization",
      "publication": "arXiv preprint arXiv:1711.05101, 2017",
      "year": 2017,
      "summary": "L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \\emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it \"weight decay\" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \\emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW",
      "standard_url": "http://arxiv.org/abs/1711.05101v3",
      "id": "2017-decoupled_weight_decay_regularization"
    },
    "69": {
      "enum": "69",
      "authors": "Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao",
      "title": "Improving multi-task deep neural networks via knowledge distillation for natural language understanding",
      "publication": "arXiv preprint arXiv:1904.09482, 2019",
      "year": 2019,
      "summary": "This paper explores the use of knowledge distillation to improve a Multi-Task Deep Neural Network (MT-DNN) (Liu et al., 2019) for learning text representations across multiple natural language understanding tasks. Although ensemble learning can improve model performance, serving an ensemble of large DNNs such as MT-DNN can be prohibitively expensive. Here we apply the knowledge distillation method (Hinton et al., 2015) in the multi-task learning setting. For each task, we train an ensemble of different MT-DNNs (teacher) that outperforms any single model, and then train a single MT-DNN (student) via multi-task learning to \\emph{distill} knowledge from these ensemble teachers. We show that the distilled MT-DNN significantly outperforms the original MT-DNN on 7 out of 9 GLUE tasks, pushing the GLUE benchmark (single model) to 83.7\\% (1.5\\% absolute improvement\\footnote{ Based on the GLUE leaderboard at https://gluebenchmark.com/leaderboard as of April 1, 2019.}). The code and pre-trained models will be made publicly available at https://github.com/namisan/mt-dnn.",
      "standard_url": "http://arxiv.org/abs/1904.09482v1",
      "id": "2019-improving_multi-task_deep_neural_networks_via_knowledge_distillation_for_natural_language_understanding"
    },
    "70": {
      "enum": "70",
      "authors": "Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao",
      "title": "Multi-task deep neural networks for natural language understanding",
      "publication": "arXiv preprint arXiv:1901.11504, 2019",
      "year": 2019,
      "summary": "In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations in order to adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement). We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. The code and pre-trained models are publicly available at https://github.com/namisan/mt-dnn.",
      "standard_url": "http://arxiv.org/abs/1901.11504v2",
      "id": "2019-multi-task_deep_neural_networks_for_natural_language_understanding"
    },
    "71": {
      "enum": "71",
      "authors": "Tal Linzen",
      "title": "How can we accelerate progress towards human-like linguistic generalization?",
      "publication": "arXiv preprint arXiv:2005.00955, 2020",
      "year": 2020,
      "summary": "This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set. This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set. This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way. We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans.",
      "standard_url": "http://arxiv.org/abs/2005.00955v1",
      "id": "2020-how_can_we_accelerate_progress_towards_human-like_linguistic_generalization?"
    },
    "72": {
      "enum": "72",
      "authors": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer",
      "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "publication": "arXiv preprint arXiv:1910.13461, 2019",
      "year": 2019,
      "summary": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.",
      "standard_url": "http://arxiv.org/abs/1910.13461v1",
      "id": "2019-bart:_denoising_sequence-to-sequence_pre-training_for_natural_language_generation_translation_and_comprehension"
    },
    "73": {
      "enum": "73",
      "authors": "Ke Li, Jitendra Malik",
      "title": "Learning to optimize neural nets",
      "publication": "arXiv preprint arXiv:1703.00441, 2017",
      "year": 2017,
      "summary": "Learning to Optimize is a recently proposed framework for learning optimization algorithms using reinforcement learning. In this paper, we explore learning an optimization algorithm for training shallow neural nets. Such high-dimensional stochastic optimization problems present interesting challenges for existing reinforcement learning algorithms. We develop an extension that is suited to learning optimization algorithms in this setting and demonstrate that the learned optimization algorithm consistently outperforms other known optimization algorithms even on unseen tasks and is robust to changes in stochasticity of gradients and the neural net architecture. More specifically, we show that an optimization algorithm trained with the proposed method on the problem of training a neural net on MNIST generalizes to the problems of training neural nets on the Toronto Faces Dataset, CIFAR-10 and CIFAR-100.",
      "standard_url": "http://arxiv.org/abs/1703.00441v2",
      "id": "2017-learning_to_optimize_neural_nets"
    },
    "74": {
      "enum": "74",
      "authors": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov",
      "title": "RoBERTa: A robustly optimized BERT pretraining approach",
      "publication": "arXiv preprint arXiv:1907.11692, 2019",
      "year": 2019,
      "summary": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
      "standard_url": "http://arxiv.org/abs/1907.11692v1",
      "id": "2019-roberta:_a_robustly_optimized_bert_pretraining_approach"
    },
    "75": {
      "enum": "75",
      "authors": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela",
      "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
      "publication": "arXiv preprint arXiv:2005.11401, 2020",
      "year": 2020,
      "summary": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
      "standard_url": "http://arxiv.org/abs/2005.11401v4",
      "id": "2020-retrieval-augmented_generation_for_knowledge-intensive_nlp_tasks"
    },
    "76": {
      "enum": "76",
      "authors": "Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, Noam Shazeer",
      "title": "Generating Wikipedia by summarizing long sequences",
      "publication": "arXiv preprint arXiv:1801.10198, 2018",
      "year": 2018,
      "summary": "We show that generating English Wikipedia articles can be approached as a multi- document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder- decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations.",
      "standard_url": "http://arxiv.org/abs/1801.10198v1",
      "id": "2018-generating_wikipedia_by_summarizing_long_sequences"
    },
    "77": {
      "enum": "77",
      "authors": "Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joseph E. Gonzalez",
      "title": "Train large, then compress: Rethinking model size for efficient training and inference of transformers, 2020",
      "publication": null,
      "year": "2020",
      "summary": null,
      "standard_url": null,
      "id": "2020-train_large_then_compress:_rethinking_model_size_for_efficient_training_and_inference_of_transformers_2020"
    },
    "78": {
      "enum": "78",
      "authors": "Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Eduard Hovy",
      "title": "Race: Large-scale reading comprehension dataset from examinations",
      "publication": "arXiv preprint arXiv:1704.04683, 2017",
      "year": 2017,
      "summary": "We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students' ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43%) and the ceiling human performance (95%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/~glai1/data/race/ and the code is available at https://github.com/qizhex/RACE_AR_baselines.",
      "standard_url": "http://arxiv.org/abs/1704.04683v5",
      "id": "2017-race:_large-scale_reading_comprehension_dataset_from_examinations"
    },
    "79": {
      "enum": "79",
      "authors": "Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira, Ming-Feng Tsai, Chuan-Ju Wang, Jimmy Lin",
      "title": "Tttttackling winogrande schemas",
      "publication": "arXiv preprint arXiv:2003.08380, 2020",
      "year": 2020,
      "summary": "We applied the T5 sequence-to-sequence model to tackle the AI2 WinoGrande Challenge by decomposing each example into two input text strings, each containing a hypothesis, and using the probabilities assigned to the \"entailment\" token as a score of the hypothesis. Our first (and only) submission to the official leaderboard yielded 0.7673 AUC on March 13, 2020, which is the best known result at this time and beats the previous state of the art by over five points.",
      "standard_url": "http://arxiv.org/abs/2003.08380v1",
      "id": "2020-tttttackling_winogrande_schemas"
    },
    "80": {
      "enum": "80",
      "authors": "David. MacKay",
      "title": "Information-based objective functions for active data selection",
      "publication": "Neural Computation, 1992",
      "year": "1992",
      "summary": null,
      "standard_url": null,
      "id": "1992-information-based_objective_functions_for_active_data_selection"
    },
    "81": {
      "enum": "81",
      "authors": "Bryan McCann, James Bradbury, Caiming Xiong, Richard Socher",
      "title": "Learned in translation: Contextualized word vectors",
      "publication": "In Advances in Neural Information Processing Systems, pages 6294–6305, 2017",
      "year": 2017,
      "summary": "Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.",
      "standard_url": "http://arxiv.org/abs/1708.00107v2",
      "id": "2017-learned_in_translation:_contextualized_word_vectors"
    },
    "82": {
      "enum": "82",
      "authors": "Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean",
      "title": "Efficient estimation of word representations in vector space",
      "publication": "arXiv preprint arXiv:1301.3781, 2013",
      "year": 2013,
      "summary": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",
      "standard_url": "http://arxiv.org/abs/1301.3781v3",
      "id": "2013-efficient_estimation_of_word_representations_in_vector_space"
    },
    "83": {
      "enum": "83",
      "authors": "Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, James Allen",
      "title": "A corpus and evaluation framework for deeper understanding of commonsense stories",
      "publication": "arXiv preprint arXiv:1604.01696, 2016",
      "year": 2016,
      "summary": "Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the 'Story Cloze Test'. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of ~50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding.",
      "standard_url": "http://arxiv.org/abs/1604.01696v1",
      "id": "2016-a_corpus_and_evaluation_framework_for_deeper_understanding_of_commonsense_stories"
    },
    "84": {
      "enum": "84",
      "authors": "Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal",
      "title": "Can a suit of armor conduct electricity? a new dataset for open book question answering",
      "publication": "ArXiv, abs/1809.02789, 2018",
      "year": 2018,
      "summary": "We present a new kind of question answering dataset, OpenBookQA, modeled after open book exams for assessing human understanding of a subject. The open book that comes with our questions is a set of 1329 elementary level science facts. Roughly 6000 questions probe an understanding of these facts and their application to novel situations. This requires combining an open book fact (e.g., metals conduct electricity) with broad common knowledge (e.g., a suit of armor is made of metal) obtained from other sources. While existing QA datasets over documents or knowledge bases, being generally self-contained, focus on linguistic understanding, OpenBookQA probes a deeper understanding of both the topic---in the context of common knowledge---and the language it is expressed in. Human performance on OpenBookQA is close to 92%, but many state-of-the-art pre-trained QA methods perform surprisingly poorly, worse than several simple neural baselines we develop. Our oracle experiments designed to circumvent the knowledge retrieval bottleneck demonstrate the value of both the open book and additional facts. We leave it as a challenge to solve the retrieval problem in this multi-hop setting and to close the large gap to human performance.",
      "standard_url": "http://arxiv.org/abs/1809.02789v1",
      "id": "2018-can_a_suit_of_armor_conduct_electricity?_a_new_dataset_for_open_book_question_answering"
    },
    "85": {
      "enum": "85",
      "authors": "Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team",
      "title": "An empirical model of large-batch training, 2018",
      "publication": null,
      "year": "2018",
      "summary": null,
      "standard_url": null,
      "id": "2018-an_empirical_model_of_large-batch_training_2018"
    },
    "86": {
      "enum": "86",
      "authors": "Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger",
      "title": "The penn treebank: annotating predicate argument structure",
      "publication": "In Proceedings of the workshop on Human Language Technology, pages 114–119. Association for Computational Linguistics, 1994",
      "year": "1994",
      "summary": null,
      "standard_url": null,
      "id": "1994-the_penn_treebank:_annotating_predicate_argument_structure"
    },
    "87": {
      "enum": "87",
      "authors": "Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, Richard Socher",
      "title": "The natural language decathlon: Multitask learning as question answering",
      "publication": "arXiv preprint arXiv:1806.08730, 2018",
      "year": 2018,
      "summary": "Deep learning has improved performance on many natural language processing (NLP) tasks individually. However, general NLP models cannot emerge within a paradigm that focuses on the particularities of a single metric, dataset, and task. We introduce the Natural Language Decathlon (decaNLP), a challenge that spans ten tasks: question answering, machine translation, summarization, natural language inference, sentiment analysis, semantic role labeling, zero-shot relation extraction, goal-oriented dialogue, semantic parsing, and commonsense pronoun resolution. We cast all tasks as question answering over a context. Furthermore, we present a new Multitask Question Answering Network (MQAN) jointly learns all tasks in decaNLP without any task-specific modules or parameters in the multitask setting. MQAN shows improvements in transfer learning for machine translation and named entity recognition, domain adaptation for sentiment analysis and natural language inference, and zero-shot capabilities for text classification. We demonstrate that the MQAN's multi-pointer-generator decoder is key to this success and performance further improves with an anti-curriculum training strategy. Though designed for decaNLP, MQAN also achieves state of the art results on the WikiSQL semantic parsing task in the single-task setting. We also release code for procuring and processing data, training and evaluating models, and reproducing all experiments for decaNLP.",
      "standard_url": "http://arxiv.org/abs/1806.08730v1",
      "id": "2018-the_natural_language_decathlon:_multitask_learning_as_question_answering"
    },
    "88": {
      "enum": "88",
      "authors": "R. Thomas McCoy, Ellie Pavlick, Tal Linzen",
      "title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
      "publication": "arXiv preprint arXiv:1902.01007, 2019",
      "year": 2019,
      "summary": "A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area",
      "standard_url": "http://arxiv.org/abs/1902.01007v4",
      "id": "2019-right_for_the_wrong_reasons:_diagnosing_syntactic_heuristics_in_natural_language_inference"
    },
    "89": {
      "enum": "89",
      "authors": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru",
      "title": "Model cards for model reporting, 2018",
      "publication": null,
      "year": "2018",
      "summary": null,
      "standard_url": null,
      "id": "2018-model_cards_for_model_reporting_2018"
    },
    "90": {
      "enum": "90",
      "authors": "Moin Nadeem, Anna Bethke, Siva Reddy",
      "title": "Stereoset: Measuring stereotypical bias in pretrained language models",
      "publication": "arXiv preprint arXiv:2004.09456, 2020",
      "year": 2020,
      "summary": "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or Asians are bad drivers. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real world data, they are known to capture stereotypical biases. In order to assess the adverse effects of these models, it is important to quantify the bias captured in them. Existing literature on quantifying bias evaluates pretrained language models on a small set of artificially constructed bias-assessing sentences. We present StereoSet, a large-scale natural dataset in English to measure stereotypical biases in four domains: gender, profession, race, and religion. We evaluate popular models like BERT, GPT-2, RoBERTa, and XLNet on our dataset and show that these models exhibit strong stereotypical biases. We also present a leaderboard with a hidden test set to track the bias of future language models at https://stereoset.mit.edu",
      "standard_url": "http://arxiv.org/abs/2004.09456v1",
      "id": "2020-stereoset:_measuring_stereotypical_bias_in_pretrained_language_models"
    },
    "91": {
      "enum": "91",
      "authors": "Timothy Niven, Hung-Yu Kao",
      "title": "Probing neural network comprehension of natural language arguments",
      "publication": "arXiv preprint arXiv:1907.07355, 2019",
      "year": 2019,
      "summary": "We are surprised to find that BERT's peak performance of 77% on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work.",
      "standard_url": "http://arxiv.org/abs/1907.07355v2",
      "id": "2019-probing_neural_network_comprehension_of_natural_language_arguments"
    },
    "92": {
      "enum": "92",
      "authors": "Peter Norvig",
      "title": "Natural language corpus data, 2009",
      "publication": null,
      "year": "2009",
      "summary": null,
      "standard_url": null,
      "id": "2009-natural_language_corpus_data_2009"
    },
    "93": {
      "enum": "93",
      "authors": "Malvina Nissim, Rik van Noord, and Rob van der Goot",
      "title": "Fair is better than sensational: Man is to doctor as woman is to doctor",
      "publication": "arXiv preprint arXiv:1905.09866, 2019",
      "year": "2019",
      "summary": null,
      "standard_url": null,
      "id": "2019-fair_is_better_than_sensational:_man_is_to_doctor_as_woman_is_to_doctor"
    },
    "94": {
      "enum": "94",
      "authors": "Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, Douwe Kiela",
      "title": "Adversarial nli: A new benchmark for natural language understanding",
      "publication": "arXiv preprint arXiv:1910.14599, 2019",
      "year": 2019,
      "summary": "We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.",
      "standard_url": "http://arxiv.org/abs/1910.14599v2",
      "id": "2019-adversarial_nli:_a_new_benchmark_for_natural_language_understanding"
    },
    "95": {
      "enum": "95",
      "authors": "University of Regensburg",
      "title": "Fascha, 2016",
      "publication": null,
      "year": "2016",
      "summary": null,
      "standard_url": null,
      "id": "2016-fascha_2016"
    },
    "96": {
      "enum": "96",
      "authors": "Mohammad Taher Pilehvar and Jose Camacho-Collados",
      "title": "WIC: 10,000 example pairs for evaluating context-sensitive representations",
      "publication": "arXiv preprint arXiv:1808.09121, 2018",
      "year": "2018",
      "summary": null,
      "standard_url": null,
      "id": "2018-wic:_10000_example_pairs_for_evaluating_context-sensitive_representations"
    },
    "97": {
      "enum": "97",
      "authors": "Jason Phang, Thibault Févry, Samuel R. Bowman",
      "title": "Sentence encoders on STILTs: Supplementary training on intermediate labeled-data tasks",
      "publication": "arXiv preprint arXiv:1811.01088, 2018",
      "year": 2018,
      "summary": "Pretraining sentence encoders with language modeling and related unsupervised tasks has recently been shown to be very effective for language understanding tasks. By supplementing language model-style pretraining with further training on data-rich supervised tasks, such as natural language inference, we obtain additional performance improvements on the GLUE benchmark. Applying supplementary training on BERT (Devlin et al., 2018), we attain a GLUE score of 81.8---the state of the art (as of 02/24/2019) and a 1.4 point improvement over BERT. We also observe reduced variance across random restarts in this setting. Our approach yields similar improvements when applied to ELMo (Peters et al., 2018a) and Radford et al. (2018)'s model. In addition, the benefits of supplementary training are particularly pronounced in data-constrained regimes, as we show in experiments with artificially limited training data.",
      "standard_url": "http://arxiv.org/abs/1811.01088v2",
      "id": "2018-sentence_encoders_on_stilts:_supplementary_training_on_intermediate_labeled-data_tasks"
    },
    "98": {
      "enum": "98",
      "authors": "Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White, Benjamin Van Durme",
      "title": "Collecting diverse natural language inference problems for sentence representation evaluation",
      "publication": "In Proceedings of EMNLP, 2018",
      "year": 2018,
      "summary": "We present a large-scale collection of diverse natural language inference (NLI) datasets that help provide insight into how well a sentence representation captures distinct types of reasoning. The collection results from recasting 13 existing datasets from 7 semantic phenomena into a common NLI structure, resulting in over half a million labeled context-hypothesis pairs in total. We refer to our collection as the DNC: Diverse Natural Language Inference Collection. The DNC is available online at https://www.decomp.net, and will grow over time as additional resources are recast and added from novel sources.",
      "standard_url": "http://arxiv.org/abs/1804.08207v2",
      "id": "2018-collecting_diverse_natural_language_inference_problems_for_sentence_representation_evaluation"
    },
    "99": {
      "enum": "99",
      "authors": "Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, Raquel Fernández",
      "title": "The lambada dataset: Word prediction requiring a broad discourse context",
      "publication": "arXiv preprint arXiv:1606.06031, 2016",
      "year": 2016,
      "summary": "We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text.",
      "standard_url": "http://arxiv.org/abs/1606.06031v1",
      "id": "2016-the_lambada_dataset:_word_prediction_requiring_a_broad_discourse_context"
    },
    "100": {
      "enum": "100",
      "authors": "Matthew E. Peters, Mark Neumann, Luke Zettlemoyer, and Wen tau Yih",
      "title": "Dissecting contextual word embeddings: Architecture and representation, 2018",
      "publication": null,
      "year": "2018",
      "summary": null,
      "standard_url": null,
      "id": "2018-dissecting_contextual_word_embeddings:_architecture_and_representation_2018"
    },
    "101": {
      "enum": "101",
      "authors": "Matt Post",
      "title": "A call for clarity in reporting BLEU scores",
      "publication": "arXiv preprint arXiv:1804.08771, 2018",
      "year": 2018,
      "summary": "The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to \"the\" BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide a new tool, SacreBLEU, to facilitate this.",
      "standard_url": "http://arxiv.org/abs/1804.08771v2",
      "id": "2018-a_call_for_clarity_in_reporting_bleu_scores"
    },
    "102": {
      "enum": "102",
      "authors": "Jeffrey Pennington, Richard Socher, and Christopher Manning",
      "title": "GloVe: Global vectors for word representation",
      "publication": "In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), 2014",
      "year": "2014",
      "summary": null,
      "standard_url": null,
      "id": "2014-glove:_global_vectors_for_word_representation"
    },
    "103": {
      "enum": "103",
      "authors": "QIANXIN",
      "title": "Sa-net on albert (ensemble), April 2020",
      "publication": null,
      "year": "2020",
      "summary": null,
      "standard_url": null,
      "id": "2020-sa-net_on_albert_(ensemble)_april_2020"
    },
    "104": {
      "enum": "104",
      "authors": "Yusu Qian, Urwa Muaz, Ben Zhang, Jae Won Hyun",
      "title": "Reducing gender bias in word-level language models with a gender-equalizing loss function",
      "publication": "arXiv preprint arXiv:1905.12801, 2019",
      "year": 2019,
      "summary": "Gender bias exists in natural language datasets which neural language models tend to learn, resulting in biased text generation. In this research, we propose a debiasing approach based on the loss function modification. We introduce a new term to the loss function which attempts to equalize the probabilities of male and female words in the output. Using an array of bias evaluation metrics, we provide empirical evidence that our approach successfully mitigates gender bias in language models without increasing perplexity. In comparison to existing debiasing strategies, data augmentation, and word embedding debiasing, our method performs better in several aspects, especially in reducing gender bias in occupation words. Finally, we introduce a combination of data augmentation and our approach, and show that it outperforms existing strategies in all bias evaluation metrics.",
      "standard_url": "http://arxiv.org/abs/1905.12801v2",
      "id": "2019-reducing_gender_bias_in_word-level_language_models_with_a_gender-equalizing_loss_function"
    },
    "105": {
      "enum": "105",
      "authors": "Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon",
      "title": "Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
      "publication": "In 2011 AAAI Spring Symposium Series, 2011",
      "year": "2011",
      "summary": null,
      "standard_url": null,
      "id": "2011-choice_of_plausible_alternatives:_an_evaluation_of_commonsense_causal_reasoning"
    },
    "106": {
      "enum": "106",
      "authors": "Siva Reddy, Danqi Chen, Christopher D. Manning",
      "title": "Coqa: A conversational question answering challenge",
      "publication": "Transactions of the Association for Computational Linguistics, 7:249–266, 2019",
      "year": 2018,
      "summary": "Humans gather information by engaging in conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning. We evaluate strong conversational and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4%, which is 23.4 points behind human performance (88.8%), indicating there is ample room for improvement. We launch CoQA as a challenge to the community at http://stanfordnlp.github.io/coqa/",
      "standard_url": "http://arxiv.org/abs/1808.07042v2",
      "id": "2018-coqa:_a_conversational_question_answering_challenge"
    },
    "107": {
      "enum": "107",
      "authors": "Scott Reed, Yutian Chen, Thomas Paine, Aäron van den Oord, S. M. Ali Eslami, Danilo Rezende, Oriol Vinyals, Nando de Freitas",
      "title": "Few-shot autoregressive density estimation: Towards learning to learn distributions",
      "publication": "arXiv preprint arXiv:1710.10304, 2017",
      "year": 2017,
      "summary": "Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet. However, such models require many thousands of gradient-based weight updates and unique image examples for training. Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks. In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset. Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision. Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.",
      "standard_url": "http://arxiv.org/abs/1710.10304v4",
      "id": "2017-few-shot_autoregressive_density_estimation:_towards_learning_to_learn_distributions"
    },
    "108": {
      "enum": "108",
      "authors": "Pranav Rajpurkar, Robin Jia, and Percy Liang",
      "title": "Know what you don’t know: Unanswerable questions for squad",
      "publication": "arXiv preprint arXiv:1806.03822, 2018",
      "year": "2018",
      "summary": null,
      "standard_url": null,
      "id": "2018-know_what_you_don’t_know:_unanswerable_questions_for_squad"
    },
    "109": {
      "enum": "109",
      "authors": "Sachin Ravi and Hugo Larochelle",
      "title": "Optimization as a model for few-shot learning",
      "publication": "ICLR 2017 (oral), 2016",
      "year": "2016",
      "summary": null,
      "standard_url": null,
      "id": "2016-optimization_as_a_model_for_few-shot_learning"
    },
    "110": {
      "enum": "110",
      "authors": "Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, Zhiyuan Liu",
      "title": "NumNet: Machine reading comprehension with numerical reasoning",
      "publication": "In Proceedings of EMNLP, 2019",
      "year": 2019,
      "summary": "Numerical reasoning, such as addition, subtraction, sorting and counting is a critical skill in human's reading comprehension, which has not been well considered in existing machine reading comprehension (MRC) systems. To address this issue, we propose a numerical MRC model named as NumNet, which utilizes a numerically-aware graph neural network to consider the comparing information and performs numerical reasoning over numbers in the question and passage. Our system achieves an EM-score of 64.56% on the DROP dataset, outperforming all existing machine reading comprehension models by considering the numerical relations among numbers.",
      "standard_url": "http://arxiv.org/abs/1910.06701v1",
      "id": "2019-numnet:_machine_reading_comprehension_with_numerical_reasoning"
    },
    "111": {
      "enum": "111",
      "authors": "Rachel Rudinger, Jason Naradowsky, Brian Leonard, Benjamin Van Durme",
      "title": "Gender bias in coreference resolution",
      "publication": "arXiv preprint arXiv:1804.09301, 2018",
      "year": 2018,
      "summary": "We present an empirical study of gender bias in coreference resolution systems. We first introduce a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender. With these \"Winogender schemas,\" we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics.",
      "standard_url": "http://arxiv.org/abs/1804.09301v1",
      "id": "2018-gender_bias_in_coreference_resolution"
    },
    "112": {
      "enum": "112",
      "authors": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever",
      "title": "Improving language understanding by generative pre-training, 2018",
      "publication": null,
      "year": "2018",
      "summary": null,
      "standard_url": null,
      "id": "2018-improving_language_understanding_by_generative_pre-training_2018"
    },
    "113": {
      "enum": "113",
      "authors": "R.S. Ross",
      "title": "Guide for conducting risk assessments",
      "publication": "NIST Special Publication, 2012",
      "year": "2012",
      "summary": null,
      "standard_url": null,
      "id": "2012-guide_for_conducting_risk_assessments"
    },
    "114": {
      "enum": "114",
      "authors": "Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit",
      "title": "A constructive prediction of the generalization error across scales, 2019",
      "publication": null,
      "year": "2019",
      "summary": null,
      "standard_url": null,
      "id": "2019-a_constructive_prediction_of_the_generalization_error_across_scales_2019"
    },
    "115": {
      "enum": "115",
      "authors": "Adam Roberts, Colin Raffel, Noam Shazeer",
      "title": "How much knowledge can you pack into the parameters of a language model?",
      "publication": "arXiv preprint arXiv:2002.08910, 2020",
      "year": 2020,
      "summary": "It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales with model size and performs competitively with open-domain systems that explicitly retrieve answers from an external knowledge source when answering questions. To facilitate reproducibility and future work, we release our code and trained models at https://goo.gle/t5-cbqa.",
      "standard_url": "http://arxiv.org/abs/2002.08910v4",
      "id": "2020-how_much_knowledge_can_you_pack_into_the_parameters_of_a_language_model?"
    },
    "116": {
      "enum": "116",
      "authors": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer, 2019",
      "publication": null,
      "year": "2019",
      "summary": null,
      "standard_url": null,
      "id": "2019-exploring_the_limits_of_transfer_learning_with_a_unified_text-to-text_transformer_2019"
    },
    "117": {
      "enum": "117",
      "authors": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever",
      "title": "Language models are unsupervised multitask learners, 2019",
      "publication": null,
      "year": "2019",
      "summary": null,
      "standard_url": null,
      "id": "2019-language_models_are_unsupervised_multitask_learners_2019"
    },
    "118": {
      "enum": "118",
      "authors": "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi",
      "title": "Winogrande: An adversarial winograd schema challenge at scale, 2019",
      "publication": null,
      "year": "2019",
      "summary": null,
      "standard_url": null,
      "id": "2019-winogrande:_an_adversarial_winograd_schema_challenge_at_scale_2019"
    },
    "119": {
      "enum": "119",
      "authors": "Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris McGuffie, and Jasmine Wang",
      "title": "Release strategies and the social impacts of language models, 2019",
      "publication": null,
      "year": "2019",
      "summary": null,
      "standard_url": null,
      "id": "2019-release_strategies_and_the_social_impacts_of_language_models_2019"
    },
    "120": {
      "enum": "120",
      "authors": "Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, Nanyun Peng",
      "title": "The woman worked as a babysitter: On biases in language generation",
      "publication": "arXiv preprint arXiv:1909.01326, 2019",
      "year": 2019,
      "summary": "We present a systematic study of biases in natural language generation (NLG) by analyzing text generated from prompts that contain mentions of different demographic groups. In this work, we introduce the notion of the regard towards a demographic, use the varying levels of regard towards different demographics as a defining metric for bias in NLG, and analyze the extent to which sentiment scores are a relevant proxy metric for regard. To this end, we collect strategically-generated text from language models and manually annotate the text with both sentiment and regard scores. Additionally, we build an automatic regard classifier through transfer learning, so that we can analyze biases in unseen text. Together, these methods reveal the extent of the biased nature of language model generations. Our analysis provides a study of biases in NLG, bias metrics and correlated human judgments, and empirical evidence on the usefulness of our annotated dataset.",
      "standard_url": "http://arxiv.org/abs/1909.01326v2",
      "id": "2019-the_woman_worked_as_a_babysitter:_on_biases_in_language_generation"
    },
    "121": {
      "enum": "121",
      "authors": "Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf",
      "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "publication": "arXiv preprint arXiv:1910.01108, 2019",
      "year": 2019,
      "summary": "As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.",
      "standard_url": "http://arxiv.org/abs/1910.01108v4",
      "id": "2019-distilbert_a_distilled_version_of_bert:_smaller_faster_cheaper_and_lighter"
    },
    "122": {
      "enum": "122",
      "authors": "Roy Schwartz, Jesse Dodge, Noah A. Smith, Oren Etzioni",
      "title": "Green AI",
      "publication": "CoRR, abs/1907.10597, 2019",
      "year": 2019,
      "summary": "The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2]. These computations have a surprisingly large carbon footprint [38]. Ironically, deep learning was inspired by the human brain, which is remarkably energy efficient. Moreover, the financial cost of the computations can make it difficult for academics, students, and researchers, in particular those from emerging economies, to engage in deep learning research.\n  This position paper advocates a practical solution by making efficiency an evaluation criterion for research alongside accuracy and related measures. In addition, we propose reporting the financial cost or \"price tag\" of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods. Our goal is to make AI both greener and more inclusive---enabling any inspired undergraduate with a laptop to write high-quality research papers. Green AI is an emerging focus at the Allen Institute for AI.",
      "standard_url": "http://arxiv.org/abs/1907.10597v3",
      "id": "2019-green_ai"
    },
    "123": {
      "enum": "123",
      "authors": "Rico Sennrich, Barry Haddow, Alexandra Birch",
      "title": "Improving neural machine translation models with monolingual data",
      "publication": "arXiv preprint arXiv:1511.06709, 2015",
      "year": 2015,
      "summary": "Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Target-side monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task English<->German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task Turkish->English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English->German.",
      "standard_url": "http://arxiv.org/abs/1511.06709v4",
      "id": "2015-improving_neural_machine_translation_models_with_monolingual_data"
    },
    "124": {
      "enum": "124",
      "authors": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean",
      "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
      "publication": "arXiv preprint arXiv:1701.06538, 2017",
      "year": 2017,
      "summary": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",
      "standard_url": "http://arxiv.org/abs/1701.06538v1",
      "id": "2017-outrageously_large_neural_networks:_the_sparsely-gated_mixture-of-experts_layer"
    },
    "125": {
      "enum": "125",
      "authors": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro",
      "title": "Megatron-lm: Training multi-billion parameter language models using model parallelism, 2019",
      "publication": null,
      "year": "2019",
      "summary": null,
      "standard_url": null,
      "id": "2019-megatron-lm:_training_multi-billion_parameter_language_models_using_model_parallelism_2019"
    },
    "126": {
      "enum": "126",
      "authors": "Timo Schick, Hinrich Schütze",
      "title": "Exploiting cloze questions for few-shot text classification and natural language inference",
      "publication": "arXiv preprint arXiv:2001.07676, 2020",
      "year": 2020,
      "summary": "Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with \"task descriptions\" in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.",
      "standard_url": "http://arxiv.org/abs/2001.07676v3",
      "id": "2020-exploiting_cloze_questions_for_few-shot_text_classification_and_natural_language_inference"
    },
    "127": {
      "enum": "127",
      "authors": "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu",
      "title": "MASS: Masked sequence to sequence pre-training for language generation",
      "publication": "arXiv preprint arXiv:1905.02450, 2019",
      "year": 2019,
      "summary": "Pre-training and fine-tuning, e.g., BERT, have achieved great success in language understanding by transferring knowledge from rich-resource pre-training task to the low/zero-resource downstream tasks. Inspired by the success of BERT, we propose MAsked Sequence to Sequence pre-training (MASS) for the encoder-decoder based language generation tasks. MASS adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence: its encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and its decoder tries to predict this masked fragment. In this way, MASS can jointly train the encoder and decoder to develop the capability of representation extraction and language modeling. By further fine-tuning on a variety of zero/low-resource language generation tasks, including neural machine translation, text summarization and conversational response generation (3 tasks and totally 8 datasets), MASS achieves significant improvements over the baselines without pre-training or with other pre-training methods. Specially, we achieve the state-of-the-art accuracy (37.5 in terms of BLEU score) on the unsupervised English-French translation, even beating the early attention-based supervised model.",
      "standard_url": "http://arxiv.org/abs/1905.02450v5",
      "id": "2019-mass:_masked_sequence_to_sequence_pre-training_for_language_generation"
    },
    "128": {
      "enum": "128",
      "authors": "Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, Pieter Abbeel",
      "title": "Domain randomization for transferring deep neural networks from simulation to the real world",
      "publication": "In 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 23–30. IEEE, 2017",
      "year": 2017,
      "summary": "Bridging the 'reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to $1.5$cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.",
      "standard_url": "http://arxiv.org/abs/1703.06907v1",
      "id": "2017-domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
    },
    "129": {
      "enum": "129",
      "authors": "Peter D. Turney, Michael L. Littman",
      "title": "Corpus-based learning of analogies and semantic relations",
      "publication": "CoRR, abs/cs/0508103, 2005",
      "year": 2005,
      "summary": "We present an algorithm for learning from unlabeled text, based on the Vector Space Model (VSM) of information retrieval, that can solve verbal analogy questions of the kind found in the SAT college entrance exam. A verbal analogy has the form A:B::C:D, meaning \"A is to B as C is to D\"; for example, mason:stone::carpenter:wood. SAT analogy questions provide a word pair, A:B, and the problem is to select the most analogous word pair, C:D, from a set of five choices. The VSM algorithm correctly answers 47% of a collection of 374 college-level analogy questions (random guessing would yield 20% correct; the average college-bound senior high school student answers about 57% correctly). We motivate this research by applying it to a difficult problem in natural language processing, determining semantic relations in noun-modifier pairs. The problem is to classify a noun-modifier pair, such as \"laser printer\", according to the semantic relation between the noun (printer) and the modifier (laser). We use a supervised nearest-neighbour algorithm that assigns a class to a given noun-modifier pair by finding the most analogous noun-modifier pair in the training data. With 30 classes of semantic relations, on a collection of 600 labeled noun-modifier pairs, the learning algorithm attains an F value of 26.5% (random guessing: 3.3%). With 5 classes of semantic relations, the F value is 43.2% (random: 20%). The performance is state-of-the-art for both verbal analogies and noun-modifier relations.",
      "standard_url": "http://arxiv.org/abs/cs/0508103v1",
      "id": "2005-corpus-based_learning_of_analogies_and_semantic_relations"
    },
    "130": {
      "enum": "130",
      "authors": "Trieu H. Trinh, Quoc V. Le",
      "title": "A simple method for commonsense reasoning",
      "publication": "arXiv preprint arXiv:1806.02847, 2018",
      "year": 2018,
      "summary": "Commonsense reasoning is a long-standing challenge for deep learning. For example, it is difficult to use neural networks to tackle the Winograd Schema dataset (Levesque et al., 2011). In this paper, we present a simple method for commonsense reasoning with neural networks, using unsupervised learning. Key to our method is the use of language models, trained on a massive amount of unlabled data, to score multiple choice questions posed by commonsense reasoning tests. On both Pronoun Disambiguation and Winograd Schema challenges, our models outperform previous state-of-the-art methods by a large margin, without using expensive annotated knowledge bases or hand-engineered features. We train an array of large RNN language models that operate at word or character level on LM-1-Billion, CommonCrawl, SQuAD, Gutenberg Books, and a customized corpus for this task and show that diversity of training data plays an important role in test performance. Further analysis also shows that our system successfully discovers important features of the context that decide the correct answer, indicating a good grasp of commonsense knowledge.",
      "standard_url": "http://arxiv.org/abs/1806.02847v2",
      "id": "2018-a_simple_method_for_commonsense_reasoning"
    },
    "131": {
      "enum": "131",
      "authors": "Peter D. Turney, Michael L. Littman, Jeffrey Bigham, Victor Shnayder",
      "title": "Combining independent modules to solve multiple-choice synonym and analogy problems",
      "publication": "CoRR, cs.CL/0309035, 2003",
      "year": 2003,
      "summary": "Existing statistical approaches to natural language problems are very coarse approximations to the true complexity of language processing. As such, no single technique will be best for all problem instances. Many researchers are examining ensemble methods that combine the output of successful, separately developed modules to create more accurate solutions. This paper examines three merging rules for combining probability distributions: the well known mixture rule, the logarithmic rule, and a novel product rule. These rules were applied with state-of-the-art results to two problems commonly used to assess human mastery of lexical semantics -- synonym questions and analogy questions. All three merging rules result in ensembles that are more accurate than any of their component modules. The differences among the three rules are not statistically significant, but it is suggestive that the popular mixture rule is not the best rule for either of the two problems.",
      "standard_url": "http://arxiv.org/abs/cs/0309035v1",
      "id": "2003-combining_independent_modules_to_solve_multiple-choice_synonym_and_analogy_problems"
    },
    "132": {
      "enum": "132",
      "authors": "Project Turing",
      "title": "Microsoft research blog, Feb 2020",
      "publication": null,
      "year": "2020",
      "summary": null,
      "standard_url": null,
      "id": "2020-microsoft_research_blog_feb_2020"
    },
    "133": {
      "enum": "133",
      "authors": "Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan Wierstra",
      "title": "Matching Networks for One Shot Learning",
      "publication": "In Advances in neural information processing systems, pages 3630–3638, 2016",
      "year": 2016,
      "summary": "Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.",
      "standard_url": "http://arxiv.org/abs/1606.04080v2",
      "id": "2016-matching_networks_for_one_shot_learning"
    },
    "134": {
      "enum": "134",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin",
      "title": "Attention is all you need",
      "publication": "In Advances in neural information processing systems, 2017",
      "year": 2017,
      "summary": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
      "standard_url": "http://arxiv.org/abs/1706.03762v7",
      "id": "2017-attention_is_all_you_need"
    },
    "135": {
      "enum": "135",
      "authors": "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R. Bowman",
      "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
      "publication": "In Advances in Neural Information Processing Systems, pages 3261–3275, 2019",
      "year": 2019,
      "summary": "In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.",
      "standard_url": "http://arxiv.org/abs/1905.00537v3",
      "id": "2019-superglue:_a_stickier_benchmark_for_general-purpose_language_understanding_systems"
    },
    "136": {
      "enum": "136",
      "authors": "Yiren Wang, Yingce Xia, Tianyu He, Fei Tian, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu",
      "title": "Multi-agent dual learning",
      "publication": "ICLR 2019, 2018",
      "year": "2018",
      "summary": null,
      "standard_url": null,
      "id": "2018-multi-agent_dual_learning"
    },
    "137": {
      "enum": "137",
      "authors": "Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le",
      "title": "Unsupervised data augmentation for consistency training, 2019",
      "publication": null,
      "year": "2019",
      "summary": null,
      "standard_url": null,
      "id": "2019-unsupervised_data_augmentation_for_consistency_training_2019"
    },
    "138": {
      "enum": "138",
      "authors": "Dani Yogatama, Cyprien de Masson d'Autume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, Phil Blunsom",
      "title": "Learning and evaluating general linguistic intelligence",
      "publication": "arXiv preprint arXiv:1901.11373, 2019",
      "year": 2019,
      "summary": "We define general linguistic intelligence as the ability to reuse previously acquired knowledge about a language's lexicon, syntax, semantics, and pragmatic conventions to adapt to new tasks quickly. Using this definition, we analyze state-of-the-art natural language understanding models and conduct an extensive empirical investigation to evaluate them against these criteria through a series of experiments that assess the task-independence of the knowledge being acquired by the learning process. In addition to task performance, we propose a new evaluation metric based on an online encoding of the test data that quantifies how quickly an existing agent (model) learns a new task. Our results show that while the field has made impressive progress in terms of model architectures that generalize to many tasks, these models still require a lot of in-domain training examples (e.g., for fine tuning, training task-specific modules), and are prone to catastrophic forgetting. Moreover, we find that far from solving general tasks (e.g., document question answering), our models are overfitting to the quirks of particular datasets (e.g., SQuAD). We discuss missing components and conjecture on how to make progress toward general linguistic intelligence.",
      "standard_url": "http://arxiv.org/abs/1901.11373v1",
      "id": "2019-learning_and_evaluating_general_linguistic_intelligence"
    },
    "139": {
      "enum": "139",
      "authors": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le",
      "title": "XLNet: Generalized autoregressive pretraining for language understanding",
      "publication": "arXiv preprint arXiv:1906.08237, 2019",
      "year": 2019,
      "summary": "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.",
      "standard_url": "http://arxiv.org/abs/1906.08237v2",
      "id": "2019-xlnet:_generalized_autoregressive_pretraining_for_language_understanding"
    },
    "140": {
      "enum": "140",
      "authors": "Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi",
      "title": "Hellaswag: Can a machine really finish your sentence?",
      "publication": "arXiv preprint arXiv:1905.07830, 2019",
      "year": 2019,
      "summary": "Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as \"A woman sits at a piano,\" a machine must select the most likely followup: \"She sets her fingers on the keys.\" With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference?\n  In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (>95% accuracy), state-of-the-art models struggle (<48%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical 'Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models.\n  Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.",
      "standard_url": "http://arxiv.org/abs/1905.07830v1",
      "id": "2019-hellaswag:_can_a_machine_really_finish_your_sentence?"
    },
    "141": {
      "enum": "141",
      "authors": "Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi",
      "title": "Defending against neural fake news",
      "publication": "arXiv preprint arXiv:1905.12616, 2019",
      "year": 2019,
      "summary": "Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news.\n  Modern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation.\n  Developing robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.",
      "standard_url": "http://arxiv.org/abs/1905.12616v3",
      "id": "2019-defending_against_neural_fake_news"
    },
    "142": {
      "enum": "142",
      "authors": "Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, Benjamin Van Durme",
      "title": "ReCoRD: Bridging the gap between human and machine commonsense reading comprehension",
      "publication": "arXiv preprint arXiv:1810.12885, 2018",
      "year": 2018,
      "summary": "We present a large-scale dataset, ReCoRD, for machine reading comprehension requiring commonsense reasoning. Experiments on this dataset demonstrate that the performance of state-of-the-art MRC systems fall far behind human performance. ReCoRD represents a challenge for future research to bridge the gap between human and machine commonsense reading comprehension. ReCoRD is available at http://nlp.jhu.edu/record.",
      "standard_url": "http://arxiv.org/abs/1810.12885v1",
      "id": "2018-record:_bridging_the_gap_between_human_and_machine_commonsense_reading_comprehension"
    },
    "143": {
      "enum": "143",
      "authors": "Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving",
      "title": "Fine-tuning language models from human preferences, 2019",
      "publication": null,
      "year": "2019",
      "summary": null,
      "standard_url": null,
      "id": "2019-fine-tuning_language_models_from_human_preferences_2019"
    },
    "144": {
      "enum": "144",
      "authors": "Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, Geoffrey Irving",
      "title": "Fine-tuning language models from human preferences",
      "publication": "ArXiv, abs/1909.08593, 2019",
      "year": 2019,
      "summary": "Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.",
      "standard_url": "http://arxiv.org/abs/1909.08593v2",
      "id": "2019-fine-tuning_language_models_from_human_preferences"
    }
  },
  "authors": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",
  "summary": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
  "standard_url": "http://arxiv.org/abs/2005.14165v4",
  "id": "2020-language_models_are_few-shot_learners"
}