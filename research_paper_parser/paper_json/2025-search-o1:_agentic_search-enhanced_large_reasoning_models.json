{
  "name": "Search-o1",
  "year": 2025,
  "url": "https://arxiv.org/html/2501.05366v1",
  "title": "Search-o1: Agentic Search-Enhanced Large Reasoning Models",
  "sections": {
    "S1": "1 Introduction",
    "S2": "2 Related Work",
    "S3": "3 Methodology",
    "S4": "4 Experiments",
    "S5": "5 Conclusion"
  },
  "citations": [
    {
      "section_id": "S1",
      "cite_enum": "22",
      "cite_id": "22",
      "sentence": "Recently emerged large reasoning models (LRMs), exemplified by OpenAI’s o1[22], Qwen-QwQ[54]and DeepSeek-R1[7], employ large-scale reinforcement learning foster impressive long-sequence stepwise reasoning capabilities, offering promising solutions to complex reasoning problems[46,31,59,84,73,74,67]"
    },
    {
      "section_id": "S1",
      "cite_enum": "54",
      "cite_id": "54",
      "sentence": "Recently emerged large reasoning models (LRMs), exemplified by OpenAI’s o1[22], Qwen-QwQ[54]and DeepSeek-R1[7], employ large-scale reinforcement learning foster impressive long-sequence stepwise reasoning capabilities, offering promising solutions to complex reasoning problems[46,31,59,84,73,74,67]"
    },
    {
      "section_id": "S1",
      "cite_enum": "7",
      "cite_id": "7",
      "sentence": "Recently emerged large reasoning models (LRMs), exemplified by OpenAI’s o1[22], Qwen-QwQ[54]and DeepSeek-R1[7], employ large-scale reinforcement learning foster impressive long-sequence stepwise reasoning capabilities, offering promising solutions to complex reasoning problems[46,31,59,84,73,74,67]"
    },
    {
      "section_id": "S1",
      "cite_enum": "46",
      "cite_id": "46",
      "sentence": "Recently emerged large reasoning models (LRMs), exemplified by OpenAI’s o1[22], Qwen-QwQ[54]and DeepSeek-R1[7], employ large-scale reinforcement learning foster impressive long-sequence stepwise reasoning capabilities, offering promising solutions to complex reasoning problems[46,31,59,84,73,74,67]"
    },
    {
      "section_id": "S1",
      "cite_enum": "31",
      "cite_id": "31",
      "sentence": "Recently emerged large reasoning models (LRMs), exemplified by OpenAI’s o1[22], Qwen-QwQ[54]and DeepSeek-R1[7], employ large-scale reinforcement learning foster impressive long-sequence stepwise reasoning capabilities, offering promising solutions to complex reasoning problems[46,31,59,84,73,74,67]"
    },
    {
      "section_id": "S1",
      "cite_enum": "59",
      "cite_id": "59",
      "sentence": "Recently emerged large reasoning models (LRMs), exemplified by OpenAI’s o1[22], Qwen-QwQ[54]and DeepSeek-R1[7], employ large-scale reinforcement learning foster impressive long-sequence stepwise reasoning capabilities, offering promising solutions to complex reasoning problems[46,31,59,84,73,74,67]"
    },
    {
      "section_id": "S1",
      "cite_enum": "84",
      "cite_id": "84",
      "sentence": "Recently emerged large reasoning models (LRMs), exemplified by OpenAI’s o1[22], Qwen-QwQ[54]and DeepSeek-R1[7], employ large-scale reinforcement learning foster impressive long-sequence stepwise reasoning capabilities, offering promising solutions to complex reasoning problems[46,31,59,84,73,74,67]"
    },
    {
      "section_id": "S1",
      "cite_enum": "73",
      "cite_id": "73",
      "sentence": "Recently emerged large reasoning models (LRMs), exemplified by OpenAI’s o1[22], Qwen-QwQ[54]and DeepSeek-R1[7], employ large-scale reinforcement learning foster impressive long-sequence stepwise reasoning capabilities, offering promising solutions to complex reasoning problems[46,31,59,84,73,74,67]"
    },
    {
      "section_id": "S1",
      "cite_enum": "74",
      "cite_id": "74",
      "sentence": "Recently emerged large reasoning models (LRMs), exemplified by OpenAI’s o1[22], Qwen-QwQ[54]and DeepSeek-R1[7], employ large-scale reinforcement learning foster impressive long-sequence stepwise reasoning capabilities, offering promising solutions to complex reasoning problems[46,31,59,84,73,74,67]"
    },
    {
      "section_id": "S1",
      "cite_enum": "67",
      "cite_id": "67",
      "sentence": "Recently emerged large reasoning models (LRMs), exemplified by OpenAI’s o1[22], Qwen-QwQ[54]and DeepSeek-R1[7], employ large-scale reinforcement learning foster impressive long-sequence stepwise reasoning capabilities, offering promising solutions to complex reasoning problems[46,31,59,84,73,74,67]"
    },
    {
      "section_id": "S1",
      "cite_enum": "49",
      "cite_id": "49",
      "sentence": "This advancement has inspired a series of foundational efforts aimed at exploring and reproducing o1-like reasoning patterns, to broaden their application to a wider range of foundational models[49,19,77,80,71,25,45]"
    },
    {
      "section_id": "S1",
      "cite_enum": "19",
      "cite_id": "19",
      "sentence": "This advancement has inspired a series of foundational efforts aimed at exploring and reproducing o1-like reasoning patterns, to broaden their application to a wider range of foundational models[49,19,77,80,71,25,45]"
    },
    {
      "section_id": "S1",
      "cite_enum": "77",
      "cite_id": "77",
      "sentence": "This advancement has inspired a series of foundational efforts aimed at exploring and reproducing o1-like reasoning patterns, to broaden their application to a wider range of foundational models[49,19,77,80,71,25,45]"
    },
    {
      "section_id": "S1",
      "cite_enum": "80",
      "cite_id": "80",
      "sentence": "This advancement has inspired a series of foundational efforts aimed at exploring and reproducing o1-like reasoning patterns, to broaden their application to a wider range of foundational models[49,19,77,80,71,25,45]"
    },
    {
      "section_id": "S1",
      "cite_enum": "71",
      "cite_id": "71",
      "sentence": "This advancement has inspired a series of foundational efforts aimed at exploring and reproducing o1-like reasoning patterns, to broaden their application to a wider range of foundational models[49,19,77,80,71,25,45]"
    },
    {
      "section_id": "S1",
      "cite_enum": "25",
      "cite_id": "25",
      "sentence": "This advancement has inspired a series of foundational efforts aimed at exploring and reproducing o1-like reasoning patterns, to broaden their application to a wider range of foundational models[49,19,77,80,71,25,45]"
    },
    {
      "section_id": "S1",
      "cite_enum": "45",
      "cite_id": "45",
      "sentence": "This advancement has inspired a series of foundational efforts aimed at exploring and reproducing o1-like reasoning patterns, to broaden their application to a wider range of foundational models[49,19,77,80,71,25,45]"
    },
    {
      "section_id": "S1",
      "cite_enum": "6",
      "cite_id": "6",
      "sentence": "It is noteworthy that o1-like reasoning patterns guide LRMs to engage in a slower thinking process[6,61]by implicitly breaking down complex problems, generating a long internal reasoning chain and then discovering suitable solutions step by step"
    },
    {
      "section_id": "S1",
      "cite_enum": "61",
      "cite_id": "61",
      "sentence": "It is noteworthy that o1-like reasoning patterns guide LRMs to engage in a slower thinking process[6,61]by implicitly breaking down complex problems, generating a long internal reasoning chain and then discovering suitable solutions step by step"
    },
    {
      "section_id": "S1",
      "cite_enum": "4",
      "cite_id": "4",
      "sentence": "While this characteristic enhances logical coherence and interpretability of reasoning, an extended chain of thought may cause overthinking[4]and increased risks of knowledge insufficiency[60,51,2], where any knowledge gap can propagate errors and disrupt the entire reasoning chain[79,40,44,41]"
    },
    {
      "section_id": "S1",
      "cite_enum": "60",
      "cite_id": "60",
      "sentence": "While this characteristic enhances logical coherence and interpretability of reasoning, an extended chain of thought may cause overthinking[4]and increased risks of knowledge insufficiency[60,51,2], where any knowledge gap can propagate errors and disrupt the entire reasoning chain[79,40,44,41]"
    },
    {
      "section_id": "S1",
      "cite_enum": "51",
      "cite_id": "51",
      "sentence": "While this characteristic enhances logical coherence and interpretability of reasoning, an extended chain of thought may cause overthinking[4]and increased risks of knowledge insufficiency[60,51,2], where any knowledge gap can propagate errors and disrupt the entire reasoning chain[79,40,44,41]"
    },
    {
      "section_id": "S1",
      "cite_enum": "2",
      "cite_id": "2",
      "sentence": "While this characteristic enhances logical coherence and interpretability of reasoning, an extended chain of thought may cause overthinking[4]and increased risks of knowledge insufficiency[60,51,2], where any knowledge gap can propagate errors and disrupt the entire reasoning chain[79,40,44,41]"
    },
    {
      "section_id": "S1",
      "cite_enum": "79",
      "cite_id": "79",
      "sentence": "While this characteristic enhances logical coherence and interpretability of reasoning, an extended chain of thought may cause overthinking[4]and increased risks of knowledge insufficiency[60,51,2], where any knowledge gap can propagate errors and disrupt the entire reasoning chain[79,40,44,41]"
    },
    {
      "section_id": "S1",
      "cite_enum": "40",
      "cite_id": "40",
      "sentence": "While this characteristic enhances logical coherence and interpretability of reasoning, an extended chain of thought may cause overthinking[4]and increased risks of knowledge insufficiency[60,51,2], where any knowledge gap can propagate errors and disrupt the entire reasoning chain[79,40,44,41]"
    },
    {
      "section_id": "S1",
      "cite_enum": "44",
      "cite_id": "44",
      "sentence": "While this characteristic enhances logical coherence and interpretability of reasoning, an extended chain of thought may cause overthinking[4]and increased risks of knowledge insufficiency[60,51,2], where any knowledge gap can propagate errors and disrupt the entire reasoning chain[79,40,44,41]"
    },
    {
      "section_id": "S1",
      "cite_enum": "41",
      "cite_id": "41",
      "sentence": "While this characteristic enhances logical coherence and interpretability of reasoning, an extended chain of thought may cause overthinking[4]and increased risks of knowledge insufficiency[60,51,2], where any knowledge gap can propagate errors and disrupt the entire reasoning chain[79,40,44,41]"
    },
    {
      "section_id": "S1",
      "cite_enum": "63",
      "cite_id": "63",
      "sentence": "Notably, the high specialization of these problems also complicates manual reasoning verification, often incurring significant costs[63]"
    },
    {
      "section_id": "S1",
      "cite_enum": "83",
      "cite_id": "83",
      "sentence": "This finding aligns with human intuition, as standard RAG retrieves relevant knowledge only once in a problem-oriented manner, while the knowledge required for each step in complex reasoning scenarios is often varied and diverse[83,41,11]"
    },
    {
      "section_id": "S1",
      "cite_enum": "41",
      "cite_id": "41",
      "sentence": "This finding aligns with human intuition, as standard RAG retrieves relevant knowledge only once in a problem-oriented manner, while the knowledge required for each step in complex reasoning scenarios is often varied and diverse[83,41,11]"
    },
    {
      "section_id": "S1",
      "cite_enum": "11",
      "cite_id": "11",
      "sentence": "This finding aligns with human intuition, as standard RAG retrieves relevant knowledge only once in a problem-oriented manner, while the knowledge required for each step in complex reasoning scenarios is often varied and diverse[83,41,11]"
    },
    {
      "section_id": "S1",
      "cite_enum": "62",
      "cite_id": "62",
      "sentence": "Retrieved documents are often lengthy and contain redundant information, directly inputting them into LRMs may disrupt the original coherence of reasoning and even introduce noise[62,72,26]"
    },
    {
      "section_id": "S1",
      "cite_enum": "72",
      "cite_id": "72",
      "sentence": "Retrieved documents are often lengthy and contain redundant information, directly inputting them into LRMs may disrupt the original coherence of reasoning and even introduce noise[62,72,26]"
    },
    {
      "section_id": "S1",
      "cite_enum": "26",
      "cite_id": "26",
      "sentence": "Retrieved documents are often lengthy and contain redundant information, directly inputting them into LRMs may disrupt the original coherence of reasoning and even introduce noise[62,72,26]"
    },
    {
      "section_id": "S1",
      "cite_enum": "39",
      "cite_id": "39",
      "sentence": "This focus has resulted in a degree of catastrophic forgetting in their general capabilities[39,10], ultimately limiting their long-context understanding of retrieved documents"
    },
    {
      "section_id": "S1",
      "cite_enum": "10",
      "cite_id": "10",
      "sentence": "This focus has resulted in a degree of catastrophic forgetting in their general capabilities[39,10], ultimately limiting their long-context understanding of retrieved documents"
    },
    {
      "section_id": "S2",
      "cite_enum": "17",
      "cite_id": "17",
      "sentence": "Large reasoning models focus on enhancing performance at test time by utilizing extended reasoning steps, contrasting with traditional large pre-trained models that achieve scalability during training by increasing model size or expanding training data[17,66,50,85,76]"
    },
    {
      "section_id": "S2",
      "cite_enum": "66",
      "cite_id": "66",
      "sentence": "Large reasoning models focus on enhancing performance at test time by utilizing extended reasoning steps, contrasting with traditional large pre-trained models that achieve scalability during training by increasing model size or expanding training data[17,66,50,85,76]"
    },
    {
      "section_id": "S2",
      "cite_enum": "50",
      "cite_id": "50",
      "sentence": "Large reasoning models focus on enhancing performance at test time by utilizing extended reasoning steps, contrasting with traditional large pre-trained models that achieve scalability during training by increasing model size or expanding training data[17,66,50,85,76]"
    },
    {
      "section_id": "S2",
      "cite_enum": "85",
      "cite_id": "85",
      "sentence": "Large reasoning models focus on enhancing performance at test time by utilizing extended reasoning steps, contrasting with traditional large pre-trained models that achieve scalability during training by increasing model size or expanding training data[17,66,50,85,76]"
    },
    {
      "section_id": "S2",
      "cite_enum": "76",
      "cite_id": "76",
      "sentence": "Large reasoning models focus on enhancing performance at test time by utilizing extended reasoning steps, contrasting with traditional large pre-trained models that achieve scalability during training by increasing model size or expanding training data[17,66,50,85,76]"
    },
    {
      "section_id": "S2",
      "cite_enum": "15",
      "cite_id": "15",
      "sentence": "Studies have shown that test-time scaling can improve the reasoning abilities of smaller models on complex tasks[15,75]"
    },
    {
      "section_id": "S2",
      "cite_enum": "75",
      "cite_id": "75",
      "sentence": "Studies have shown that test-time scaling can improve the reasoning abilities of smaller models on complex tasks[15,75]"
    },
    {
      "section_id": "S2",
      "cite_enum": "22",
      "cite_id": "22",
      "sentence": "Recently, models like OpenAI-o1[22], Qwen-QwQ[54]and DeepSeek-R1[7]explicitly demonstrate chain-of-thought reasoning[59], mimicking human problem-solving approaches in domains such as mathematics, coding, and so on"
    },
    {
      "section_id": "S2",
      "cite_enum": "54",
      "cite_id": "54",
      "sentence": "Recently, models like OpenAI-o1[22], Qwen-QwQ[54]and DeepSeek-R1[7]explicitly demonstrate chain-of-thought reasoning[59], mimicking human problem-solving approaches in domains such as mathematics, coding, and so on"
    },
    {
      "section_id": "S2",
      "cite_enum": "7",
      "cite_id": "7",
      "sentence": "Large reasoning models focus on enhancing performance at test time by utilizing extended reasoning steps, contrasting with traditional large pre-trained models that achieve scalability during training by increasing model size or expanding training data[17,66,50,85,76]"
    },
    {
      "section_id": "S2",
      "cite_enum": "59",
      "cite_id": "59",
      "sentence": "Recently, models like OpenAI-o1[22], Qwen-QwQ[54]and DeepSeek-R1[7]explicitly demonstrate chain-of-thought reasoning[59], mimicking human problem-solving approaches in domains such as mathematics, coding, and so on"
    },
    {
      "section_id": "S2",
      "cite_enum": "25",
      "cite_id": "25",
      "sentence": "Some methods combine policy and reward models with Monte Carlo Tree Search (MCTS)[25], though this does not internalize reasoning within the model"
    },
    {
      "section_id": "S2",
      "cite_enum": "49",
      "cite_id": "49",
      "sentence": "Other studies incorporate deliberate errors in reasoning paths during training to partially internalize these abilities[49,71]"
    },
    {
      "section_id": "S2",
      "cite_enum": "71",
      "cite_id": "71",
      "sentence": "Other studies incorporate deliberate errors in reasoning paths during training to partially internalize these abilities[49,71]"
    },
    {
      "section_id": "S2",
      "cite_enum": "45",
      "cite_id": "45",
      "sentence": "Additionally, distilling training data has been shown to enhance models’ o1-like reasoning skills[45]"
    },
    {
      "section_id": "S2",
      "cite_enum": "65",
      "cite_id": "65",
      "sentence": "The o1-like reasoning paradigm has demonstrated strong performance across diverse domains, including vision-language reasoning[65,11,48,69], code generation[81,32], healthcare[3], and machine translation[57]"
    },
    {
      "section_id": "S2",
      "cite_enum": "11",
      "cite_id": "11",
      "sentence": "The o1-like reasoning paradigm has demonstrated strong performance across diverse domains, including vision-language reasoning[65,11,48,69], code generation[81,32], healthcare[3], and machine translation[57]"
    },
    {
      "section_id": "S2",
      "cite_enum": "48",
      "cite_id": "48",
      "sentence": "The o1-like reasoning paradigm has demonstrated strong performance across diverse domains, including vision-language reasoning[65,11,48,69], code generation[81,32], healthcare[3], and machine translation[57]"
    },
    {
      "section_id": "S2",
      "cite_enum": "69",
      "cite_id": "69",
      "sentence": "The o1-like reasoning paradigm has demonstrated strong performance across diverse domains, including vision-language reasoning[65,11,48,69], code generation[81,32], healthcare[3], and machine translation[57]"
    },
    {
      "section_id": "S2",
      "cite_enum": "81",
      "cite_id": "81",
      "sentence": "The o1-like reasoning paradigm has demonstrated strong performance across diverse domains, including vision-language reasoning[65,11,48,69], code generation[81,32], healthcare[3], and machine translation[57]"
    },
    {
      "section_id": "S2",
      "cite_enum": "32",
      "cite_id": "32",
      "sentence": "The o1-like reasoning paradigm has demonstrated strong performance across diverse domains, including vision-language reasoning[65,11,48,69], code generation[81,32], healthcare[3], and machine translation[57]"
    },
    {
      "section_id": "S2",
      "cite_enum": "3",
      "cite_id": "3",
      "sentence": "The o1-like reasoning paradigm has demonstrated strong performance across diverse domains, including vision-language reasoning[65,11,48,69], code generation[81,32], healthcare[3], and machine translation[57]"
    },
    {
      "section_id": "S2",
      "cite_enum": "57",
      "cite_id": "57",
      "sentence": "The o1-like reasoning paradigm has demonstrated strong performance across diverse domains, including vision-language reasoning[65,11,48,69], code generation[81,32], healthcare[3], and machine translation[57]"
    },
    {
      "section_id": "S2",
      "cite_enum": "30",
      "cite_id": "30",
      "sentence": "Retrieval-augmented generation (RAG) introduces retrieval mechanisms to address the limitations of static parameters in generative models, allowing access to external knowledge to solve more complex problems[30,82,35,86]"
    },
    {
      "section_id": "S2",
      "cite_enum": "82",
      "cite_id": "82",
      "sentence": "Retrieval-augmented generation (RAG) introduces retrieval mechanisms to address the limitations of static parameters in generative models, allowing access to external knowledge to solve more complex problems[30,82,35,86]"
    },
    {
      "section_id": "S2",
      "cite_enum": "35",
      "cite_id": "35",
      "sentence": "Retrieval-augmented generation (RAG) introduces retrieval mechanisms to address the limitations of static parameters in generative models, allowing access to external knowledge to solve more complex problems[30,82,35,86]"
    },
    {
      "section_id": "S2",
      "cite_enum": "86",
      "cite_id": "86",
      "sentence": "Retrieval-augmented generation (RAG) introduces retrieval mechanisms to address the limitations of static parameters in generative models, allowing access to external knowledge to solve more complex problems[30,82,35,86]"
    },
    {
      "section_id": "S2",
      "cite_enum": "53",
      "cite_id": "53",
      "sentence": "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on"
    },
    {
      "section_id": "S2",
      "cite_enum": "43",
      "cite_id": "43",
      "sentence": "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on"
    },
    {
      "section_id": "S2",
      "cite_enum": "58",
      "cite_id": "58",
      "sentence": "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on"
    },
    {
      "section_id": "S2",
      "cite_enum": "64",
      "cite_id": "64",
      "sentence": "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on"
    },
    {
      "section_id": "S2",
      "cite_enum": "42",
      "cite_id": "42",
      "sentence": "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on"
    },
    {
      "section_id": "S2",
      "cite_enum": "12",
      "cite_id": "12",
      "sentence": "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on"
    },
    {
      "section_id": "S2",
      "cite_enum": "24",
      "cite_id": "24",
      "sentence": "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on"
    },
    {
      "section_id": "S2",
      "cite_enum": "27",
      "cite_id": "27",
      "sentence": "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on"
    },
    {
      "section_id": "S2",
      "cite_enum": "88",
      "cite_id": "88",
      "sentence": "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on"
    },
    {
      "section_id": "S2",
      "cite_enum": "9",
      "cite_id": "9",
      "sentence": "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on"
    },
    {
      "section_id": "S2",
      "cite_enum": "8",
      "cite_id": "8",
      "sentence": "Retrieval-augmented generation (RAG) introduces retrieval mechanisms to address the limitations of static parameters in generative models, allowing access to external knowledge to solve more complex problems[30,82,35,86]"
    },
    {
      "section_id": "S2",
      "cite_enum": "87",
      "cite_id": "87",
      "sentence": "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on"
    },
    {
      "section_id": "S2",
      "cite_enum": "1",
      "cite_id": "1",
      "sentence": "Advanced research in this field enhances the RAG system from multiple aspects, including the necessity of retrieval[53], pre-processing of queries[43,58], retrieved documents compressing[64], denoising[42,12], refining[24,27,88], instruction following[9,8,87]and so on"
    },
    {
      "section_id": "S2",
      "cite_enum": "36",
      "cite_id": "36",
      "sentence": "Furthermore, some studies have explored end-to-end model training to implement RAG systems[1,36,33,34]and knowledge-graph-based RAG systems[14,37]"
    },
    {
      "section_id": "S2",
      "cite_enum": "33",
      "cite_id": "33",
      "sentence": "Furthermore, some studies have explored end-to-end model training to implement RAG systems[1,36,33,34]and knowledge-graph-based RAG systems[14,37]"
    },
    {
      "section_id": "S2",
      "cite_enum": "34",
      "cite_id": "34",
      "sentence": "Furthermore, some studies have explored end-to-end model training to implement RAG systems[1,36,33,34]and knowledge-graph-based RAG systems[14,37]"
    },
    {
      "section_id": "S2",
      "cite_enum": "14",
      "cite_id": "14",
      "sentence": "Furthermore, some studies have explored end-to-end model training to implement RAG systems[1,36,33,34]and knowledge-graph-based RAG systems[14,37]"
    },
    {
      "section_id": "S2",
      "cite_enum": "37",
      "cite_id": "37",
      "sentence": "Furthermore, some studies have explored end-to-end model training to implement RAG systems[1,36,33,34]and knowledge-graph-based RAG systems[14,37]"
    },
    {
      "section_id": "S2",
      "cite_enum": "5",
      "cite_id": "5",
      "sentence": "Recently, agentic RAG systems empower models to autonomously determine when and what knowledge to retrieve as needed, showcasing enhanced planning and problem-solving capabilities[5,56,70]"
    },
    {
      "section_id": "S2",
      "cite_enum": "56",
      "cite_id": "56",
      "sentence": "Recently, agentic RAG systems empower models to autonomously determine when and what knowledge to retrieve as needed, showcasing enhanced planning and problem-solving capabilities[5,56,70]"
    },
    {
      "section_id": "S2",
      "cite_enum": "70",
      "cite_id": "70",
      "sentence": "Recently, agentic RAG systems empower models to autonomously determine when and what knowledge to retrieve as needed, showcasing enhanced planning and problem-solving capabilities[5,56,70]"
    },
    {
      "section_id": "S2",
      "cite_enum": "78",
      "cite_id": "78",
      "sentence": "There is also research combining agent-based systems with MCTS to optimize complex workflows, leveraging retrievers and other tools to accomplish tasks[78]"
    },
    {
      "section_id": "S4",
      "cite_enum": "52",
      "cite_id": "52",
      "sentence": "Challenging reasoning tasks:(1)GPQA[52]is a PhD-level science multiple-choice QA dataset"
    },
    {
      "section_id": "S4",
      "cite_enum": "38",
      "cite_id": "38",
      "sentence": "(2)Math benchmarksincludeMATH500[38],AMC2023111https://huggingface"
    },
    {
      "section_id": "S4",
      "cite_enum": "16",
      "cite_id": "16",
      "sentence": "MATH500 consists of 500 questions from the MATH test set[16]"
    },
    {
      "section_id": "S4",
      "cite_enum": "23",
      "cite_id": "23",
      "sentence": "(2)Math benchmarksincludeMATH500[38],AMC2023111https://huggingface"
    },
    {
      "section_id": "S4",
      "cite_enum": "29",
      "cite_id": "29",
      "sentence": "Open-domain QA tasks:(1)Single-hop QA datasets:Natural Questions (NQ)[29]contains questions from real Google search queries with answers from Wikipedia articles"
    },
    {
      "section_id": "S4",
      "cite_enum": "28",
      "cite_id": "28",
      "sentence": "TriviaQA[28]is a large-scale dataset with questions from trivia websites and competitions, featuring complex entity relationships"
    },
    {
      "section_id": "S4",
      "cite_enum": "68",
      "cite_id": "68",
      "sentence": "(2)Multi-hop QA datasets:HotpotQA[68]is the first large-scale dataset requiring reasoning across multiple Wikipedia paragraphs"
    },
    {
      "section_id": "S4",
      "cite_enum": "18",
      "cite_id": "18",
      "sentence": "2WikiMultihopQA (2WIKI)[18]provides explicit reasoning paths for multi-hop questions"
    },
    {
      "section_id": "S4",
      "cite_enum": "55",
      "cite_id": "55",
      "sentence": "MuSiQue[55]features 2-4 hop questions built from five existing single-hop datasets"
    },
    {
      "section_id": "S4",
      "cite_enum": "47",
      "cite_id": "47",
      "sentence": "Bamboogle[47]collects complex questions that Google answers incorrectly to evaluate models’ compositional reasoning across various domains"
    },
    {
      "section_id": "S4",
      "cite_enum": "50",
      "cite_id": "50",
      "sentence": "5-32B-Instruct[50], Qwen2"
    },
    {
      "section_id": "S4",
      "cite_enum": "20",
      "cite_id": "20",
      "sentence": "5-Coder-32B-Instruct[20], QwQ-32B-Preview[54], Qwen2"
    },
    {
      "section_id": "S4",
      "cite_enum": "54",
      "cite_id": "54",
      "sentence": "5-Coder-32B-Instruct[20], QwQ-32B-Preview[54], Qwen2"
    },
    {
      "section_id": "S4",
      "cite_enum": "50",
      "cite_id": "50",
      "sentence": "5-32B-Instruct[50], Qwen2"
    },
    {
      "section_id": "S4",
      "cite_enum": "13",
      "cite_id": "13",
      "sentence": "3-70B-Instruct[13]"
    },
    {
      "section_id": "S4",
      "cite_enum": "7",
      "cite_id": "7",
      "sentence": "5-72B-Instruct[50], and Llama3"
    },
    {
      "section_id": "S4",
      "cite_enum": "21",
      "cite_id": "21",
      "sentence": "Closed-source non-proprietary models include DeepSeek-R1-Lite-Preview[7], OpenAI GPT-4o[21], and o1-preview[22]"
    },
    {
      "section_id": "S4",
      "cite_enum": "22",
      "cite_id": "22",
      "sentence": "Closed-source non-proprietary models include DeepSeek-R1-Lite-Preview[7], OpenAI GPT-4o[21], and o1-preview[22]"
    },
    {
      "section_id": "S4",
      "cite_enum": "70",
      "cite_id": "70",
      "sentence": "To manage the length of retrieved documents, inspired by ReAct[70], we first retrieve the top-10 snippets during reasoning"
    },
    {
      "section_id": "S4",
      "cite_enum": "54",
      "cite_id": "54",
      "sentence": "For the backbone large reasoning model in Search-o1, we utilize the open-sourced QwQ-32B-Preview[54]"
    },
    {
      "section_id": "S4",
      "cite_enum": "52",
      "cite_id": "52",
      "sentence": "For all retrieval-based methods, following[52], we apply a back-off strategy where, when a final answer is not provided, we use the result from direct reasoning"
    },
    {
      "section_id": "S4",
      "cite_enum": "59",
      "cite_id": "59",
      "sentence": "For baseline models not specifically trained for o1-like reasoning, we apply Chain-of-Thought (CoT)[59]prompting to perform reasoning before generating answers"
    },
    {
      "section_id": "S4",
      "cite_enum": "52",
      "cite_id": "52",
      "sentence": "Performance comparison with human experts on the GPQA extended set[52]"
    }
  ],
  "preview": "<div class=\"ltx_para\" id=\"S1.p1\">\n<p class=\"ltx_p\" id=\"S1.p1.1\">Recently emerged large reasoning models (LRMs), exemplified by OpenAI’s o1 , Qwen-QwQ  and DeepSeek-R1 , employ large-scale reinforcement learning foster impressive long-sequence stepwise reasoning capabilities, offering promising solutions to complex reasoning problems . This advancement has inspired a series of foundational efforts aimed at exploring and reproducing o1-like reasoning patterns, to broaden their application to a wider range of foundational models .</p>\n</div><div class=\"ltx_para\" id=\"S1.p2\">\n<p class=\"ltx_p\" id=\"S1.p2.1\">It is noteworthy that o1-like reasoning patterns guide LRMs to engage in a slower thinking process  by implicitly breaking down complex problems, generating a long internal reasoning chain and then discovering suitable solutions step by step. While this characteristic enhances logical coherence and interpretability of reasoning, an extended chain of thought may cause overthinking  and increased risks of knowledge insufficiency , where any knowledge gap can propagate errors and disrupt the entire reasoning chain .</p>\n</div><div class=\"ltx_para\" id=\"S1.p3\">\n<p class=\"ltx_p\" id=\"S1.p3.1\">To address this limitation, we conduct preliminary experiments to assess the frequency of uncertain words decoded by the LRMs due to knowledge gaps. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2501.05366v1#S1.F1\" title=\"Figure 1 ‣ 1 Introduction ‣ Search-o1: Agentic Search-Enhanced Large Reasoning Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the extended thinking process leads LRM to frequently decode numerous uncertain terms in challenging reasoning problems, with “<span class=\"ltx_text ltx_font_italic\" id=\"S1.p3.1.1\">perhaps</span>” averaging over 30 occurrences in each reasoning process. Notably, the high specialization of these problems also complicates manual reasoning verification, often incurring significant costs . Consequently, automating the supplementation of knowledge required for the o1-like reasoning process has become a significant challenge, limiting the progress of LRMs in achieving universally trustworthy reasoning.</p>\n</div><div class=\"ltx_para\" id=\"S1.p4\">\n<p class=\"ltx_p\" id=\"S1.p4.1\">To shed light on this topic, our core motivation is to enhance the LRMs with o1-like reasoning pattern through autonomous retrieval. We propose <span class=\"ltx_text ltx_font_bold\" id=\"S1.p4.1.1\">Search-o1</span>, which integrates the reasoning process of LRMs with two core components: an agentic retrieval-augmented generation (RAG) mechanism and a knowledge refinement module. This design aims to enable LRMs to incorporate the agentic search workflow into the reasoning process, retrieving external knowledge on demand to support step-wise reasoning while preserving coherence throughout.</p>\n</div><div class=\"ltx_para\" id=\"S1.p5\">\n<p class=\"ltx_p\" id=\"S1.p5.1\">Specifically, our results in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2501.05366v1#S1.F1\" title=\"Figure 1 ‣ 1 Introduction ‣ Search-o1: Agentic Search-Enhanced Large Reasoning Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reveal that traditional problem-oriented RAG techniques do not effectively address the knowledge gaps compared to direct reasoning (Standard RAG vs. Direct Reasoning). This finding aligns with human intuition, as standard RAG retrieves relevant knowledge only once in a problem-oriented manner, while the knowledge required for each step in complex reasoning scenarios is often varied and diverse . Unlike them, Search-o1 employs an agentic RAG technique that guides the model to actively decode search queries when facing knowledge shortages, thereby triggering the retrieval mechanism to obtain relevant external knowledge. Owing to the benefits of this design, our retrieval mechanism can be triggered and iterated multiple times within a single reasoning session to fulfill the knowledge needs of various reasoning steps.</p>\n</div><div class=\"ltx_para\" id=\"S1.p6\">\n<p class=\"ltx_p\" id=\"S1.p6.1\">To effectively integrate retrieved knowledge into the LRM’s reasoning process, we further identify two key challenges when directly incorporating retrieved documents into the reasoning chain during practical experiments:\n(1) <span class=\"ltx_text ltx_font_bold\" id=\"S1.p6.1.1\">Redundant Information in Retrieved Documents.</span> Retrieved documents are often lengthy and contain redundant information, directly inputting them into LRMs may disrupt the original coherence of reasoning and even introduce noise .\n(2) <span class=\"ltx_text ltx_font_bold\" id=\"S1.p6.1.2\">Limited Ability to Understand Long Documents.</span> Most LRMs have been specifically aligned for complex reasoning tasks during the pre-training and fine-tuning stages. This focus has resulted in a degree of catastrophic forgetting in their general capabilities , ultimately limiting their long-context understanding of retrieved documents.</p>\n</div><div class=\"ltx_para\" id=\"S1.p7\">\n<p class=\"ltx_p\" id=\"S1.p7.1\">To address these challenges, we introduce the Reason-in-Documents module, which operates independently from the main reasoning chain. This module first conducts a thorough analysis of retrieved documents based on both the current search query and previous reasoning steps, and then produces refined information that seamlessly integrates with the prior reasoning chain.</p>\n</div><div class=\"ltx_para\" id=\"S1.p8\">\n<p class=\"ltx_p\" id=\"S1.p8.1\">In summary, our contributions are as follows:</p>\n</div><div class=\"ltx_para\" id=\"S1.p9\">\n<ul class=\"ltx_itemize\" id=\"S1.I1\">\n<li class=\"ltx_item\" id=\"S1.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S1.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"S1.I1.i1.p1.1\">We propose Search-o1, the first framework that integrates the agentic search workflow into the o1-like reasoning process of LRM for achieving autonomous knowledge supplementation.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S1.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"S1.I1.i2.p1.1\">To effectively integrate external knowledge during reasoning, Search-o1 combines the reasoning process with an agentic RAG mechanism and a knowledge refinement module. This design enables the LRM to retrieve external knowledge on demand, seamlessly incorporating it into the reasoning chain while maintaining the original logical flow.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S1.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"S1.I1.i3.p1.1\">With five complex reasoning domains and six open-domain QA benchmarks, we demonstrate that Search-o1 achieves remarkable performance in the reasoning field while maintaining substantial improvements in the general knowledge. Further quantitative analysis confirms its efficiency and scalability, offering practical guidance for trustworthy reasoning in LRMs.</p>\n</div>\n</li>\n</ul>\n</div><div class=\"ltx_para\" id=\"S1.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"S1.I1.i1.p1.1\">We propose Search-o1, the first framework that integrates the agentic search workflow into the o1-like reasoning process of LRM for achieving autonomous knowledge supplementation.</p>\n</div><div class=\"ltx_para\" id=\"S1.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"S1.I1.i2.p1.1\">To effectively integrate external knowledge during reasoning, Search-o1 combines the reasoning process with an agentic RAG mechanism and a knowledge refinement module. This design enables the LRM to retrieve external knowledge on demand, seamlessly incorporating it into the reasoning chain while maintaining the original logical flow.</p>\n</div><div class=\"ltx_para\" id=\"S1.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"S1.I1.i3.p1.1\">With five complex reasoning domains and six open-domain QA benchmarks, we demonstrate that Search-o1 achieves remarkable performance in the reasoning field while maintaining substantial improvements in the general knowledge. Further quantitative analysis confirms its efficiency and scalability, offering practical guidance for trustworthy reasoning in LRMs.</p>\n</div>",
  "references": {
    "1": {
      "enum": "1",
      "authors": "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi",
      "title": "Self-rag: Learning to retrieve, generate, and critique through self-reflection",
      "publication": "arXiv preprint arXiv:2310.11511, 2023",
      "year": 2023,
      "summary": "Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.",
      "standard_url": "http://arxiv.org/abs/2310.11511v1",
      "id": "2023-self-rag:_learning_to_retrieve_generate_and_critique_through_self-reflection"
    },
    "2": {
      "enum": "2",
      "authors": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",
      "title": "Language models are few-shot learners",
      "publication": "In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020",
      "year": 2020,
      "summary": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
      "standard_url": "http://arxiv.org/abs/2005.14165v4",
      "id": "2020-language_models_are_few-shot_learners"
    },
    "3": {
      "enum": "3",
      "authors": "Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, Benyou Wang",
      "title": "Huatuogpt-o1, towards medical complex reasoning with llms",
      "publication": "arXiv preprint arXiv:2412.18925, 2024",
      "year": 2024,
      "summary": "The breakthrough of OpenAI o1 highlights the potential of enhancing reasoning to improve LLM. Yet, most research in reasoning has focused on mathematical tasks, leaving domains like medicine underexplored. The medical domain, though distinct from mathematics, also demands robust reasoning to provide reliable answers, given the high standards of healthcare. However, verifying medical reasoning is challenging, unlike those in mathematics. To address this, we propose verifiable medical problems with a medical verifier to check the correctness of model outputs. This verifiable nature enables advancements in medical reasoning through a two-stage approach: (1) using the verifier to guide the search for a complex reasoning trajectory for fine-tuning LLMs, (2) applying reinforcement learning (RL) with verifier-based rewards to enhance complex reasoning further. Finally, we introduce HuatuoGPT-o1, a medical LLM capable of complex reasoning, which outperforms general and medical-specific baselines using only 40K verifiable problems. Experiments show complex reasoning improves medical problem-solving and benefits more from RL. We hope our approach inspires advancements in reasoning across medical and other specialized domains.",
      "standard_url": "http://arxiv.org/abs/2412.18925v1",
      "id": "2024-huatuogpt-o1_towards_medical_complex_reasoning_with_llms"
    },
    "4": {
      "enum": "4",
      "authors": "Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu",
      "title": "Do not think that much for 2+3=? on the overthinking of o1-like llms, 2024",
      "publication": null,
      "year": "2024",
      "summary": null,
      "standard_url": null,
      "id": "2024-do_not_think_that_much_for_2+3=?_on_the_overthinking_of_o1-like_llms_2024"
    },
    "5": {
      "enum": "5",
      "authors": "Zehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu, Wenwei Zhang, Kai Chen, Feng Zhao",
      "title": "Mindsearch: Mimicking human minds elicits deep ai searcher",
      "publication": "arXiv preprint arXiv:2407.20183, 2024",
      "year": 2024,
      "summary": "Information seeking and integration is a complex cognitive task that consumes enormous time and effort. Inspired by the remarkable progress of Large Language Models, recent works attempt to solve this task by combining LLMs and search engines. However, these methods still obtain unsatisfying performance due to three challenges: (1) complex requests often cannot be accurately and completely retrieved by the search engine once (2) corresponding information to be integrated is spread over multiple web pages along with massive noise, and (3) a large number of web pages with long contents may quickly exceed the maximum context length of LLMs. Inspired by the cognitive process when humans solve these problems, we introduce MindSearch to mimic the human minds in web information seeking and integration, which can be instantiated by a simple yet effective LLM-based multi-agent framework. The WebPlanner models the human mind of multi-step information seeking as a dynamic graph construction process: it decomposes the user query into atomic sub-questions as nodes in the graph and progressively extends the graph based on the search result from WebSearcher. Tasked with each sub-question, WebSearcher performs hierarchical information retrieval with search engines and collects valuable information for WebPlanner. The multi-agent design of MindSearch enables the whole framework to seek and integrate information parallelly from larger-scale (e.g., more than 300) web pages in 3 minutes, which is worth 3 hours of human effort. MindSearch demonstrates significant improvement in the response quality in terms of depth and breadth, on both close-set and open-set QA problems. Besides, responses from MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Web and Perplexity.ai applications, which implies that MindSearch can already deliver a competitive solution to the proprietary AI search engine.",
      "standard_url": "http://arxiv.org/abs/2407.20183v2",
      "id": "2024-mindsearch:_mimicking_human_minds_elicits_deep_ai_searcher"
    },
    "6": {
      "enum": "6",
      "authors": "Grady Booch, Francesco Fabiano, Lior Horesh, Kiran Kate, Jon Lenchner, Nick Linck, Andrea Loreggia, Keerthiram Murugesan, Nicholas Mattei, Francesca Rossi, Biplav Srivastava",
      "title": "Thinking, fast and slow",
      "publication": "2017",
      "year": 2020,
      "summary": "This paper proposes a research direction to advance AI which draws inspiration from cognitive theories of human decision making. The premise is that if we gain insights about the causes of some human capabilities that are still lacking in AI (for instance, adaptability, generalizability, common sense, and causal reasoning), we may obtain similar capabilities in an AI system by embedding these causal components. We hope that the high-level description of our vision included in this paper, as well as the several research questions that we propose to consider, can stimulate the AI research community to define, try and evaluate new methodologies, frameworks, and evaluation metrics, in the spirit of achieving a better understanding of both human and machine intelligence.",
      "standard_url": "http://arxiv.org/abs/2010.06002v2",
      "id": "2020-thinking_fast_and_slow"
    },
    "7": {
      "enum": "7",
      "authors": "DeepSeek-AI",
      "title": "Deepseek-r1-lite-preview is now live: unleashing supercharged reasoning power!, November 2024",
      "publication": null,
      "year": "2024",
      "summary": null,
      "standard_url": null,
      "id": "2024-deepseek-r1-lite-preview_is_now_live:_unleashing_supercharged_reasoning_power!_november_2024"
    },
    "8": {
      "enum": "8",
      "authors": "Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, Jingren Zhou",
      "title": "Self-play with execution feedback: Improving instruction-following capabilities of large language models",
      "publication": "CoRR, abs/2406.13542, 2024",
      "year": 2024,
      "summary": "One core capability of large language models (LLMs) is to follow natural language instructions. However, the issue of automatically constructing high-quality training data to enhance the complex instruction-following abilities of LLMs without manual annotation remains unresolved. In this paper, we introduce AutoIF, the first scalable and reliable method for automatically generating instruction-following training data. AutoIF transforms the validation of instruction-following data quality into code verification, requiring LLMs to generate instructions, the corresponding code to check the correctness of the instruction responses, and unit test samples to verify the code's correctness. Then, execution feedback-based rejection sampling can generate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) training. AutoIF achieves significant improvements across three training algorithms, SFT, Offline DPO, and Online DPO, when applied to the top open-source LLMs, Qwen2 and LLaMA3, in self-alignment and strong-to-weak distillation settings. Our code is publicly available at https://github.com/QwenLM/AutoIF.",
      "standard_url": "http://arxiv.org/abs/2406.13542v3",
      "id": "2024-self-play_with_execution_feedback:_improving_instruction-following_capabilities_of_large_language_models"
    },
    "9": {
      "enum": "9",
      "authors": "Guanting Dong, Xiaoshuai Song, Yutao Zhu, Runqi Qiao, Zhicheng Dou, Ji-Rong Wen",
      "title": "Toward general instruction-following alignment for retrieval-augmented generation",
      "publication": "CoRR, abs/2410.09584, 2024",
      "year": 2024,
      "summary": "Following natural instructions is crucial for the effective application of Retrieval-Augmented Generation (RAG) systems. Despite recent advancements in Large Language Models (LLMs), research on assessing and improving instruction-following (IF) alignment within the RAG domain remains limited. To address this issue, we propose VIF-RAG, the first automated, scalable, and verifiable synthetic pipeline for instruction-following alignment in RAG systems. We start by manually crafting a minimal set of atomic instructions (<100) and developing combination rules to synthesize and verify complex instructions for a seed set. We then use supervised models for instruction rewriting while simultaneously generating code to automate the verification of instruction quality via a Python executor. Finally, we integrate these instructions with extensive RAG and general data samples, scaling up to a high-quality VIF-RAG-QA dataset (>100k) through automated processes. To further bridge the gap in instruction-following auto-evaluation for RAG systems, we introduce FollowRAG Benchmark, which includes approximately 3K test samples, covering 22 categories of general instruction constraints and four knowledge-intensive QA datasets. Due to its robust pipeline design, FollowRAG can seamlessly integrate with different RAG benchmarks. Using FollowRAG and eight widely-used IF and foundational abilities benchmarks for LLMs, we demonstrate that VIF-RAG markedly enhances LLM performance across a broad range of general instruction constraints while effectively leveraging its capabilities in RAG scenarios. Further analysis offers practical insights for achieving IF alignment in RAG systems. Our code and datasets are released at https://FollowRAG.github.io.",
      "standard_url": "http://arxiv.org/abs/2410.09584v1",
      "id": "2024-toward_general_instruction-following_alignment_for_retrieval-augmented_generation"
    },
    "10": {
      "enum": "10",
      "authors": "Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, Jingren Zhou",
      "title": "How abilities in large language models are affected by supervised fine-tuning data composition",
      "publication": "In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 177–198. Association for Computational Linguistics, 2024",
      "year": 2023,
      "summary": "Large language models (LLMs) with enormous pre-training tokens and parameters emerge diverse abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). While the open-source community has explored ad-hoc SFT for enhancing individual capabilities, proprietary LLMs exhibit versatility across various skills. Therefore, understanding the facilitation of multiple abilities via SFT is paramount. In this study, we specifically focuses on the interplay of data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. We propose four intriguing research questions to explore the association between model performance and various factors including data amount, composition ratio, model size and SFT strategies. Our experiments reveal that distinct capabilities scale differently and larger models generally show superior performance with same amount of data. Mathematical reasoning and code generation consistently improve with increasing data amount, whereas general abilities plateau after roughly a thousand samples. Moreover, we observe data composition appears to enhance various abilities under limited data conditions, yet can lead to performance conflicts when data is plentiful. Our findings also suggest the amount of composition data influences performance more than the composition ratio. In analysis of SFT strategies, we find that sequentially learning multiple skills risks catastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT) strategy offers a promising solution to learn multiple abilities with different scaling patterns.",
      "standard_url": "http://arxiv.org/abs/2310.05492v4",
      "id": "2023-how_abilities_in_large_language_models_are_affected_by_supervised_fine-tuning_data_composition"
    },
    "11": {
      "enum": "11",
      "authors": "Guanting Dong, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, Ji-Rong Wen",
      "title": "Progressive multimodal reasoning via active retrieval",
      "publication": "arXiv preprint arXiv:2412.14835, 2024",
      "year": 2024,
      "summary": "Multi-step multimodal reasoning tasks pose significant challenges for multimodal large language models (MLLMs), and finding effective ways to enhance their performance in such scenarios remains an unresolved issue. In this paper, we propose AR-MCTS, a universal framework designed to progressively improve the reasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo Tree Search (MCTS). Our approach begins with the development of a unified retrieval module that retrieves key supporting insights for solving complex reasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in automated multimodal reasoning verification, we employ the MCTS algorithm combined with an active retrieval mechanism, which enables the automatic generation of step-wise annotations. This strategy dynamically retrieves key insights for each reasoning step, moving beyond traditional beam search sampling to improve the diversity and reliability of the reasoning space. Additionally, we introduce a process reward model that aligns progressively to support the automatic verification of multimodal reasoning tasks. Experimental results across three complex multimodal reasoning benchmarks confirm the effectiveness of the AR-MCTS framework in enhancing the performance of various multimodal models. Further analysis demonstrates that AR-MCTS can optimize sampling diversity and accuracy, yielding reliable multimodal reasoning.",
      "standard_url": "http://arxiv.org/abs/2412.14835v1",
      "id": "2024-progressive_multimodal_reasoning_via_active_retrieval"
    },
    "12": {
      "enum": "12",
      "authors": "Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, Ji-Rong Wen",
      "title": "Understand what LLM needs: Dual preference alignment for retrieval-augmented generation",
      "publication": "CoRR, abs/2406.18676, 2024",
      "year": 2024,
      "summary": "Retrieval-augmented generation (RAG) has demonstrated effectiveness in mitigating the hallucination problem of large language models (LLMs). However, the difficulty of aligning the retriever with the diverse LLMs' knowledge preferences inevitably poses an inevitable challenge in developing a reliable RAG system. To address this issue, we propose DPA-RAG, a universal framework designed to align diverse knowledge preferences within RAG systems. Specifically, we initially introduce a preference knowledge construction pipline and incorporate five novel query augmentation strategies to alleviate preference data scarcity. Based on preference data, DPA-RAG accomplishes both external and internal preference alignment: 1) It jointly integrate pair-wise, point-wise, and contrastive preference alignment abilities into the reranker, achieving external preference alignment among RAG components. 2) It further introduces a pre-aligned stage before vanilla Supervised Fine-tuning (SFT), enabling LLMs to implicitly capture knowledge aligned with their reasoning preferences, achieving LLMs' internal alignment. Experimental results across four knowledge-intensive QA datasets demonstrate that DPA-RAG outperforms all baselines and seamlessly integrates both black-box and open-sourced LLM readers. Further qualitative analysis and discussions also provide empirical guidance for achieving reliable RAG systems. Our code is publicly available at https://github.com/dongguanting/DPA-RAG.",
      "standard_url": "http://arxiv.org/abs/2406.18676v2",
      "id": "2024-understand_what_llm_needs:_dual_preference_alignment_for_retrieval-augmented_generation"
    },
    "13": {
      "enum": "13",
      "authors": "Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, Zhiyu Ma",
      "title": "The llama 3 herd of models",
      "publication": "arXiv preprint arXiv:2407.21783, 2024",
      "year": 2024,
      "summary": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
      "standard_url": "http://arxiv.org/abs/2407.21783v3",
      "id": "2024-the_llama_3_herd_of_models"
    },
    "14": {
      "enum": "14",
      "authors": "Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson",
      "title": "From local to global: A graph rag approach to query-focused summarization, 2024",
      "publication": null,
      "year": "2024",
      "summary": null,
      "standard_url": null,
      "id": "2024-from_local_to_global:_a_graph_rag_approach_to_query-focused_summarization_2024"
    },
    "15": {
      "enum": "15",
      "authors": "Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, Liwei Wang",
      "title": "Towards revealing the mystery behind chain of thought: a theoretical perspective",
      "publication": "Advances in Neural Information Processing Systems, 36, 2024",
      "year": 2023,
      "summary": "Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the expressivity of LLMs with CoT in solving fundamental mathematical and decision-making problems. By using circuit complexity theory, we first give impossibility results showing that bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a commonly used math language format. Moreover, we show LLMs with CoT can handle a general class of decision-making problems known as Dynamic Programming, thus justifying its power in tackling complex real-world tasks. Finally, an extensive set of experiments show that, while Transformers always fail to directly predict the answers, they can consistently learn to generate correct solutions step-by-step given sufficient CoT demonstrations.",
      "standard_url": "http://arxiv.org/abs/2305.15408v5",
      "id": "2023-towards_revealing_the_mystery_behind_chain_of_thought:_a_theoretical_perspective"
    },
    "16": {
      "enum": "16",
      "authors": "Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt",
      "title": "Measuring mathematical problem solving with the MATH dataset",
      "publication": "In Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021",
      "year": 2021,
      "summary": "Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",
      "standard_url": "http://arxiv.org/abs/2103.03874v2",
      "id": "2021-measuring_mathematical_problem_solving_with_the_math_dataset"
    },
    "17": {
      "enum": "17",
      "authors": "Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, Sam McCandlish",
      "title": "Scaling laws for autoregressive generative modeling",
      "publication": "arXiv preprint arXiv:2010.14701, 2020",
      "year": 2020,
      "summary": "We identify empirical scaling laws for the cross-entropy loss in four domains: generative image modeling, video modeling, multimodal image$\\leftrightarrow$text models, and mathematical problem solving. In all cases autoregressive Transformers smoothly improve in performance as model size and compute budgets increase, following a power-law plus constant scaling law. The optimal model size also depends on the compute budget through a power-law, with exponents that are nearly universal across all data domains.\n  The cross-entropy loss has an information theoretic interpretation as $S($True$) + D_{\\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws suggest a prediction for both the true data distribution's entropy and the KL divergence between the true and model distributions. With this interpretation, billion-parameter Transformers are nearly perfect models of the YFCC100M image distribution downsampled to an $8\\times 8$ resolution, and we can forecast the model size needed to achieve any given reducible loss (ie $D_{\\mathrm{KL}}$) in nats/image for other resolutions.\n  We find a number of additional scaling laws in specific domains: (a) we identify a scaling relation for the mutual information between captions and images in multimodal models, and show how to answer the question \"Is a picture worth a thousand words?\"; (b) in the case of mathematical problem solving, we identify scaling laws for model performance when extrapolating beyond the training distribution; (c) we finetune generative image models for ImageNet classification and find smooth scaling of the classification loss and error rate, even as the generative loss levels off. Taken together, these results strengthen the case that scaling laws have important implications for neural network performance, including on downstream tasks.",
      "standard_url": "http://arxiv.org/abs/2010.14701v2",
      "id": "2020-scaling_laws_for_autoregressive_generative_modeling"
    },
    "18": {
      "enum": "18",
      "authors": "Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, Akiko Aizawa",
      "title": "Constructing A multi-hop QA dataset for comprehensive evaluation of reasoning steps",
      "publication": "In Donia Scott, Núria Bel, and Chengqing Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pages 6609–6625. International Committee on Computational Linguistics, 2020",
      "year": 2020,
      "summary": "A multi-hop question answering (QA) dataset aims to test reasoning and inference skills by requiring a model to read multiple paragraphs to answer a given question. However, current datasets do not provide a complete explanation for the reasoning process from the question to the answer. Further, previous studies revealed that many examples in existing multi-hop datasets do not require multi-hop reasoning to answer a question. In this study, we present a new multi-hop QA dataset, called 2WikiMultiHopQA, which uses structured and unstructured data. In our dataset, we introduce the evidence information containing a reasoning path for multi-hop questions. The evidence information has two benefits: (i) providing a comprehensive explanation for predictions and (ii) evaluating the reasoning skills of a model. We carefully design a pipeline and a set of templates when generating a question-answer pair that guarantees the multi-hop steps and the quality of the questions. We also exploit the structured format in Wikidata and use logical rules to create questions that are natural but still require multi-hop reasoning. Through experiments, we demonstrate that our dataset is challenging for multi-hop models and it ensures that multi-hop reasoning is required.",
      "standard_url": "http://arxiv.org/abs/2011.01060v2",
      "id": "2020-constructing_a_multi-hop_qa_dataset_for_comprehensive_evaluation_of_reasoning_steps"
    },
    "19": {
      "enum": "19",
      "authors": "Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, Pengfei Liu",
      "title": "O1 replication journey–part 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson?",
      "publication": "arXiv preprint arXiv:2411.16489, 2024",
      "year": 2024,
      "summary": "This paper presents a critical examination of current approaches to replicating OpenAI's O1 model capabilities, with particular focus on the widespread but often undisclosed use of knowledge distillation techniques. While our previous work explored the fundamental technical path to O1 replication, this study reveals how simple distillation from O1's API, combined with supervised fine-tuning, can achieve superior performance on complex mathematical reasoning tasks. Through extensive experiments, we show that a base model fine-tuned on simply tens of thousands of samples O1-distilled long-thought chains outperforms O1-preview on the American Invitational Mathematics Examination (AIME) with minimal technical complexity. Moreover, our investigation extends beyond mathematical reasoning to explore the generalization capabilities of O1-distilled models across diverse tasks: hallucination, safety and open-domain QA. Notably, despite training only on mathematical problem-solving data, our models demonstrated strong generalization to open-ended QA tasks and became significantly less susceptible to sycophancy after fine-tuning. We deliberately make this finding public to promote transparency in AI research and to challenge the current trend of obscured technical claims in the field. Our work includes: (1) A detailed technical exposition of the distillation process and its effectiveness, (2) A comprehensive benchmark framework for evaluating and categorizing O1 replication attempts based on their technical transparency and reproducibility, (3) A critical discussion of the limitations and potential risks of over-relying on distillation approaches, our analysis culminates in a crucial bitter lesson: while the pursuit of more capable AI systems is important, the development of researchers grounded in first-principles thinking is paramount.",
      "standard_url": "http://arxiv.org/abs/2411.16489v1",
      "id": "2024-o1_replication_journey–part_2:_surpassing_o1-preview_through_simple_distillation_big_progress_or_bitter_lesson?"
    },
    "20": {
      "enum": "20",
      "authors": "Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, Junyang Lin",
      "title": "Qwen2.5-coder technical report",
      "publication": "CoRR, abs/2409.12186, 2024",
      "year": 2024,
      "summary": "In this report, we introduce the Qwen2.5-Coder series, a significant upgrade from its predecessor, CodeQwen1.5. This series includes six models: Qwen2.5-Coder-(0.5B/1.5B/3B/7B/14B/32B). As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning, scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder demonstrates impressive code generation capabilities while retaining general and math skills. These models have been evaluated on a wide range of code-related tasks, achieving state-of-the-art (SOTA) performance across more than 10 benchmarks, including code generation, completion, reasoning, and repair, consistently outperforming larger models of the same model size. We believe that the release of the Qwen2.5-Coder series will advance research in code intelligence and, with its permissive licensing, support wider adoption by developers in real-world applications.",
      "standard_url": "http://arxiv.org/abs/2409.12186v3",
      "id": "2024-qwen2.5-coder_technical_report"
    },
    "21": {
      "enum": "21",
      "authors": "OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Mądry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian O'Connell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, Yury Malkov",
      "title": "Gpt-4o system card",
      "publication": "arXiv preprint arXiv:2410.21276, 2024",
      "year": 2024,
      "summary": "GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network. GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time in conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50\\% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models. In line with our commitment to building AI safely and consistent with our voluntary commitments to the White House, we are sharing the GPT-4o System Card, which includes our Preparedness Framework evaluations. In this System Card, we provide a detailed look at GPT-4o's capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures we've implemented to ensure the model is safe and aligned. We also include third-party assessments on dangerous capabilities, as well as discussion of potential societal impacts of GPT-4o's text and vision capabilities.",
      "standard_url": "http://arxiv.org/abs/2410.21276v1",
      "id": "2024-gpt-4o_system_card"
    },
    "22": {
      "enum": "22",
      "authors": "OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quiñonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, Zhuohan Li",
      "title": "Openai o1 system card",
      "publication": "arXiv preprint arXiv:2412.16720, 2024",
      "year": 2024,
      "summary": "The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning capabilities provide new avenues for improving the safety and robustness of our models. In particular, our models can reason about our safety policies in context when responding to potentially unsafe prompts, through deliberative alignment. This leads to state-of-the-art performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. Training models to incorporate a chain of thought before answering has the potential to unlock substantial benefits, while also increasing potential risks that stem from heightened intelligence. Our results underscore the need for building robust alignment methods, extensively stress-testing their efficacy, and maintaining meticulous risk management protocols. This report outlines the safety work carried out for the OpenAI o1 and OpenAI o1-mini models, including safety evaluations, external red teaming, and Preparedness Framework evaluations.",
      "standard_url": "http://arxiv.org/abs/2412.16720v1",
      "id": "2024-openai_o1_system_card"
    },
    "23": {
      "enum": "23",
      "authors": "Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, Ion Stoica",
      "title": "Livecodebench: Holistic and contamination free evaluation of large language models for code",
      "publication": "CoRR, abs/2403.07974, 2024",
      "year": 2024,
      "summary": "Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and May 2024. We have evaluated 18 base LLMs and 34 instruction-tuned LLMs on LiveCodeBench. We present empirical findings on contamination, holistic performance comparisons, potential overfitting in existing benchmarks as well as individual model comparisons. We will release all prompts and model completions for further community analysis, along with a general toolkit for adding new scenarios and model",
      "standard_url": "http://arxiv.org/abs/2403.07974v2",
      "id": "2024-livecodebench:_holistic_and_contamination_free_evaluation_of_large_language_models_for_code"
    },
    "24": {
      "enum": "24",
      "authors": "Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu",
      "title": "Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression",
      "publication": "arXiv preprint arXiv:2310.06839, 2023",
      "year": 2023,
      "summary": "In long context scenarios, large language models (LLMs) face three main challenges: higher computational cost, performance reduction, and position bias. Research indicates that LLM performance hinges on the density and position of key information in the input prompt. Inspired by these findings, we propose LongLLMLingua for prompt compression towards improving LLMs' perception of the key information to simultaneously address the three challenges. Our extensive evaluation across various long context scenarios demonstrates that LongLLMLingua not only enhances performance but also significantly reduces costs and latency. For instance, in the NaturalQuestions benchmark, LongLLMLingua boosts performance by up to 21.4% with around 4x fewer tokens in GPT-3.5-Turbo, leading to substantial cost savings. It achieves a 94.0% cost reduction in the LooGLE benchmark. Moreover, when compressing prompts of about 10k tokens at ratios of 2x-6x, LongLLMLingua can accelerate end-to-end latency by 1.4x-2.6x. Our code is available at https://aka.ms/LongLLMLingua.",
      "standard_url": "http://arxiv.org/abs/2310.06839v2",
      "id": "2023-longllmlingua:_accelerating_and_enhancing_llms_in_long_context_scenarios_via_prompt_compression"
    },
    "25": {
      "enum": "25",
      "authors": "Jinhao Jiang, Zhipeng Chen, Yingqian Min, Jie Chen, Xiaoxue Cheng, Jiapeng Wang, Yiru Tang, Haoxiang Sun, Jia Deng, Wayne Xin Zhao, et al",
      "title": "Technical report: Enhancing llm reasoning with reward-guided tree search",
      "publication": "arXiv preprint arXiv:2411.11694, 2024",
      "year": "2024",
      "summary": null,
      "standard_url": null,
      "id": "2024-technical_report:_enhancing_llm_reasoning_with_reward-guided_tree_search"
    },
    "26": {
      "enum": "26",
      "authors": "Bowen Jin, Jinsung Yoon, Jiawei Han, Sercan O. Arik",
      "title": "Long-context llms meet RAG: overcoming challenges for long inputs in RAG",
      "publication": "CoRR, abs/2410.05983, 2024",
      "year": 2024,
      "summary": "Retrieval-augmented generation (RAG) empowers large language models (LLMs) to utilize external knowledge sources. The increasing capacity of LLMs to process longer input sequences opens up avenues for providing more retrieved information, to potentially enhance the quality of generated outputs. It is plausible to assume that a larger retrieval set would contain more relevant information (higher recall), that might result in improved performance. However, our empirical findings demonstrate that for many long-context LLMs, the quality of generated output initially improves first, but then subsequently declines as the number of retrieved passages increases. This paper investigates this phenomenon, identifying the detrimental impact of retrieved \"hard negatives\" as a key contributor. To mitigate this and enhance the robustness of long-context LLM-based RAG, we propose both training-free and training-based approaches. We first showcase the effectiveness of retrieval reordering as a simple yet powerful training-free optimization. Furthermore, we explore training-based methods, specifically RAG-specific implicit LLM fine-tuning and RAG-oriented fine-tuning with intermediate reasoning, demonstrating their capacity for substantial performance gains. Finally, we conduct a systematic analysis of design choices for these training-based methods, including data distribution, retriever selection, and training context length.",
      "standard_url": "http://arxiv.org/abs/2410.05983v1",
      "id": "2024-long-context_llms_meet_rag:_overcoming_challenges_for_long_inputs_in_rag"
    },
    "27": {
      "enum": "27",
      "authors": "Jiajie Jin, Yutao Zhu, Yujia Zhou, Zhicheng Dou",
      "title": "Bider: Bridging knowledge inconsistency for efficient retrieval-augmented llms via key supporting evidence",
      "publication": "arXiv preprint arXiv:2402.12174, 2024",
      "year": 2024,
      "summary": "Retrieval-augmented large language models (LLMs) have demonstrated efficacy in knowledge-intensive tasks such as open-domain QA, addressing inherent challenges in knowledge update and factual inadequacy. However, inconsistencies between retrieval knowledge and the necessary knowledge for LLMs, leading to a decline in LLM's answer quality. This paper introduces BIDER, an approach that refines retrieval documents into Key Supporting Evidence (KSE) through knowledge synthesis, supervised fine-tuning (SFT), and preference alignment. We train BIDER by learning from crafting KSE, while maximizing its output to align with LLM's information acquisition preferences through reinforcement learning. Evaluations across five datasets show BIDER boosts LLMs' answer quality by 7% while reducing input content length in retrieval documents by 80%, outperforming existing methods. The proposed KSE simulation effectively equips LLMs with essential information for accurate question answering.",
      "standard_url": "http://arxiv.org/abs/2402.12174v2",
      "id": "2024-bider:_bridging_knowledge_inconsistency_for_efficient_retrieval-augmented_llms_via_key_supporting_evidence"
    },
    "28": {
      "enum": "28",
      "authors": "Mandar Joshi, Eunsol Choi, Daniel S. Weld, Luke Zettlemoyer",
      "title": "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension",
      "publication": "In ACL, pages 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics",
      "year": 2017,
      "summary": "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at -- http://nlp.cs.washington.edu/triviaqa/",
      "standard_url": "http://arxiv.org/abs/1705.03551v2",
      "id": "2017-triviaqa:_a_large_scale_distantly_supervised_challenge_dataset_for_reading_comprehension"
    },
    "29": {
      "enum": "29",
      "authors": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al",
      "title": "Natural questions: a benchmark for question answering research",
      "publication": "Transactions of the Association for Computational Linguistics, 7:453–466, 2019",
      "year": "2019",
      "summary": null,
      "standard_url": null,
      "id": "2019-natural_questions:_a_benchmark_for_question_answering_research"
    },
    "30": {
      "enum": "30",
      "authors": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela",
      "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
      "publication": "Advances in Neural Information Processing Systems, 33:9459–9474, 2020",
      "year": 2020,
      "summary": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
      "standard_url": "http://arxiv.org/abs/2005.11401v4",
      "id": "2020-retrieval-augmented_generation_for_knowledge-intensive_nlp_tasks"
    },
    "31": {
      "enum": "31",
      "authors": "Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra",
      "title": "Solving quantitative reasoning problems with language models",
      "publication": "In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022",
      "year": 2022,
      "summary": "Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.",
      "standard_url": "http://arxiv.org/abs/2206.14858v2",
      "id": "2022-solving_quantitative_reasoning_problems_with_language_models"
    },
    "32": {
      "enum": "32",
      "authors": "Chengpeng Li, Guanting Dong, Mingfeng Xue, Ru Peng, Xiang Wang, Dayiheng Liu",
      "title": "Dotamath: Decomposition of thought with code assistance and self-correction for mathematical reasoning",
      "publication": "CoRR, abs/2407.04078, 2024",
      "year": 2024,
      "summary": "Large language models (LLMs) have made impressive progress in handling simple math problems, yet they still struggle with more challenging and complex mathematical tasks. In this paper, we introduce a series of LLMs that employs the Decomposition of thought with code assistance and self-correction for mathematical reasoning, dubbed as DotaMath. DotaMath models tackle complex mathematical tasks by decomposing them into simpler logical subtasks, leveraging code to solve these subtasks, obtaining fine-grained feedback from the code interpreter, and engaging in self-reflection and correction. By annotating diverse interactive tool-use trajectories and employing query evolution on GSM8K and MATH datasets, we generate an instruction fine-tuning dataset called DotaMathQA with 574K query-response pairs. We train a series of base LLMs using imitation learning on DotaMathQA, resulting in DotaMath models that achieve remarkable performance compared to open-source LLMs across various in-domain and out-of-domain benchmarks. Notably, DotaMath-deepseek-7B showcases an outstanding performance of 64.8% on the competitive MATH dataset and 86.7% on GSM8K. Besides, DotaMath-deepseek-7B maintains strong competitiveness on a series of in-domain and out-of-domain benchmarks (Avg. 80.1%). Looking forward, we anticipate that the DotaMath paradigm will open new pathways for addressing intricate mathematical problems. Our code is publicly available at https://github.com/ChengpengLi1003/DotaMath.",
      "standard_url": "http://arxiv.org/abs/2407.04078v3",
      "id": "2024-dotamath:_decomposition_of_thought_with_code_assistance_and_self-correction_for_mathematical_reasoning"
    },
    "33": {
      "enum": "33",
      "authors": "Xiaoxi Li, Zhicheng Dou, Yujia Zhou, Fangchao Liu",
      "title": "Corpuslm: Towards a unified language model on corpus for knowledge-intensive tasks",
      "publication": "In Grace Hui Yang, Hongning Wang, Sam Han, Claudia Hauff, Guido Zuccon, and Yi Zhang, editors, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024, pages 26–37. ACM, 2024",
      "year": 2024,
      "summary": "Large language models (LLMs) have gained significant attention in various fields but prone to hallucination, especially in knowledge-intensive (KI) tasks. To address this, retrieval-augmented generation (RAG) has emerged as a popular solution to enhance factual accuracy. However, traditional retrieval modules often rely on large document index and disconnect with generative tasks. With the advent of generative retrieval (GR), language models can retrieve by directly generating document identifiers (DocIDs), offering superior performance in retrieval tasks. However, the potential relationship between GR and downstream tasks remains unexplored. In this paper, we propose \\textbf{CorpusLM}, a unified language model that leverages external corpus to tackle various knowledge-intensive tasks by integrating generative retrieval, closed-book generation, and RAG through a unified greedy decoding process. We design the following mechanisms to facilitate effective retrieval and generation, and improve the end-to-end effectiveness of KI tasks: (1) We develop a ranking-oriented DocID list generation strategy, which refines GR by directly learning from a DocID ranking list, to improve retrieval quality. (2) We design a continuous DocIDs-References-Answer generation strategy, which facilitates effective and efficient RAG. (3) We employ well-designed unsupervised DocID understanding tasks, to comprehend DocID semantics and their relevance to downstream tasks. We evaluate our approach on the widely used KILT benchmark with two variants of backbone models, i.e., T5 and Llama2. Experimental results demonstrate the superior performance of our models in both retrieval and downstream tasks.",
      "standard_url": "http://arxiv.org/abs/2402.01176v2",
      "id": "2024-corpuslm:_towards_a_unified_language_model_on_corpus_for_knowledge-intensive_tasks"
    },
    "34": {
      "enum": "34",
      "authors": "Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yongkang Wu, Zhonghua Li, Qi Ye, Zhicheng Dou",
      "title": "Retrollm: Empowering large language models to retrieve fine-grained evidence within generation",
      "publication": "arXiv preprint arXiv:2412.11919, 2024",
      "year": 2024,
      "summary": "Large language models (LLMs) exhibit remarkable generative capabilities but often suffer from hallucinations. Retrieval-augmented generation (RAG) offers an effective solution by incorporating external knowledge, but existing methods still face several limitations: additional deployment costs of separate retrievers, redundant input tokens from retrieved text chunks, and the lack of joint optimization of retrieval and generation. To address these issues, we propose \\textbf{RetroLLM}, a unified framework that integrates retrieval and generation into a single, cohesive process, enabling LLMs to directly generate fine-grained evidence from the corpus with constrained decoding. Moreover, to mitigate false pruning in the process of constrained evidence generation, we introduce (1) hierarchical FM-Index constraints, which generate corpus-constrained clues to identify a subset of relevant documents before evidence generation, reducing irrelevant decoding space; and (2) a forward-looking constrained decoding strategy, which considers the relevance of future sequences to improve evidence accuracy. Extensive experiments on five open-domain QA datasets demonstrate RetroLLM's superior performance across both in-domain and out-of-domain tasks. The code is available at \\url{https://github.com/sunnynexus/RetroLLM}.",
      "standard_url": "http://arxiv.org/abs/2412.11919v1",
      "id": "2024-retrollm:_empowering_large_language_models_to_retrieve_fine-grained_evidence_within_generation"
    },
    "35": {
      "enum": "35",
      "authors": "Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, Zhicheng Dou",
      "title": "From matching to generation: A survey on generative information retrieval",
      "publication": "CoRR, abs/2404.14851, 2024",
      "year": 2024,
      "summary": "Information Retrieval (IR) systems are crucial tools for users to access information, which have long been dominated by traditional methods relying on similarity matching. With the advancement of pre-trained language models, generative information retrieval (GenIR) emerges as a novel paradigm, attracting increasing attention. Based on the form of information provided to users, current research in GenIR can be categorized into two aspects: \\textbf{(1) Generative Document Retrieval} (GR) leverages the generative model's parameters for memorizing documents, enabling retrieval by directly generating relevant document identifiers without explicit indexing. \\textbf{(2) Reliable Response Generation} employs language models to directly generate information users seek, breaking the limitations of traditional IR in terms of document granularity and relevance matching while offering flexibility, efficiency, and creativity to meet practical needs. This paper aims to systematically review the latest research progress in GenIR. We will summarize the advancements in GR regarding model training and structure, document identifier, incremental learning, etc., as well as progress in reliable response generation in aspects of internal knowledge memorization, external knowledge augmentation, etc. We also review the evaluation, challenges and future developments in GenIR systems. This review aims to offer a comprehensive reference for researchers, encouraging further development in the GenIR field. Github Repository: https://github.com/RUC-NLPIR/GenIR-Survey",
      "standard_url": "http://arxiv.org/abs/2404.14851v4",
      "id": "2024-from_matching_to_generation:_a_survey_on_generative_information_retrieval"
    },
    "36": {
      "enum": "36",
      "authors": "Xiaoxi Li, Yujia Zhou, Zhicheng Dou",
      "title": "Unigen: A unified generative framework for retrieval and question answering with large language models",
      "publication": "In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan, editors, Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 8688–8696. AAAI Press, 2024",
      "year": 2023,
      "summary": "Generative information retrieval, encompassing two major tasks of Generative Document Retrieval (GDR) and Grounded Answer Generation (GAR), has gained significant attention in the area of information retrieval and natural language processing. Existing methods for GDR and GAR rely on separate retrieval and reader modules, which hinder simultaneous optimization. To overcome this, we present \\textbf{UniGen}, a \\textbf{Uni}fied \\textbf{Gen}erative framework for retrieval and question answering that integrates both tasks into a single generative model leveraging the capabilities of large language models. UniGen employs a shared encoder and two distinct decoders for generative retrieval and question answering. To facilitate the learning of both tasks, we introduce connectors, generated by large language models, to bridge the gaps between query inputs and generation targets, as well as between document identifiers and answers. Furthermore, we propose an iterative enhancement strategy that leverages generated answers and retrieved documents to iteratively improve both tasks. Through extensive experiments on the MS MARCO and NQ datasets, we demonstrate the effectiveness of UniGen, showcasing its superior performance in both the retrieval and the question answering tasks.",
      "standard_url": "http://arxiv.org/abs/2312.11036v1",
      "id": "2023-unigen:_a_unified_generative_framework_for_retrieval_and_question_answering_with_large_language_models"
    },
    "37": {
      "enum": "37",
      "authors": "Lei Liang, Mengshu Sun, Zhengke Gui, Zhongshu Zhu, Zhouyu Jiang, Ling Zhong, Yuan Qu, Peilong Zhao, Zhongpu Bo, Jin Yang, Huaidong Xiong, Lin Yuan, Jun Xu, Zaoyang Wang, Zhiqiang Zhang, Wen Zhang, Huajun Chen, Wenguang Chen, and Jun Zhou",
      "title": "Kag: Boosting llms in professional domains via knowledge augmented generation, 2024",
      "publication": null,
      "year": "2024",
      "summary": null,
      "standard_url": null,
      "id": "2024-kag:_boosting_llms_in_professional_domains_via_knowledge_augmented_generation_2024"
    },
    "38": {
      "enum": "38",
      "authors": "Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, Karl Cobbe",
      "title": "Let’s verify step by step",
      "publication": "In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024",
      "year": 2023,
      "summary": "In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.",
      "standard_url": "http://arxiv.org/abs/2305.20050v1",
      "id": "2023-let’s_verify_step_by_step"
    },
    "39": {
      "enum": "39",
      "authors": "Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, Yejin Choi",
      "title": "The unlocking spell on base llms: Rethinking alignment via in-context learning",
      "publication": "In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024",
      "year": 2023,
      "summary": "The alignment tuning process of large language models (LLMs) typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). A recent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for SFT can achieve significant alignment performance as well, suggesting that the effect of alignment tuning might be \"superficial.\" This raises questions about how exactly the alignment tuning transforms a base LLM.\n  We analyze the effect of alignment tuning by examining the token distribution shift between base LLMs and their aligned counterpart. Our findings reveal that base LLMs and their alignment-tuned versions perform nearly identically in decoding on the majority of token positions. Most distribution shifts occur with stylistic tokens. These direct evidence strongly supports the Superficial Alignment Hypothesis suggested by LIMA.\n  Based on these findings, we rethink the alignment of LLMs by posing the research question: how effectively can we align base LLMs without SFT or RLHF? To address this, we introduce a simple, tuning-free alignment method, URIAL. URIAL achieves effective alignment purely through in-context learning (ICL) with base LLMs, requiring as few as three constant stylistic examples and a system prompt. We conduct a fine-grained and interpretable evaluation on a diverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that base LLMs with URIAL can match or even surpass the performance of LLMs aligned with SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based alignment methods can be significantly reduced through strategic prompting and ICL. Our findings on the superficial nature of alignment tuning and results with URIAL suggest that deeper analysis and theoretical understanding of alignment is crucial to future LLM research.",
      "standard_url": "http://arxiv.org/abs/2312.01552v1",
      "id": "2023-the_unlocking_spell_on_base_llms:_rethinking_alignment_via_in-context_learning"
    },
    "40": {
      "enum": "40",
      "authors": "Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su",
      "title": "Deductive verification of chain-of-thought reasoning",
      "publication": "In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023",
      "year": 2023,
      "summary": "Large Language Models (LLMs) significantly benefit from Chain-of-Thought (CoT) prompting in performing various reasoning tasks. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification. However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like ChatGPT. In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. To facilitate this procedure, we propose Natural Program, a natural language-based deductive reasoning format. Our approach enables models to generate precise reasoning steps where subsequent steps are more rigorously grounded on prior steps. It also empowers language models to carry out reasoning self-verification in a step-by-step manner. By integrating this verification process into each deductive reasoning stage, we significantly enhance the rigor and trustfulness of generated reasoning steps. Along this process, we also improve the answer correctness on complex reasoning tasks. Code will be released at https://github.com/lz1oceani/verify_cot.",
      "standard_url": "http://arxiv.org/abs/2306.03872v3",
      "id": "2023-deductive_verification_of_chain-of-thought_reasoning"
    },
    "41": {
      "enum": "41",
      "authors": "Jingyu Liu, Jiaen Lin, Yong Liu",
      "title": "How much can RAG help the reasoning of llm?",
      "publication": "CoRR, abs/2410.02338, 2024",
      "year": 2024,
      "summary": "Retrieval-Augmented Generation (RAG) has gained significant popularity in modern Large Language Models (LLMs) due to its effectiveness in introducing new knowledge and reducing hallucinations. However, the deep understanding of RAG remains limited, how does RAG help the reasoning process and can RAG help improve the reasoning capability remains question. While external documents are typically considered as a method to incorporate domain-specific information, they also contain intermediate reasoning results related to the query, this suggests that documents could enhance the reasoning capability of LLMs, which has not been previously explored. In this paper, we investigate this issue in depth and find that while RAG can assist with reasoning, the help is limited. If we conceptualize the reasoning process as a tree with fixed depth, then RAG struggles to assist LLMs in performing deeper reasoning. Additionally, the information in the documents requires preprocessing to filter out noise. We demonstrate that this preprocessing is difficult to achieve simply fine-tuning of the LLM, it often necessitates numerous additional transformer layers to solve the problem. To simplify the problem, we propose DPrompt tuning, which effectively resolves the issue within just limited transformer layers, leading to improved performance.",
      "standard_url": "http://arxiv.org/abs/2410.02338v2",
      "id": "2024-how_much_can_rag_help_the_reasoning_of_llm?"
    },
    "42": {
      "enum": "42",
      "authors": "Jingyu Liu, Jiaen Lin, Yong Liu",
      "title": "How much can RAG help the reasoning of llm?",
      "publication": "CoRR, abs/2410.02338, 2024",
      "year": 2024,
      "summary": "Retrieval-Augmented Generation (RAG) has gained significant popularity in modern Large Language Models (LLMs) due to its effectiveness in introducing new knowledge and reducing hallucinations. However, the deep understanding of RAG remains limited, how does RAG help the reasoning process and can RAG help improve the reasoning capability remains question. While external documents are typically considered as a method to incorporate domain-specific information, they also contain intermediate reasoning results related to the query, this suggests that documents could enhance the reasoning capability of LLMs, which has not been previously explored. In this paper, we investigate this issue in depth and find that while RAG can assist with reasoning, the help is limited. If we conceptualize the reasoning process as a tree with fixed depth, then RAG struggles to assist LLMs in performing deeper reasoning. Additionally, the information in the documents requires preprocessing to filter out noise. We demonstrate that this preprocessing is difficult to achieve simply fine-tuning of the LLM, it often necessitates numerous additional transformer layers to solve the problem. To simplify the problem, we propose DPrompt tuning, which effectively resolves the issue within just limited transformer layers, leading to improved performance.",
      "standard_url": "http://arxiv.org/abs/2410.02338v2",
      "id": "2024-how_much_can_rag_help_the_reasoning_of_llm?"
    },
    "43": {
      "enum": "43",
      "authors": "Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, Nan Duan",
      "title": "Query rewriting for retrieval-augmented large language models",
      "publication": "arXiv preprint arXiv:2305.14283, 2023",
      "year": 2023,
      "summary": "Large Language Models (LLMs) play powerful, black-box readers in the retrieve-then-read pipeline, making remarkable progress in knowledge-intensive tasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of the previous retrieve-then-read for the retrieval-augmented LLMs from the perspective of the query rewriting. Unlike prior studies focusing on adapting either the retriever or the reader, our approach pays attention to the adaptation of the search query itself, for there is inevitably a gap between the input text and the needed knowledge in retrieval. We first prompt an LLM to generate the query, then use a web search engine to retrieve contexts. Furthermore, to better align the query to the frozen modules, we propose a trainable scheme for our pipeline. A small language model is adopted as a trainable rewriter to cater to the black-box LLM reader. The rewriter is trained using the feedback of the LLM reader by reinforcement learning. Evaluation is conducted on downstream tasks, open-domain QA and multiple-choice QA. Experiments results show consistent performance improvement, indicating that our framework is proven effective and scalable, and brings a new framework for retrieval-augmented LLM.",
      "standard_url": "http://arxiv.org/abs/2305.14283v3",
      "id": "2023-query_rewriting_for_retrieval-augmented_large_language_models"
    },
    "44": {
      "enum": "44",
      "authors": "Ning Miao, Yee Whye Teh, Tom Rainforth",
      "title": "Selfcheck: Using llms to zero-shot check their own step-by-step reasoning",
      "publication": "In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024",
      "year": 2023,
      "summary": "The recent progress in large language models (LLMs), especially the invention of chain-of-thought prompting, has made it possible to automatically answer questions by stepwise reasoning. However, when faced with more complicated problems that require non-linear thinking, even the strongest LLMs make mistakes. To address this, we explore whether LLMs are able to recognize errors in their own step-by-step reasoning, without resorting to external resources. To this end, we propose SelfCheck, a general-purpose zero-shot verification schema for recognizing such errors. We then use the results of these checks to improve question-answering performance by conducting weighted voting on multiple solutions to the question. We test SelfCheck on three datasets (GSM8K, MathQA, and MATH) and find that it successfully recognizes errors and, in turn, increases final answer accuracies.",
      "standard_url": "http://arxiv.org/abs/2308.00436v3",
      "id": "2023-selfcheck:_using_llms_to_zero-shot_check_their_own_step-by-step_reasoning"
    },
    "45": {
      "enum": "45",
      "authors": "Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, Ji-Rong Wen",
      "title": "Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning systems",
      "publication": "arXiv preprint arXiv:2412.09413, 2024",
      "year": 2024,
      "summary": "Recently, slow-thinking reasoning systems, such as o1, have demonstrated remarkable capabilities in solving complex reasoning tasks. These systems typically engage in an extended thinking process before responding to a query, allowing them to generate more thorough, accurate, and well-reasoned solutions. These systems are primarily developed and maintained by industry, with their core techniques not publicly disclosed. In response, an increasing number of studies from the research community aim to explore the technical foundations underlying these powerful reasoning systems. Building on these prior efforts, this paper presents a reproduction report on implementing o1-like reasoning systems. We introduce an ``imitate, explore, and self-improve'' framework, denoted as \\textbf{STILL-2}, as our primary technical approach to train the reasoning model. In the initial phase, we use distilled long-form thought data to fine-tune the reasoning model, enabling it to invoke a slow-thinking mode. The model is then encouraged to explore challenging problems by generating multiple rollouts, which can result in increasingly more high-quality trajectories that lead to correct answers. Furthermore, the model undergoes self-improvement by iteratively refining its training dataset. To verify the effectiveness of this approach, we conduct extensive experiments on three challenging benchmarks. The experimental results demonstrate that our approach achieves competitive performance compared to industry-level reasoning systems on these benchmarks.",
      "standard_url": "http://arxiv.org/abs/2412.09413v2",
      "id": "2024-imitate_explore_and_self-improve:_a_reproduction_report_on_slow-thinking_reasoning_systems"
    },
    "46": {
      "enum": "46",
      "authors": "OpenAI",
      "title": "Learning to reason with llms, 2024",
      "publication": null,
      "year": "2024",
      "summary": null,
      "standard_url": null,
      "id": "2024-learning_to_reason_with_llms_2024"
    },
    "47": {
      "enum": "47",
      "authors": "Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, Mike Lewis",
      "title": "Measuring and narrowing the compositionality gap in language models",
      "publication": "In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 5687–5711. Association for Computational Linguistics, 2023",
      "year": 2022,
      "summary": "We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning.\n  We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly. We present a new method, self-ask, that further improves on chain of thought. In our method, the model explicitly asks itself (and answers) follow-up questions before answering the initial question. We finally show that self-ask's structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy.",
      "standard_url": "http://arxiv.org/abs/2210.03350v3",
      "id": "2022-measuring_and_narrowing_the_compositionality_gap_in_language_models"
    },
    "48": {
      "enum": "48",
      "authors": "Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, Runfeng Qiao, Yifan Zhang, Xiao Zong, Yida Xu, Muxi Diao, Zhimin Bao, Chen Li, Honggang Zhang",
      "title": "We-math: Does your large multimodal model achieve human-like mathematical reasoning?",
      "publication": "CoRR, abs/2407.01284, 2024",
      "year": 2024,
      "summary": "Visual mathematical reasoning, as a fundamental visual reasoning ability, has received widespread attention from the Large Multimodal Models (LMMs) community. Existing benchmarks, such as MathVista and MathVerse, focus more on the result-oriented performance but neglect the underlying principles in knowledge acquisition and generalization. Inspired by human-like mathematical reasoning, we introduce WE-MATH, the first benchmark specifically designed to explore the problem-solving principles beyond end-to-end performance. We meticulously collect and categorize 6.5K visual math problems, spanning 67 hierarchical knowledge concepts and five layers of knowledge granularity. We decompose composite problems into sub-problems according to the required knowledge concepts and introduce a novel four-dimensional metric, namely Insufficient Knowledge (IK), Inadequate Generalization (IG), Complete Mastery (CM), and Rote Memorization (RM), to hierarchically assess inherent issues in LMMs' reasoning process. With WE-MATH, we conduct a thorough evaluation of existing LMMs in visual mathematical reasoning and reveal a negative correlation between solving steps and problem-specific performance. We confirm the IK issue of LMMs can be effectively improved via knowledge augmentation strategies. More notably, the primary challenge of GPT-4o has significantly transitioned from IK to IG, establishing it as the first LMM advancing towards the knowledge generalization stage. In contrast, other LMMs exhibit a marked inclination towards Rote Memorization - they correctly solve composite problems involving multiple knowledge concepts yet fail to answer sub-problems. We anticipate that WE-MATH will open new pathways for advancements in visual mathematical reasoning for LMMs. The WE-MATH data and evaluation code are available at https://github.com/We-Math/We-Math.",
      "standard_url": "http://arxiv.org/abs/2407.01284v1",
      "id": "2024-we-math:_does_your_large_multimodal_model_achieve_human-like_mathematical_reasoning?"
    },
    "49": {
      "enum": "49",
      "authors": "Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, Pengfei Liu",
      "title": "O1 replication journey: A strategic progress report–part 1",
      "publication": "arXiv preprint arXiv:2410.18982, 2024",
      "year": 2024,
      "summary": "This paper introduces a pioneering approach to artificial intelligence research, embodied in our O1 Replication Journey. In response to the announcement of OpenAI's groundbreaking O1 model, we embark on a transparent, real-time exploration to replicate its capabilities while reimagining the process of conducting and communicating AI research. Our methodology addresses critical challenges in modern AI research, including the insularity of prolonged team-based projects, delayed information sharing, and the lack of recognition for diverse contributions. By providing comprehensive, real-time documentation of our replication efforts, including both successes and failures, we aim to foster open science, accelerate collective advancement, and lay the groundwork for AI-driven scientific discovery. Our research progress report diverges significantly from traditional research papers, offering continuous updates, full process transparency, and active community engagement throughout the research journey. Technologically, we proposed the journey learning paradigm, which encourages models to learn not just shortcuts, but the complete exploration process, including trial and error, reflection, and backtracking. With only 327 training samples and without any additional tricks, journey learning outperformed conventional supervised learning by over 8\\% on the MATH dataset, demonstrating its extremely powerful potential. We believe this to be the most crucial component of O1 technology that we have successfully decoded. We share valuable resources including technical hypotheses and insights, cognitive exploration maps, custom-developed tools, etc at https://github.com/GAIR-NLP/O1-Journey.",
      "standard_url": "http://arxiv.org/abs/2410.18982v1",
      "id": "2024-o1_replication_journey:_a_strategic_progress_report–part_1"
    },
    "50": {
      "enum": "50",
      "authors": "Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu",
      "title": "Qwen2.5 technical report, 2024",
      "publication": null,
      "year": "2024",
      "summary": null,
      "standard_url": null,
      "id": "2024-qwen2.5_technical_report_2024"
    },
    "51": {
      "enum": "51",
      "authors": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "publication": "J. Mach. Learn. Res., 21:140:1–140:67, 2020",
      "year": 2019,
      "summary": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
      "standard_url": "http://arxiv.org/abs/1910.10683v4",
      "id": "2019-exploring_the_limits_of_transfer_learning_with_a_unified_text-to-text_transformer"
    },
    "52": {
      "enum": "52",
      "authors": "David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R. Bowman",
      "title": "GPQA: A graduate-level google-proof q&a benchmark",
      "publication": "CoRR, abs/2311.12022, 2023",
      "year": 2023,
      "summary": "We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are \"Google-proof\"). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",
      "standard_url": "http://arxiv.org/abs/2311.12022v1",
      "id": "2023-gpqa:_a_graduate-level_google-proof_q&a_benchmark"
    },
    "53": {
      "enum": "53",
      "authors": "Jiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo, Kun Fang, Ji-Rong Wen",
      "title": "Small models, big insights: Leveraging slim proxy models to decide when and what to retrieve for llms",
      "publication": "arXiv preprint arXiv:2402.12052, 2024",
      "year": 2024,
      "summary": "The integration of large language models (LLMs) and search engines represents a significant evolution in knowledge acquisition methodologies. However, determining the knowledge that an LLM already possesses and the knowledge that requires the help of a search engine remains an unresolved issue. Most existing methods solve this problem through the results of preliminary answers or reasoning done by the LLM itself, but this incurs excessively high computational costs. This paper introduces a novel collaborative approach, namely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model, to enhance the LLM's knowledge acquisition process. We employ a proxy model which has far fewer parameters, and take its answers as heuristic answers. Heuristic answers are then utilized to predict the knowledge required to answer the user question, as well as the known and unknown knowledge within the LLM. We only conduct retrieval for the missing knowledge in questions that the LLM does not know. Extensive experimental results on five datasets with two LLMs demonstrate a notable improvement in the end-to-end performance of LLMs in question-answering tasks, achieving or surpassing current state-of-the-art models with lower LLM inference costs.",
      "standard_url": "http://arxiv.org/abs/2402.12052v3",
      "id": "2024-small_models_big_insights:_leveraging_slim_proxy_models_to_decide_when_and_what_to_retrieve_for_llms"
    },
    "54": {
      "enum": "54",
      "authors": "Qwen Team",
      "title": "Qwq: Reflect deeply on the boundaries of the unknown, November 2024",
      "publication": null,
      "year": "2024",
      "summary": null,
      "standard_url": null,
      "id": "2024-qwq:_reflect_deeply_on_the_boundaries_of_the_unknown_november_2024"
    },
    "55": {
      "enum": "55",
      "authors": "Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal",
      "title": "♫ musique: Multihop questions via single-hop question composition",
      "publication": "Transactions of the Association for Computational Linguistics, 10:539–554, 2022",
      "year": 2021,
      "summary": "Multihop reasoning remains an elusive goal as existing multihop benchmarks are known to be largely solvable via shortcuts. Can we create a question answering (QA) dataset that, by construction, \\emph{requires} proper multihop reasoning? To this end, we introduce a bottom-up approach that systematically selects composable pairs of single-hop questions that are connected, i.e., where one reasoning step critically relies on information from another. This bottom-up methodology lets us explore a vast space of questions and add stringent filters as well as other mechanisms targeting connected reasoning. It provides fine-grained control over the construction process and the properties of the resulting $k$-hop questions. We use this methodology to create MuSiQue-Ans, a new multihop QA dataset with 25K 2-4 hop questions. Relative to existing datasets, MuSiQue-Ans is more difficult overall (3x increase in human-machine gap), and harder to cheat via disconnected reasoning (e.g., a single-hop model has a 30 point drop in F1). We further add unanswerable contrast questions to produce a more stringent dataset, MuSiQue-Full. We hope our datasets will help the NLP community develop models that perform genuine multihop reasoning.",
      "standard_url": "http://arxiv.org/abs/2108.00573v3",
      "id": "2021-♫_musique:_multihop_questions_via_single-hop_question_composition"
    },
    "56": {
      "enum": "56",
      "authors": "Prakhar Verma, Sukruta Prakash Midigeshi, Gaurav Sinha, Arno Solin, Nagarajan Natarajan, and Amit Sharma",
      "title": "Planxrag: Planning-guided retrieval augmented generation",
      "publication": "arXiv preprint arXiv:2410.20753, 2024",
      "year": "2024",
      "summary": null,
      "standard_url": null,
      "id": "2024-planxrag:_planning-guided_retrieval_augmented_generation"
    },
    "57": {
      "enum": "57",
      "authors": "Jiaan Wang, Fandong Meng, Yunlong Liang, and Jie Zhou",
      "title": "Drt-o1: Optimized deep reasoning translation via long chain-of-thought",
      "publication": "arXiv preprint arXiv:2412.17498, 2024",
      "year": "2024",
      "summary": null,
      "standard_url": null,
      "id": "2024-drt-o1:_optimized_deep_reasoning_translation_via_long_chain-of-thought"
    },
    "58": {
      "enum": "58",
      "authors": "Liang Wang, Nan Yang, Furu Wei",
      "title": "Query2doc: Query expansion with large language models",
      "publication": "arXiv preprint arXiv:2303.07678, 2023",
      "year": 2023,
      "summary": "This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo-documents. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results.",
      "standard_url": "http://arxiv.org/abs/2303.07678v2",
      "id": "2023-query2doc:_query_expansion_with_large_language_models"
    },
    "59": {
      "enum": "59",
      "authors": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "publication": "Advances in neural information processing systems, 35:24824–24837, 2022",
      "year": 2022,
      "summary": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
      "standard_url": "http://arxiv.org/abs/2201.11903v6",
      "id": "2022-chain-of-thought_prompting_elicits_reasoning_in_large_language_models"
    },
    "60": {
      "enum": "60",
      "authors": "Ken C. L. Wong, Hongzhi Wang, Etienne E. Vos, Bianca Zadrozny, Campbell D. Watson, Tanveer Syeda-Mahmood",
      "title": "Addressing deep learning model uncertainty in long-range climate forecasting with late fusion",
      "publication": "CoRR, abs/2112.05254, 2021",
      "year": 2021,
      "summary": "Global warming leads to the increase in frequency and intensity of climate extremes that cause tremendous loss of lives and property. Accurate long-range climate prediction allows more time for preparation and disaster risk management for such extreme events. Although machine learning approaches have shown promising results in long-range climate forecasting, the associated model uncertainties may reduce their reliability. To address this issue, we propose a late fusion approach that systematically combines the predictions from multiple models to reduce the expected errors of the fused results. We also propose a network architecture with the novel denormalization layer to gain the benefits of data normalization without actually normalizing the data. The experimental results on long-range 2m temperature forecasting show that the framework outperforms the 30-year climate normals, and the accuracy can be improved by increasing the number of models.",
      "standard_url": "http://arxiv.org/abs/2112.05254v1",
      "id": "2021-addressing_deep_learning_model_uncertainty_in_long-range_climate_forecasting_with_late_fusion"
    },
    "61": {
      "enum": "61",
      "authors": "Siwei Wu, Zhongyuan Peng, Xinrun Du, Tuney Zheng, Minghao Liu, Jialong Wu, Jiachen Ma, Yizhi Li, Jian Yang, Wangchunshu Zhou, Qunshu Lin, Junbo Zhao, Zhaoxiang Zhang, Wenhao Huang, Ge Zhang, Chenghua Lin, J. H. Liu",
      "title": "A comparative study on reasoning patterns of openai’s o1 model",
      "publication": "CoRR, abs/2410.13639, 2024",
      "year": 2024,
      "summary": "Enabling Large Language Models (LLMs) to handle a wider range of complex tasks (e.g., coding, math) has drawn great attention from many researchers. As LLMs continue to evolve, merely increasing the number of model parameters yields diminishing performance improvements and heavy computational costs. Recently, OpenAI's o1 model has shown that inference strategies (i.e., Test-time Compute methods) can also significantly enhance the reasoning capabilities of LLMs. However, the mechanisms behind these methods are still unexplored. In our work, to investigate the reasoning patterns of o1, we compare o1 with existing Test-time Compute methods (BoN, Step-wise BoN, Agent Workflow, and Self-Refine) by using OpenAI's GPT-4o as a backbone on general reasoning benchmarks in three domains (i.e., math, coding, commonsense reasoning). Specifically, first, our experiments show that the o1 model has achieved the best performance on most datasets. Second, as for the methods of searching diverse responses (e.g., BoN), we find the reward models' capability and the search space both limit the upper boundary of these methods. Third, as for the methods that break the problem into many sub-problems, the Agent Workflow has achieved better performance than Step-wise BoN due to the domain-specific system prompt for planning better reasoning processes. Fourth, it is worth mentioning that we have summarized six reasoning patterns of o1, and provided a detailed analysis on several reasoning benchmarks.",
      "standard_url": "http://arxiv.org/abs/2410.13639v2",
      "id": "2024-a_comparative_study_on_reasoning_patterns_of_openai’s_o1_model"
    },
    "62": {
      "enum": "62",
      "authors": "Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, and Yanghua Xiao",
      "title": "How easily do irrelevant inputs skew the responses of large language models?, 2024",
      "publication": null,
      "year": "2024",
      "summary": null,
      "standard_url": null,
      "id": "2024-how_easily_do_irrelevant_inputs_skew_the_responses_of_large_language_models?_2024"
    },
    "63": {
      "enum": "63",
      "authors": "Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, Pengfei Liu",
      "title": "Evaluating mathematical reasoning beyond accuracy",
      "publication": "CoRR, abs/2404.05692, 2024",
      "year": 2024,
      "summary": "The leaderboard of Large Language Models (LLMs) in mathematical tasks has been continuously updated. However, the majority of evaluations focus solely on the final results, neglecting the quality of the intermediate steps. This oversight can mask underlying problems, such as logical errors or unnecessary steps in the reasoning process. To measure reasoning beyond final-answer accuracy, we introduce ReasonEval, a new methodology for evaluating the quality of reasoning steps. ReasonEval employs validity and redundancy to characterize the reasoning quality, as well as accompanying LLMs to assess them automatically. We explore different design options for the LLM-based evaluators and empirically demonstrate that ReasonEval, when instantiated with base models possessing strong mathematical knowledge and trained with high-quality labeled data, consistently outperforms baseline methods in the meta-evaluation datasets. We also highlight the strong generalization capabilities of ReasonEval. By utilizing ReasonEval to evaluate LLMs specialized in math, we find that an increase in final-answer accuracy does not necessarily guarantee an improvement in the overall quality of the reasoning steps for challenging mathematical problems. Additionally, we observe that ReasonEval can play a significant role in data selection. We open-source the best-performing model, meta-evaluation script, and all evaluation results to facilitate future research.",
      "standard_url": "http://arxiv.org/abs/2404.05692v2",
      "id": "2024-evaluating_mathematical_reasoning_beyond_accuracy"
    },
    "64": {
      "enum": "64",
      "authors": "Fangyuan Xu, Weijia Shi, Eunsol Choi",
      "title": "Recomp: Improving retrieval-augmented lms with compression and selective augmentation",
      "publication": "arXiv preprint arXiv:2310.04408, 2023",
      "year": 2023,
      "summary": "Retrieving documents and prepending them in-context at inference time improves performance of language model (LMs) on a wide range of tasks. However, these documents, often spanning hundreds of words, make inference substantially more expensive. We propose compressing the retrieved documents into textual summaries prior to in-context integration. This not only reduces the computational costs but also relieves the burden of LMs to identify relevant information in long retrieved documents. We present two compressors -- an extractive compressor which selects useful sentences from retrieved documents and an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs' performance on end tasks when the generated summaries are prepended to the LMs' input, while keeping the summary concise.If the retrieved documents are irrelevant to the input or offer no additional information to LM, our compressor can return an empty string, implementing selective augmentation.We evaluate our approach on language modeling task and open domain question answering task. We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summarization models. We show that our compressors trained for one LM can transfer to other LMs on the language modeling task and provide summaries largely faithful to the retrieved documents.",
      "standard_url": "http://arxiv.org/abs/2310.04408v1",
      "id": "2023-recomp:_improving_retrieval-augmented_lms_with_compression_and_selective_augmentation"
    },
    "65": {
      "enum": "65",
      "authors": "Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan",
      "title": "Llava-o1: Let vision language models reason step-by-step",
      "publication": "arXiv preprint arXiv:2411.10440, 2024",
      "year": "2024",
      "summary": null,
      "standard_url": null,
      "id": "2024-llava-o1:_let_vision_language_models_reason_step-by-step"
    },
    "66": {
      "enum": "66",
      "authors": "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, Zhihao Fan",
      "title": "Qwen2 technical report",
      "publication": "CoRR, abs/2407.10671, 2024",
      "year": 2024,
      "summary": "This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach.\n  To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face and ModelScope, and the supplementary materials including example code on GitHub. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.",
      "standard_url": "http://arxiv.org/abs/2407.10671v4",
      "id": "2024-qwen2_technical_report"
    },
    "67": {
      "enum": "67",
      "authors": "An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, Zhenru Zhang",
      "title": "Qwen2.5-math technical report: Toward mathematical expert model via self-improvement",
      "publication": "CoRR, abs/2409.12122, 2024",
      "year": 2024,
      "summary": "In this report, we present a series of math-specific large language models: Qwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B. The core innovation of the Qwen2.5 series lies in integrating the philosophy of self-improvement throughout the entire pipeline, from pre-training and post-training to inference: (1) During the pre-training phase, Qwen2-Math-Instruct is utilized to generate large-scale, high-quality mathematical data. (2) In the post-training phase, we develop a reward model (RM) by conducting massive sampling from Qwen2-Math-Instruct. This RM is then applied to the iterative evolution of data in supervised fine-tuning (SFT). With a stronger SFT model, it's possible to iteratively train and update the RM, which in turn guides the next round of SFT data iteration. On the final SFT model, we employ the ultimate RM for reinforcement learning, resulting in the Qwen2.5-Math-Instruct. (3) Furthermore, during the inference stage, the RM is used to guide sampling, optimizing the model's performance.\n  Qwen2.5-Math-Instruct supports both Chinese and English, and possess advanced mathematical reasoning capabilities, including Chain-of-Thought (CoT) and Tool-Integrated Reasoning (TIR). We evaluate our models on 10 mathematics datasets in both English and Chinese, such as GSM8K, MATH, GaoKao, AMC23, and AIME24, covering a range of difficulties from grade school level to math competition problems.",
      "standard_url": "http://arxiv.org/abs/2409.12122v1",
      "id": "2024-qwen2.5-math_technical_report:_toward_mathematical_expert_model_via_self-improvement"
    },
    "68": {
      "enum": "68",
      "authors": "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, Christopher D. Manning",
      "title": "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
      "publication": "In EMNLP, pages 2369–2380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics",
      "year": 2018,
      "summary": "Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems' ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.",
      "standard_url": "http://arxiv.org/abs/1809.09600v1",
      "id": "2018-hotpotqa:_a_dataset_for_diverse_explainable_multi-hop_question_answering"
    },
    "69": {
      "enum": "69",
      "authors": "Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, Dacheng Tao",
      "title": "Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search",
      "publication": "arXiv preprint arXiv:2412.18319, 2024",
      "year": 2024,
      "summary": "In this work, we aim to develop an MLLM that understands and solves questions by learning to create each intermediate step of the reasoning involved till the final answer. To this end, we propose Collective Monte Carlo Tree Search (CoMCTS), a new learning-to-reason method for MLLMs, which introduces the concept of collective learning into ``tree search'' for effective and efficient reasoning-path searching and learning. The core idea of CoMCTS is to leverage collective knowledge from multiple models to collaboratively conjecture, search and identify effective reasoning paths toward correct answers via four iterative operations including Expansion, Simulation and Error Positioning, Backpropagation, and Selection. Using CoMCTS, we construct Mulberry-260k, a multimodal dataset with a tree of rich, explicit and well-defined reasoning nodes for each question. With Mulberry-260k, we perform collective SFT to train our model, Mulberry, a series of MLLMs with o1-like step-by-step Reasoning and Reflection capabilities. Extensive experiments demonstrate the superiority of our proposed methods on various benchmarks. Code will be available at https://github.com/HJYao00/Mulberry",
      "standard_url": "http://arxiv.org/abs/2412.18319v2",
      "id": "2024-mulberry:_empowering_mllm_with_o1-like_reasoning_and_reflection_via_collective_monte_carlo_tree_search"
    },
    "70": {
      "enum": "70",
      "authors": "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao",
      "title": "React: Synergizing reasoning and acting in language models",
      "publication": "arXiv preprint arXiv:2210.03629, 2022",
      "year": 2022,
      "summary": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",
      "standard_url": "http://arxiv.org/abs/2210.03629v3",
      "id": "2022-react:_synergizing_reasoning_and_acting_in_language_models"
    },
    "71": {
      "enum": "71",
      "authors": "Tian Ye, Zicheng Xu, Yuanzhi Li, Zeyuan Allen-Zhu",
      "title": "Physics of language models: Part 2.2, how to learn from mistakes on grade-school math problems",
      "publication": "arXiv preprint arXiv:2408.16293, 2024",
      "year": 2024,
      "summary": "Language models have demonstrated remarkable performance in solving reasoning tasks; however, even the strongest models still occasionally make reasoning mistakes. Recently, there has been active research aimed at improving reasoning accuracy, particularly by using pretrained language models to \"self-correct\" their mistakes via multi-round prompting. In this paper, we follow this line of work but focus on understanding the usefulness of incorporating \"error-correction\" data directly into the pretraining stage. This data consists of erroneous solution steps immediately followed by their corrections. Using a synthetic math dataset, we show promising results: this type of pretrain data can help language models achieve higher reasoning accuracy directly (i.e., through simple auto-regression, without multi-round prompting) compared to pretraining on the same amount of error-free data. We also delve into many details, such as (1) how this approach differs from beam search, (2) how such data can be prepared, (3) whether masking is needed on the erroneous tokens, (4) the amount of error required, (5) whether such data can be deferred to the fine-tuning stage, and many others.",
      "standard_url": "http://arxiv.org/abs/2408.16293v1",
      "id": "2024-physics_of_language_models:_part_2.2_how_to_learn_from_mistakes_on_grade-school_math_problems"
    },
    "72": {
      "enum": "72",
      "authors": "Ori Yoran, Tomer Wolfson, Ori Ram, Jonathan Berant",
      "title": "Making retrieval-augmented language models robust to irrelevant context",
      "publication": "In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024",
      "year": 2023,
      "summary": "Retrieval-augmented language models (RALMs) hold promise to produce language understanding systems that are are factual, efficient, and up-to-date. An important desideratum of RALMs, is that retrieved information helps model performance when it is relevant, and does not harm performance when it is not. This is particularly important in multi-hop reasoning scenarios, where misuse of irrelevant evidence can lead to cascading errors. However, recent work has shown that retrieval augmentation can sometimes have a negative effect on performance. In this work, we present a thorough analysis on five open-domain question answering benchmarks, characterizing cases when retrieval reduces accuracy. We then propose two methods to mitigate this issue. First, a simple baseline that filters out retrieved passages that do not entail question-answer pairs according to a natural language inference (NLI) model. This is effective in preventing performance reduction, but at a cost of also discarding relevant passages. Thus, we propose a method for automatically generating data to fine-tune the language model to properly leverage retrieved passages, using a mix of relevant and irrelevant contexts at training time. We empirically show that even 1,000 examples suffice to train the model to be robust to irrelevant contexts while maintaining high performance on examples with relevant ones.",
      "standard_url": "http://arxiv.org/abs/2310.01558v2",
      "id": "2023-making_retrieval-augmented_language_models_robust_to_irrelevant_context"
    },
    "73": {
      "enum": "73",
      "authors": "Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu",
      "title": "Metamath: Bootstrap your own mathematical questions for large language models",
      "publication": "In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024",
      "year": 2023,
      "summary": "Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4% on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of 82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use.",
      "standard_url": "http://arxiv.org/abs/2309.12284v4",
      "id": "2023-metamath:_bootstrap_your_own_mathematical_questions_for_large_language_models"
    },
    "74": {
      "enum": "74",
      "authors": "Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, Jingren Zhou",
      "title": "Scaling relationship on learning mathematical reasoning with large language models",
      "publication": "CoRR, abs/2308.01825, 2023",
      "year": 2023,
      "summary": "Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored. In this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM. We find that pre-training loss is a better indicator of the model's performance than the model's parameter count. We apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets. To augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets. We find with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs. We also find RFT brings more improvement for less performant LLMs. Furthermore, we combine rejection samples from multiple models which push LLaMA-7B to an accuracy of 49.3\\% on GSM8K which outperforms the supervised fine-tuning (SFT) accuracy of 35.9\\% significantly.",
      "standard_url": "http://arxiv.org/abs/2308.01825v2",
      "id": "2023-scaling_relationship_on_learning_mathematical_reasoning_with_large_language_models"
    },
    "75": {
      "enum": "75",
      "authors": "Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, Noah D. Goodman",
      "title": "Quiet-star: Language models can teach themselves to think before speaking",
      "publication": "arXiv preprint arXiv:2403.09629, 2024",
      "year": 2024,
      "summary": "When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continuations, 2) the fact that the LM does not initially know how to generate or use internal thoughts, and 3) the need to predict beyond individual next tokens. To resolve these, we propose a tokenwise parallel sampling algorithm, using learnable tokens indicating a thought's start and end, and an extended teacher-forcing technique. Encouragingly, generated rationales disproportionately help model difficult-to-predict tokens and improve the LM's ability to directly answer difficult questions. In particular, after continued pretraining of an LM on a corpus of internet text with Quiet-STaR, we find zero-shot improvements on GSM8K (5.9%$\\rightarrow$10.9%) and CommonsenseQA (36.3%$\\rightarrow$47.2%) and observe a perplexity improvement of difficult tokens in natural text. Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR marks a step towards LMs that can learn to reason in a more general and scalable way.",
      "standard_url": "http://arxiv.org/abs/2403.09629v2",
      "id": "2024-quiet-star:_language_models_can_teach_themselves_to_think_before_speaking"
    },
    "76": {
      "enum": "76",
      "authors": "Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Bo Wang, Shimin Li, Yunhua Zhou, Qipeng Guo, Xuanjing Huang, Xipeng Qiu",
      "title": "Scaling of search and learning: A roadmap to reproduce o1 from reinforcement learning perspective",
      "publication": "arXiv preprint arXiv:2412.14135, 2024",
      "year": 2024,
      "summary": "OpenAI o1 represents a significant milestone in Artificial Inteiligence, which achieves expert-level performances on many challanging tasks that require strong reasoning ability.OpenAI has claimed that the main techinique behinds o1 is the reinforcement learining. Recent works use alternative approaches like knowledge distillation to imitate o1's reasoning style, but their effectiveness is limited by the capability ceiling of the teacher model. Therefore, this paper analyzes the roadmap to achieving o1 from the perspective of reinforcement learning, focusing on four key components: policy initialization, reward design, search, and learning. Policy initialization enables models to develop human-like reasoning behaviors, equipping them with the ability to effectively explore solution spaces for complex problems. Reward design provides dense and effective signals via reward shaping or reward modeling, which is the guidance for both search and learning. Search plays a crucial role in generating high-quality solutions during both training and testing phases, which can produce better solutions with more computation. Learning utilizes the data generated by search for improving policy, which can achieve the better performance with more parameters and more searched data. Existing open-source projects that attempt to reproduce o1 can be seem as a part or a variant of our roadmap. Collectively, these components underscore how learning and search drive o1's advancement, making meaningful contributions to the development of LLM.",
      "standard_url": "http://arxiv.org/abs/2412.14135v1",
      "id": "2024-scaling_of_search_and_learning:_a_roadmap_to_reproduce_o1_from_reinforcement_learning_perspective"
    },
    "77": {
      "enum": "77",
      "authors": "Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, Wanli Ouyang, Dongzhan Zhou",
      "title": "Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning",
      "publication": "CoRR, abs/2410.02884, 2024",
      "year": 2024,
      "summary": "This paper presents an advanced mathematical problem-solving framework, LLaMA-Berry, for enhancing the mathematical reasoning ability of Large Language Models (LLMs). The framework combines Monte Carlo Tree Search (MCTS) with iterative Self-Refine to optimize the reasoning path and utilizes a pairwise reward model to evaluate different paths globally. By leveraging the self-critic and rewriting capabilities of LLMs, Self-Refine applied to MCTS (SR-MCTS) overcomes the inefficiencies and limitations of conventional step-wise and greedy search algorithms by fostering a more efficient exploration of solution spaces. Pairwise Preference Reward Model~(PPRM), inspired by Reinforcement Learning from Human Feedback (RLHF), is then used to model pairwise preferences between solutions, utilizing an Enhanced Borda Count (EBC) method to synthesize these preferences into a global ranking score to find better answers. This approach addresses the challenges of scoring variability and non-independent distributions in mathematical reasoning tasks. The framework has been tested on general and advanced benchmarks, showing superior performance in terms of search efficiency and problem-solving capability compared to existing methods like ToT and rStar, particularly in complex Olympiad-level benchmarks, including GPQA, AIME24 and AMC23.",
      "standard_url": "http://arxiv.org/abs/2410.02884v2",
      "id": "2024-llama-berry:_pairwise_optimization_for_o1-like_olympiad-level_mathematical_reasoning"
    },
    "78": {
      "enum": "78",
      "authors": "Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, Bingnan Zheng, Bang Liu, Yuyu Luo, Chenglin Wu",
      "title": "Aflow: Automating agentic workflow generation",
      "publication": "arXiv preprint arXiv:2410.10762, 2024",
      "year": 2024,
      "summary": "Large language models (LLMs) have demonstrated remarkable potential in solving complex tasks across diverse domains, typically by employing agentic workflows that follow detailed instructions and operational sequences. However, constructing these workflows requires significant human effort, limiting scalability and generalizability. Recent research has sought to automate the generation and optimization of these workflows, but existing methods still rely on initial manual setup and fall short of achieving fully automated and effective workflow generation. To address this challenge, we reformulate workflow optimization as a search problem over code-represented workflows, where LLM-invoking nodes are connected by edges. We introduce AFlow, an automated framework that efficiently explores this space using Monte Carlo Tree Search, iteratively refining workflows through code modification, tree-structured experience, and execution feedback. Empirical evaluations across six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7% average improvement over state-of-the-art baselines. Furthermore, AFlow enables smaller models to outperform GPT-4o on specific tasks at 4.55% of its inference cost in dollars. The code is available at https://github.com/FoundationAgents/AFlow.",
      "standard_url": "http://arxiv.org/abs/2410.10762v4",
      "id": "2024-aflow:_automating_agentic_workflow_generation"
    },
    "79": {
      "enum": "79",
      "authors": "Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Chen Xu, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, Shuming Shi",
      "title": "Siren’s song in the AI ocean: A survey on hallucination in large language models",
      "publication": "CoRR, abs/2309.01219, 2023",
      "year": 2023,
      "summary": "While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.",
      "standard_url": "http://arxiv.org/abs/2309.01219v3",
      "id": "2023-siren’s_song_in_the_ai_ocean:_a_survey_on_hallucination_in_large_language_models"
    },
    "80": {
      "enum": "80",
      "authors": "Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, Jitao Sang",
      "title": "o1-coder: an o1 replication for coding",
      "publication": "CoRR, abs/2412.00154, 2024",
      "year": 2024,
      "summary": "The technical report introduces O1-CODER, an attempt to replicate OpenAI's o1 model with a focus on coding tasks. It integrates reinforcement learning (RL) and Monte Carlo Tree Search (MCTS) to enhance the model's System-2 thinking capabilities. The framework includes training a Test Case Generator (TCG) for standardized code testing, using MCTS to generate code data with reasoning processes, and iteratively fine-tuning the policy model to initially produce pseudocode and then generate the full code. The report also addresses the opportunities and challenges in deploying o1-like models in real-world applications, suggesting transitioning to the System-2 paradigm and highlighting the imperative for world model construction. Updated model progress and experimental results will be reported in subsequent versions. All source code, curated datasets, as well as the derived models are disclosed at https://github.com/ADaM-BJTU/O1-CODER .",
      "standard_url": "http://arxiv.org/abs/2412.00154v2",
      "id": "2024-o1-coder:_an_o1_replication_for_coding"
    },
    "81": {
      "enum": "81",
      "authors": "Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, Jitao Sang",
      "title": "o1-coder: an o1 replication for coding",
      "publication": "arXiv preprint arXiv:2412.00154, 2024",
      "year": 2024,
      "summary": "The technical report introduces O1-CODER, an attempt to replicate OpenAI's o1 model with a focus on coding tasks. It integrates reinforcement learning (RL) and Monte Carlo Tree Search (MCTS) to enhance the model's System-2 thinking capabilities. The framework includes training a Test Case Generator (TCG) for standardized code testing, using MCTS to generate code data with reasoning processes, and iteratively fine-tuning the policy model to initially produce pseudocode and then generate the full code. The report also addresses the opportunities and challenges in deploying o1-like models in real-world applications, suggesting transitioning to the System-2 paradigm and highlighting the imperative for world model construction. Updated model progress and experimental results will be reported in subsequent versions. All source code, curated datasets, as well as the derived models are disclosed at https://github.com/ADaM-BJTU/O1-CODER .",
      "standard_url": "http://arxiv.org/abs/2412.00154v2",
      "id": "2024-o1-coder:_an_o1_replication_for_coding"
    },
    "82": {
      "enum": "82",
      "authors": "Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, Bin Cui",
      "title": "Retrieval-augmented generation for ai-generated content: A survey",
      "publication": "arXiv preprint arXiv:2402.19473, 2024",
      "year": 2024,
      "summary": "Advancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Github: https://github.com/PKU-DAIR/RAG-Survey.",
      "standard_url": "http://arxiv.org/abs/2402.19473v6",
      "id": "2024-retrieval-augmented_generation_for_ai-generated_content:_a_survey"
    },
    "83": {
      "enum": "83",
      "authors": "Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, Junyang Lin",
      "title": "Processbench: Identifying process errors in mathematical reasoning",
      "publication": "arXiv preprint arXiv:2412.06559, 2024",
      "year": 2024,
      "summary": "As language models regularly make mistakes when solving math problems, automated identification of errors in the reasoning process becomes increasingly significant for their scalable oversight. In this paper, we introduce ProcessBench for measuring the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. Each test case contains a step-by-step solution with error location annotated by human experts. Models are required to identify the earliest step that contains an error, or conclude that all steps are correct. We conduct extensive evaluation on ProcessBench, involving two types of models: process reward models (PRMs) and critic models, where for the latter we prompt general language models to critique each solution step by step. We draw two main observations: (1) Existing PRMs typically fail to generalize to more challenging math problems beyond GSM8K and MATH. They underperform both critic models (i.e., prompted general language models) and our own trained PRM that is straightforwardly fine-tuned on the PRM800K dataset. (2) The best open-source model, QwQ-32B-Preview, has demonstrated the critique capability competitive with the proprietary model GPT-4o, despite that it still lags behind the reasoning-specialized o1-mini. We hope ProcessBench can foster future research in reasoning process assessment, paving the way toward scalable oversight of language models.",
      "standard_url": "http://arxiv.org/abs/2412.06559v4",
      "id": "2024-processbench:_identifying_process_errors_in_mathematical_reasoning"
    },
    "84": {
      "enum": "84",
      "authors": "Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Zeyu Zhang, Yifan Zhou, Shizhe Liang, Zihao Wu, Yanjun Lyu, Peng Shu, Xiaowei Yu, Chao Cao, Hanqi Jiang, Hanxu Chen, Yiwei Li, Junhao Chen, Huawen Hu, Yiheng Liu, Huaqin Zhao, Shaochen Xu, Haixing Dai, Lin Zhao, Ruidong Zhang, Wei Zhao, Zhenyuan Yang, Jingyuan Chen, Peilong Wang, Wei Ruan, Hui Wang, Huan Zhao, Jing Zhang, Yiming Ren, Shihuan Qin, Tong Chen, Jiaxi Li, Arif Hassan Zidan, Afrar Jahin, Minheng Chen, Sichen Xia, Jason Holmes, Yan Zhuang, Jiaqi Wang, Bochen Xu, Weiran Xia, Jichao Yu, Kaibo Tang, Yaxuan Yang, Bolun Sun, Tao Yang, Guoyu Lu, Xianqiao Wang, Lilong Chai, He Li, Jin Lu, Xin Zhang, Bao Ge, Xintao Hu, Lian Zhang, Hua Zhou, Lu Zhang, Shu Zhang, Zhen Xiang, Yudan Ren, Jun Liu, Xi Jiang, Yu Bao, Wei Zhang, Xiang Li, Gang Li, Wei Liu, Dinggang Shen, Andrea Sikora, Xiaoming Zhai, Dajiang Zhu, Tuo Zhang, Tianming Liu",
      "title": "Evaluation of openai o1: Opportunities and challenges of AGI",
      "publication": "CoRR, abs/2409.18486, 2024",
      "year": 2024,
      "summary": "This comprehensive study evaluates the performance of OpenAI's o1-preview large language model across a diverse array of complex reasoning tasks, spanning multiple domains, including computer science, mathematics, natural sciences, medicine, linguistics, and social sciences. Through rigorous testing, o1-preview demonstrated remarkable capabilities, often achieving human-level or superior performance in areas ranging from coding challenges to scientific reasoning and from language processing to creative problem-solving. Key findings include:\n  -83.3% success rate in solving complex competitive programming problems, surpassing many human experts.\n  -Superior ability in generating coherent and accurate radiology reports, outperforming other evaluated models.\n  -100% accuracy in high school-level mathematical reasoning tasks, providing detailed step-by-step solutions.\n  -Advanced natural language inference capabilities across general and specialized domains like medicine.\n  -Impressive performance in chip design tasks, outperforming specialized models in areas such as EDA script generation and bug analysis.\n  -Remarkable proficiency in anthropology and geology, demonstrating deep understanding and reasoning in these specialized fields.\n  -Strong capabilities in quantitative investing. O1 has comprehensive financial knowledge and statistical modeling skills.\n  -Effective performance in social media analysis, including sentiment analysis and emotion recognition.\n  The model excelled particularly in tasks requiring intricate reasoning and knowledge integration across various fields. While some limitations were observed, including occasional errors on simpler problems and challenges with certain highly specialized concepts, the overall results indicate significant progress towards artificial general intelligence.",
      "standard_url": "http://arxiv.org/abs/2409.18486v4",
      "id": "2024-evaluation_of_openai_o1:_opportunities_and_challenges_of_agi"
    },
    "85": {
      "enum": "85",
      "authors": "Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Zeyu Zhang, Yifan Zhou, Shizhe Liang, Zihao Wu, Yanjun Lyu, Peng Shu, Xiaowei Yu, Chao Cao, Hanqi Jiang, Hanxu Chen, Yiwei Li, Junhao Chen, Huawen Hu, Yiheng Liu, Huaqin Zhao, Shaochen Xu, Haixing Dai, Lin Zhao, Ruidong Zhang, Wei Zhao, Zhenyuan Yang, Jingyuan Chen, Peilong Wang, Wei Ruan, Hui Wang, Huan Zhao, Jing Zhang, Yiming Ren, Shihuan Qin, Tong Chen, Jiaxi Li, Arif Hassan Zidan, Afrar Jahin, Minheng Chen, Sichen Xia, Jason Holmes, Yan Zhuang, Jiaqi Wang, Bochen Xu, Weiran Xia, Jichao Yu, Kaibo Tang, Yaxuan Yang, Bolun Sun, Tao Yang, Guoyu Lu, Xianqiao Wang, Lilong Chai, He Li, Jin Lu, Xin Zhang, Bao Ge, Xintao Hu, Lian Zhang, Hua Zhou, Lu Zhang, Shu Zhang, Zhen Xiang, Yudan Ren, Jun Liu, Xi Jiang, Yu Bao, Wei Zhang, Xiang Li, Gang Li, Wei Liu, Dinggang Shen, Andrea Sikora, Xiaoming Zhai, Dajiang Zhu, Tuo Zhang, Tianming Liu",
      "title": "Evaluation of openai o1: Opportunities and challenges of agi",
      "publication": "arXiv preprint arXiv:2409.18486, 2024",
      "year": 2024,
      "summary": "This comprehensive study evaluates the performance of OpenAI's o1-preview large language model across a diverse array of complex reasoning tasks, spanning multiple domains, including computer science, mathematics, natural sciences, medicine, linguistics, and social sciences. Through rigorous testing, o1-preview demonstrated remarkable capabilities, often achieving human-level or superior performance in areas ranging from coding challenges to scientific reasoning and from language processing to creative problem-solving. Key findings include:\n  -83.3% success rate in solving complex competitive programming problems, surpassing many human experts.\n  -Superior ability in generating coherent and accurate radiology reports, outperforming other evaluated models.\n  -100% accuracy in high school-level mathematical reasoning tasks, providing detailed step-by-step solutions.\n  -Advanced natural language inference capabilities across general and specialized domains like medicine.\n  -Impressive performance in chip design tasks, outperforming specialized models in areas such as EDA script generation and bug analysis.\n  -Remarkable proficiency in anthropology and geology, demonstrating deep understanding and reasoning in these specialized fields.\n  -Strong capabilities in quantitative investing. O1 has comprehensive financial knowledge and statistical modeling skills.\n  -Effective performance in social media analysis, including sentiment analysis and emotion recognition.\n  The model excelled particularly in tasks requiring intricate reasoning and knowledge integration across various fields. While some limitations were observed, including occasional errors on simpler problems and challenges with certain highly specialized concepts, the overall results indicate significant progress towards artificial general intelligence.",
      "standard_url": "http://arxiv.org/abs/2409.18486v4",
      "id": "2024-evaluation_of_openai_o1:_opportunities_and_challenges_of_agi"
    },
    "86": {
      "enum": "86",
      "authors": "Yujia Zhou, Yan Liu, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Zheng Liu, Chaozhuo Li, Zhicheng Dou, Tsung-Yi Ho, Philip S. Yu",
      "title": "Trustworthiness in retrieval-augmented generation systems: A survey",
      "publication": "CoRR, abs/2409.10102, 2024",
      "year": 2024,
      "summary": "Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal paradigm in the development of Large Language Models (LLMs). While much of the current research in this field focuses on performance optimization, particularly in terms of accuracy and efficiency, the trustworthiness of RAG systems remains an area still under exploration. From a positive perspective, RAG systems are promising to enhance LLMs by providing them with useful and up-to-date knowledge from vast external databases, thereby mitigating the long-standing problem of hallucination. While from a negative perspective, RAG systems are at the risk of generating undesirable contents if the retrieved information is either inappropriate or poorly utilized. To address these concerns, we propose a unified framework that assesses the trustworthiness of RAG systems across six key dimensions: factuality, robustness, fairness, transparency, accountability, and privacy. Within this framework, we thoroughly review the existing literature on each dimension. Additionally, we create the evaluation benchmark regarding the six dimensions and conduct comprehensive evaluations for a variety of proprietary and open-source models. Finally, we identify the potential challenges for future research based on our investigation results. Through this work, we aim to lay a structured foundation for future investigations and provide practical insights for enhancing the trustworthiness of RAG systems in real-world applications.",
      "standard_url": "http://arxiv.org/abs/2409.10102v1",
      "id": "2024-trustworthiness_in_retrieval-augmented_generation_systems:_a_survey"
    },
    "87": {
      "enum": "87",
      "authors": "Yujia Zhou, Zheng Liu, Zhicheng Dou",
      "title": "Assistrag: Boosting the potential of large language models with an intelligent information assistant",
      "publication": "CoRR, abs/2411.06805, 2024",
      "year": 2024,
      "summary": "The emergence of Large Language Models (LLMs) has significantly advanced natural language processing, but these models often generate factually incorrect information, known as \"hallucination\". Initial retrieval-augmented generation (RAG) methods like the \"Retrieve-Read\" framework was inadequate for complex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised Fine-Tuning (SFT) methods improved performance but required frequent retraining and risked altering foundational LLM capabilities. To cope with these challenges, we propose Assistant-based Retrieval-Augmented Generation (AssistRAG), integrating an intelligent information assistant within LLMs. This assistant manages memory and knowledge through tool usage, action execution, memory building, and plan specification. Using a two-phase training approach, Curriculum Assistant Learning and Reinforced Preference Optimization. AssistRAG enhances information retrieval and decision-making. Experiments show AssistRAG significantly outperforms benchmarks, especially benefiting less advanced LLMs, by providing superior reasoning capabilities and accurate responses.",
      "standard_url": "http://arxiv.org/abs/2411.06805v1",
      "id": "2024-assistrag:_boosting_the_potential_of_large_language_models_with_an_intelligent_information_assistant"
    },
    "88": {
      "enum": "88",
      "authors": "Yujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, Zhicheng Dou",
      "title": "Metacognitive retrieval-augmented large language models",
      "publication": "In Tat-Seng Chua, Chong-Wah Ngo, Ravi Kumar, Hady W. Lauw, and Roy Ka-Wei Lee, editors, Proceedings of the ACM on Web Conference 2024, WWW 2024, Singapore, May 13-17, 2024, pages 1453–1463. ACM, 2024",
      "year": 2024,
      "summary": "Retrieval-augmented generation have become central in natural language processing due to their efficacy in generating factual content. While traditional methods employ single-time retrieval, more recent approaches have shifted towards multi-time retrieval for multi-hop reasoning tasks. However, these strategies are bound by predefined reasoning steps, potentially leading to inaccuracies in response generation. This paper introduces MetaRAG, an approach that combines the retrieval-augmented generation process with metacognition. Drawing from cognitive psychology, metacognition allows an entity to self-reflect and critically evaluate its cognitive processes. By integrating this, MetaRAG enables the model to monitor, evaluate, and plan its response strategies, enhancing its introspective reasoning abilities. Through a three-step metacognitive regulation pipeline, the model can identify inadequacies in initial cognitive responses and fixes them. Empirical evaluations show that MetaRAG significantly outperforms existing methods.",
      "standard_url": "http://arxiv.org/abs/2402.11626v1",
      "id": "2024-metacognitive_retrieval-augmented_large_language_models"
    }
  },
  "authors": "Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, Zhicheng Dou",
  "summary": "Large reasoning models (LRMs) like OpenAI-o1 have demonstrated impressive long stepwise reasoning capabilities through large-scale reinforcement learning. However, their extended reasoning processes often suffer from knowledge insufficiency, leading to frequent uncertainties and potential errors. To address this limitation, we introduce \\textbf{Search-o1}, a framework that enhances LRMs with an agentic retrieval-augmented generation (RAG) mechanism and a Reason-in-Documents module for refining retrieved documents. Search-o1 integrates an agentic search workflow into the reasoning process, enabling dynamic retrieval of external knowledge when LRMs encounter uncertain knowledge points. Additionally, due to the verbose nature of retrieved documents, we design a separate Reason-in-Documents module to deeply analyze the retrieved information before injecting it into the reasoning chain, minimizing noise and preserving coherent reasoning flow. Extensive experiments on complex reasoning tasks in science, mathematics, and coding, as well as six open-domain QA benchmarks, demonstrate the strong performance of Search-o1. This approach enhances the trustworthiness and applicability of LRMs in complex reasoning tasks, paving the way for more reliable and versatile intelligent systems. The code is available at \\url{https://github.com/sunnynexus/Search-o1}.",
  "standard_url": "http://arxiv.org/abs/2501.05366v1",
  "id": "2025-search-o1:_agentic_search-enhanced_large_reasoning_models"
}